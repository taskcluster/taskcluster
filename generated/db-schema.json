{
  "access": {
    "auth": {
      "tables": {
        "clients": "write",
        "roles": "write"
      }
    },
    "github": {
      "tables": {
        "github_builds": "write",
        "github_checks": "write",
        "github_integrations": "write"
      }
    },
    "hooks": {
      "tables": {
        "hooks": "write",
        "hooks_last_fires": "write",
        "hooks_queues": "write"
      }
    },
    "index": {
      "tables": {
        "index_namespaces": "write",
        "indexed_tasks": "write"
      }
    },
    "notify": {
      "tables": {
        "denylisted_notifications": "write"
      }
    },
    "object": {
      "tables": {
        "object_hashes": "write",
        "objects": "write"
      }
    },
    "purge_cache": {
      "tables": {
        "cache_purges": "write"
      }
    },
    "queue": {
      "tables": {
        "azure_queue_messages": "write",
        "queue_artifacts": "write",
        "queue_workers": "write",
        "task_dependencies": "write",
        "task_groups": "write",
        "task_queues": "write",
        "tasks": "write"
      }
    },
    "secrets": {
      "tables": {
        "secrets": "write"
      }
    },
    "web_server": {
      "tables": {
        "access_tokens": "write",
        "authorization_codes": "write",
        "github_access_tokens": "write",
        "sessions": "write"
      }
    },
    "worker_manager": {
      "tables": {
        "queue_workers": "read",
        "worker_pool_errors": "write",
        "worker_pools": "write",
        "workers": "write"
      }
    }
  },
  "tables": {
    "access_tokens": {
      "client_details": "jsonb not null",
      "client_id": "text not null",
      "encrypted_access_token": "jsonb not null",
      "expires": "timestamp with time zone not null",
      "hashed_access_token": "text not null",
      "identity": "text not null",
      "identity_provider_id": "text not null",
      "redirect_uri": "text not null"
    },
    "authorization_codes": {
      "client_details": "jsonb not null",
      "client_id": "text not null",
      "code": "text not null",
      "expires": "timestamp with time zone not null",
      "identity": "text not null",
      "identity_provider_id": "text not null",
      "redirect_uri": "text not null"
    },
    "azure_queue_messages": {
      "expires": "timestamp with time zone not null",
      "inserted": "timestamp with time zone not null",
      "message_id": "uuid not null",
      "message_text": "text not null",
      "pop_receipt": "uuid",
      "queue_name": "text not null",
      "visible": "timestamp with time zone not null"
    },
    "cache_purges": {
      "before": "timestamp with time zone not null",
      "cache_name": "text not null",
      "expires": "timestamp with time zone not null",
      "worker_pool_id": "text not null"
    },
    "clients": {
      "client_id": "text not null",
      "created": "timestamp with time zone not null",
      "delete_on_expiration": "boolean not null",
      "description": "text not null",
      "disabled": "boolean not null",
      "encrypted_access_token": "jsonb not null",
      "expires": "timestamp with time zone not null",
      "last_date_used": "timestamp with time zone not null",
      "last_modified": "timestamp with time zone not null",
      "last_rotated": "timestamp with time zone not null",
      "scopes": "jsonb not null"
    },
    "denylisted_notifications": {
      "notification_address": "text not null",
      "notification_type": "text not null"
    },
    "github_access_tokens": {
      "encrypted_access_token": "jsonb not null",
      "user_id": "text not null"
    },
    "github_builds": {
      "created": "timestamp with time zone not null",
      "event_id": "text not null",
      "event_type": "text not null",
      "installation_id": "integer not null",
      "organization": "text not null",
      "repository": "text not null",
      "sha": "text not null",
      "state": "text not null",
      "task_group_id": "text not null",
      "updated": "timestamp with time zone not null"
    },
    "github_checks": {
      "check_run_id": "text not null",
      "check_suite_id": "text not null",
      "task_group_id": "text not null",
      "task_id": "text not null"
    },
    "github_integrations": {
      "installation_id": "integer not null",
      "owner": "text not null"
    },
    "hooks": {
      "bindings": "jsonb not null",
      "encrypted_next_task_id": "jsonb not null",
      "encrypted_trigger_token": "jsonb not null",
      "hook_group_id": "text not null",
      "hook_id": "text not null",
      "metadata": "jsonb not null",
      "next_scheduled_date": "timestamp with time zone not null",
      "schedule": "jsonb not null",
      "task": "jsonb not null",
      "trigger_schema": "jsonb not null"
    },
    "hooks_last_fires": {
      "error": "text not null",
      "fired_by": "text not null",
      "hook_group_id": "text not null",
      "hook_id": "text not null",
      "result": "text not null",
      "task_create_time": "timestamp with time zone not null",
      "task_id": "text not null"
    },
    "hooks_queues": {
      "bindings": "jsonb not null",
      "hook_group_id": "text not null",
      "hook_id": "text not null",
      "queue_name": "text not null"
    },
    "index_namespaces": {
      "expires": "timestamp with time zone not null",
      "name": "text not null",
      "parent": "text not null"
    },
    "indexed_tasks": {
      "data": "jsonb not null",
      "expires": "timestamp with time zone not null",
      "name": "text not null",
      "namespace": "text not null",
      "rank": "integer not null",
      "task_id": "text not null"
    },
    "object_hashes": {
      "algorithm": "text not null",
      "hash": "text not null",
      "name": "text not null"
    },
    "objects": {
      "backend_id": "text not null",
      "data": "jsonb not null",
      "expires": "timestamp with time zone not null",
      "name": "text not null",
      "project_id": "text not null",
      "upload_expires": "timestamp with time zone",
      "upload_id": "text"
    },
    "queue_artifacts": {
      "content_type": "text not null",
      "details": "jsonb not null",
      "expires": "timestamp with time zone not null",
      "name": "text not null",
      "present": "boolean not null",
      "run_id": "integer not null",
      "storage_type": "text not null",
      "task_id": "text not null"
    },
    "queue_workers": {
      "expires": "timestamp with time zone not null",
      "first_claim": "timestamp with time zone not null",
      "last_date_active": "timestamp with time zone",
      "quarantine_until": "timestamp with time zone not null",
      "recent_tasks": "jsonb not null",
      "task_queue_id": "text not null",
      "worker_group": "text not null",
      "worker_id": "text not null"
    },
    "roles": {
      "created": "timestamp with time zone not null",
      "description": "text not null",
      "etag": "uuid not null",
      "last_modified": "timestamp with time zone not null",
      "role_id": "text not null",
      "scopes": "jsonb not null"
    },
    "secrets": {
      "encrypted_secret": "jsonb not null",
      "expires": "timestamp with time zone not null",
      "name": "text not null"
    },
    "sessions": {
      "data": "jsonb not null",
      "encrypted_session_id": "jsonb not null",
      "expires": "timestamp with time zone not null",
      "hashed_session_id": "text not null"
    },
    "task_dependencies": {
      "dependent_task_id": "text not null",
      "expires": "timestamp with time zone not null",
      "required_task_id": "text not null",
      "requires": "task_requires not null",
      "satisfied": "boolean not null"
    },
    "task_groups": {
      "expires": "timestamp with time zone not null",
      "scheduler_id": "text not null",
      "task_group_id": "text not null"
    },
    "task_queues": {
      "description": "text not null",
      "expires": "timestamp with time zone not null",
      "last_date_active": "timestamp with time zone not null",
      "stability": "text not null",
      "task_queue_id": "text not null"
    },
    "tasks": {
      "created": "timestamp with time zone not null",
      "deadline": "timestamp with time zone not null",
      "dependencies": "jsonb not null",
      "ever_resolved": "boolean not null",
      "expires": "timestamp with time zone not null",
      "extra": "jsonb not null",
      "metadata": "jsonb not null",
      "payload": "jsonb not null",
      "priority": "task_priority not null",
      "project_id": "text",
      "requires": "task_requires not null",
      "retries": "integer not null",
      "retries_left": "integer not null",
      "routes": "jsonb not null",
      "runs": "jsonb not null",
      "scheduler_id": "text not null",
      "scopes": "jsonb not null",
      "tags": "jsonb not null",
      "taken_until": "timestamp with time zone",
      "task_group_id": "text not null",
      "task_id": "text not null",
      "task_queue_id": "text"
    },
    "worker_pool_errors": {
      "description": "text not null",
      "error_id": "text not null",
      "extra": "jsonb",
      "kind": "text not null",
      "reported": "timestamp with time zone not null",
      "title": "text not null",
      "worker_pool_id": "text not null"
    },
    "worker_pools": {
      "config": "jsonb not null",
      "created": "timestamp with time zone not null",
      "description": "text not null",
      "email_on_error": "boolean not null",
      "last_modified": "timestamp with time zone not null",
      "owner": "text not null",
      "previous_provider_ids": "jsonb not null",
      "provider_data": "jsonb not null",
      "provider_id": "text not null",
      "worker_pool_id": "text not null"
    },
    "workers": {
      "capacity": "integer not null",
      "created": "timestamp with time zone not null",
      "etag": "uuid not null",
      "expires": "timestamp with time zone not null",
      "last_checked": "timestamp with time zone not null",
      "last_modified": "timestamp with time zone not null",
      "provider_data": "jsonb not null",
      "provider_id": "text not null",
      "secret": "jsonb",
      "state": "text not null",
      "worker_group": "text not null",
      "worker_id": "text not null",
      "worker_pool_id": "text not null"
    }
  },
  "versions": [
    {
      "description": "fake wigets table for DB deployment testing",
      "downgradeScript": "begin\n  revoke select, insert, update, delete on widgets from $db_user_prefix$_notify;\n  drop table widgets;\nend",
      "methods": {
        "update_widgets": {
          "args": "name_in text",
          "body": "begin\n  insert into widgets (name) values (name_in);\n  return query select widgets.name from widgets;\nend",
          "deprecated": false,
          "description": "Temporary method to test infrastructure support fo database access",
          "mode": "write",
          "returns": "table (name text)",
          "serviceName": "notify"
        }
      },
      "migrationScript": "begin\n  create table widgets (\n    name text\n  );\n  grant select, insert, update, delete on widgets to $db_user_prefix$_notify;\nend",
      "version": 1
    },
    {
      "description": "define all taskcluster-lib-entities tables and functions",
      "downgradeScript": "begin\n  revoke select, insert, update, delete on clients_entities from $db_user_prefix$_auth;\n  revoke select, insert, update, delete on roles_entities from $db_user_prefix$_auth;\n\n  revoke select, insert, update, delete on taskcluster_check_runs_entities from $db_user_prefix$_github;\n  revoke select, insert, update, delete on taskcluster_checks_to_tasks_entities from $db_user_prefix$_github;\n  revoke select, insert, update, delete on taskcluster_github_builds_entities from $db_user_prefix$_github;\n  revoke select, insert, update, delete on taskcluster_integration_owners_entities from $db_user_prefix$_github;\n\n  revoke select, insert, update, delete on hooks_entities from $db_user_prefix$_hooks;\n  revoke select, insert, update, delete on last_fire_3_entities from $db_user_prefix$_hooks;\n  revoke select, insert, update, delete on queues_entities from $db_user_prefix$_hooks;\n\n  revoke select, insert, update, delete on indexed_tasks_entities from $db_user_prefix$_index;\n  revoke select, insert, update, delete on namespaces_entities from $db_user_prefix$_index;\n\n  revoke select, insert, update, delete on denylisted_notification_entities from $db_user_prefix$_notify;\n\n  revoke select, insert, update, delete on cache_purges_entities from $db_user_prefix$_purge_cache;\n\n  revoke select, insert, update, delete on secrets_entities from $db_user_prefix$_secrets;\n\n  revoke select, insert, update, delete on access_token_table_entities from $db_user_prefix$_web_server;\n  revoke select, insert, update, delete on authorization_codes_table_entities from $db_user_prefix$_web_server;\n  revoke select, insert, update, delete on github_access_token_table_entities from $db_user_prefix$_web_server;\n  revoke select, insert, update, delete on session_storage_table_entities from $db_user_prefix$_web_server;\n\n  revoke select, insert, update, delete on wmworker_pool_errors_entities from $db_user_prefix$_worker_manager;\n  revoke select, insert, update, delete on wmworker_pools_entities from $db_user_prefix$_worker_manager;\n  revoke select, insert, update, delete on wmworkers_entities from $db_user_prefix$_worker_manager;\n\n  revoke select, insert, update, delete on queue_tasks_entities from $db_user_prefix$_queue;\n  revoke select, insert, update, delete on queue_artifacts_entities from $db_user_prefix$_queue;\n  revoke select, insert, update, delete on queue_task_groups_entities from $db_user_prefix$_queue;\n  revoke select, insert, update, delete on queue_task_group_members_entities from $db_user_prefix$_queue;\n  revoke select, insert, update, delete on queue_task_group_active_sets_entities from $db_user_prefix$_queue;\n  revoke select, insert, update, delete on queue_task_requirement_entities from $db_user_prefix$_queue;\n  revoke select, insert, update, delete on queue_task_dependency_entities from $db_user_prefix$_queue;\n  revoke select, insert, update, delete on queue_worker_entities from $db_user_prefix$_queue;\n  revoke select, insert, update, delete on queue_worker_type_entities from $db_user_prefix$_queue;\n  revoke select, insert, update, delete on queue_provisioner_entities from $db_user_prefix$_queue;\n\n  drop table clients_entities;\n  drop table roles_entities;\n  drop table taskcluster_github_builds_entities;\n  drop table taskcluster_integration_owners_entities;\n  drop table taskcluster_checks_to_tasks_entities;\n  drop table taskcluster_check_runs_entities;\n  drop table hooks_entities;\n  drop table queues_entities;\n  drop table last_fire_3_entities;\n  drop table indexed_tasks_entities;\n  drop table namespaces_entities;\n  drop table denylisted_notification_entities;\n  drop table cache_purges_entities;\n  drop table queue_tasks_entities;\n  drop table queue_artifacts_entities;\n  drop table queue_task_groups_entities;\n  drop table queue_task_group_members_entities;\n  drop table queue_task_group_active_sets_entities;\n  drop table queue_task_requirement_entities;\n  drop table queue_task_dependency_entities;\n  drop table queue_worker_entities;\n  drop table queue_worker_type_entities;\n  drop table queue_provisioner_entities;\n  drop table secrets_entities;\n  drop table authorization_codes_table_entities;\n  drop table access_token_table_entities;\n  drop table session_storage_table_entities;\n  drop table github_access_token_table_entities;\n  drop table wmworkers_entities;\n  drop table wmworker_pools_entities;\n  drop table wmworker_pool_errors_entities;\nend",
      "methods": {
        "access_token_table_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into access_token_table_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, access_token_table_entities_create.version, new_etag)\n    where access_token_table_entities.partition_key = access_token_table_entities_create.pk and access_token_table_entities.row_key = access_token_table_entities_create.rk;\n  else\n    insert into access_token_table_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "web_server"
        },
        "access_token_table_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select access_token_table_entities.partition_key, access_token_table_entities.row_key, access_token_table_entities.value, access_token_table_entities.version,\n  access_token_table_entities.etag from access_token_table_entities\n  where access_token_table_entities.partition_key = access_token_table_entities_load.partition_key and access_token_table_entities.row_key = access_token_table_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "web_server"
        },
        "access_token_table_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update access_token_table_entities\n  set (value, version, etag) = (properties, access_token_table_entities_modify.version, new_etag)\n  where access_token_table_entities.partition_key = access_token_table_entities_modify.partition_key and access_token_table_entities.row_key = access_token_table_entities_modify.row_key and access_token_table_entities.etag = access_token_table_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform access_token_table_entities.etag from access_token_table_entities\n  where access_token_table_entities.partition_key = access_token_table_entities_modify.partition_key and access_token_table_entities.row_key = access_token_table_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "web_server"
        },
        "access_token_table_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from access_token_table_entities\n  where access_token_table_entities.partition_key = access_token_table_entities_remove.partition_key and access_token_table_entities.row_key = access_token_table_entities_remove.row_key\n  returning access_token_table_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "web_server"
        },
        "access_token_table_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select access_token_table_entities.partition_key, access_token_table_entities.row_key, access_token_table_entities.value, access_token_table_entities.version, access_token_table_entities.etag from access_token_table_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if access_token_table_entities_scan.pk is not null or access_token_table_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if access_token_table_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(access_token_table_entities_scan.pk);\n  end if;\n\n  if access_token_table_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(access_token_table_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by access_token_table_entities.partition_key, access_token_table_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "web_server"
        },
        "authorization_codes_table_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into authorization_codes_table_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, authorization_codes_table_entities_create.version, new_etag)\n    where authorization_codes_table_entities.partition_key = authorization_codes_table_entities_create.pk and authorization_codes_table_entities.row_key = authorization_codes_table_entities_create.rk;\n  else\n    insert into authorization_codes_table_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "web_server"
        },
        "authorization_codes_table_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select authorization_codes_table_entities.partition_key, authorization_codes_table_entities.row_key, authorization_codes_table_entities.value, authorization_codes_table_entities.version,\n  authorization_codes_table_entities.etag from authorization_codes_table_entities\n  where authorization_codes_table_entities.partition_key = authorization_codes_table_entities_load.partition_key and authorization_codes_table_entities.row_key = authorization_codes_table_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "web_server"
        },
        "authorization_codes_table_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update authorization_codes_table_entities\n  set (value, version, etag) = (properties, authorization_codes_table_entities_modify.version, new_etag)\n  where authorization_codes_table_entities.partition_key = authorization_codes_table_entities_modify.partition_key and authorization_codes_table_entities.row_key = authorization_codes_table_entities_modify.row_key and authorization_codes_table_entities.etag = authorization_codes_table_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform authorization_codes_table_entities.etag from authorization_codes_table_entities\n  where authorization_codes_table_entities.partition_key = authorization_codes_table_entities_modify.partition_key and authorization_codes_table_entities.row_key = authorization_codes_table_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "web_server"
        },
        "authorization_codes_table_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from authorization_codes_table_entities\n  where authorization_codes_table_entities.partition_key = authorization_codes_table_entities_remove.partition_key and authorization_codes_table_entities.row_key = authorization_codes_table_entities_remove.row_key\n  returning authorization_codes_table_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "web_server"
        },
        "authorization_codes_table_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select authorization_codes_table_entities.partition_key, authorization_codes_table_entities.row_key, authorization_codes_table_entities.value, authorization_codes_table_entities.version, authorization_codes_table_entities.etag from authorization_codes_table_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if authorization_codes_table_entities_scan.pk is not null or authorization_codes_table_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if authorization_codes_table_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(authorization_codes_table_entities_scan.pk);\n  end if;\n\n  if authorization_codes_table_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(authorization_codes_table_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by authorization_codes_table_entities.partition_key, authorization_codes_table_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "web_server"
        },
        "cache_purges_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into cache_purges_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, cache_purges_entities_create.version, new_etag)\n    where cache_purges_entities.partition_key = cache_purges_entities_create.pk and cache_purges_entities.row_key = cache_purges_entities_create.rk;\n  else\n    insert into cache_purges_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "purge_cache"
        },
        "cache_purges_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select cache_purges_entities.partition_key, cache_purges_entities.row_key, cache_purges_entities.value, cache_purges_entities.version,\n  cache_purges_entities.etag from cache_purges_entities\n  where cache_purges_entities.partition_key = cache_purges_entities_load.partition_key and cache_purges_entities.row_key = cache_purges_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "purge_cache"
        },
        "cache_purges_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update cache_purges_entities\n  set (value, version, etag) = (properties, cache_purges_entities_modify.version, new_etag)\n  where cache_purges_entities.partition_key = cache_purges_entities_modify.partition_key and cache_purges_entities.row_key = cache_purges_entities_modify.row_key and cache_purges_entities.etag = cache_purges_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform cache_purges_entities.etag from cache_purges_entities\n  where cache_purges_entities.partition_key = cache_purges_entities_modify.partition_key and cache_purges_entities.row_key = cache_purges_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "purge_cache"
        },
        "cache_purges_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from cache_purges_entities\n  where cache_purges_entities.partition_key = cache_purges_entities_remove.partition_key and cache_purges_entities.row_key = cache_purges_entities_remove.row_key\n  returning cache_purges_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "purge_cache"
        },
        "cache_purges_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select cache_purges_entities.partition_key, cache_purges_entities.row_key, cache_purges_entities.value, cache_purges_entities.version, cache_purges_entities.etag from cache_purges_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if cache_purges_entities_scan.pk is not null or cache_purges_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if cache_purges_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(cache_purges_entities_scan.pk);\n  end if;\n\n  if cache_purges_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(cache_purges_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by cache_purges_entities.partition_key, cache_purges_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "purge_cache"
        },
        "clients_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into clients_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, clients_entities_create.version, new_etag)\n    where clients_entities.partition_key = clients_entities_create.pk and clients_entities.row_key = clients_entities_create.rk;\n  else\n    insert into clients_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "auth"
        },
        "clients_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select clients_entities.partition_key, clients_entities.row_key, clients_entities.value, clients_entities.version,\n  clients_entities.etag from clients_entities\n  where clients_entities.partition_key = clients_entities_load.partition_key and clients_entities.row_key = clients_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "auth"
        },
        "clients_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update clients_entities\n  set (value, version, etag) = (properties, clients_entities_modify.version, new_etag)\n  where clients_entities.partition_key = clients_entities_modify.partition_key and clients_entities.row_key = clients_entities_modify.row_key and clients_entities.etag = clients_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform clients_entities.etag from clients_entities\n  where clients_entities.partition_key = clients_entities_modify.partition_key and clients_entities.row_key = clients_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "auth"
        },
        "clients_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from clients_entities\n  where clients_entities.partition_key = clients_entities_remove.partition_key and clients_entities.row_key = clients_entities_remove.row_key\n  returning clients_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "auth"
        },
        "clients_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select clients_entities.partition_key, clients_entities.row_key, clients_entities.value, clients_entities.version, clients_entities.etag from clients_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if clients_entities_scan.pk is not null or clients_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if clients_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(clients_entities_scan.pk);\n  end if;\n\n  if clients_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(clients_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by clients_entities.partition_key, clients_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "auth"
        },
        "denylisted_notification_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into denylisted_notification_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, denylisted_notification_entities_create.version, new_etag)\n    where denylisted_notification_entities.partition_key = denylisted_notification_entities_create.pk and denylisted_notification_entities.row_key = denylisted_notification_entities_create.rk;\n  else\n    insert into denylisted_notification_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "notify"
        },
        "denylisted_notification_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select denylisted_notification_entities.partition_key, denylisted_notification_entities.row_key, denylisted_notification_entities.value, denylisted_notification_entities.version,\n  denylisted_notification_entities.etag from denylisted_notification_entities\n  where denylisted_notification_entities.partition_key = denylisted_notification_entities_load.partition_key and denylisted_notification_entities.row_key = denylisted_notification_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "notify"
        },
        "denylisted_notification_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update denylisted_notification_entities\n  set (value, version, etag) = (properties, denylisted_notification_entities_modify.version, new_etag)\n  where denylisted_notification_entities.partition_key = denylisted_notification_entities_modify.partition_key and denylisted_notification_entities.row_key = denylisted_notification_entities_modify.row_key and denylisted_notification_entities.etag = denylisted_notification_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform denylisted_notification_entities.etag from denylisted_notification_entities\n  where denylisted_notification_entities.partition_key = denylisted_notification_entities_modify.partition_key and denylisted_notification_entities.row_key = denylisted_notification_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "notify"
        },
        "denylisted_notification_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from denylisted_notification_entities\n  where denylisted_notification_entities.partition_key = denylisted_notification_entities_remove.partition_key and denylisted_notification_entities.row_key = denylisted_notification_entities_remove.row_key\n  returning denylisted_notification_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "notify"
        },
        "denylisted_notification_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select denylisted_notification_entities.partition_key, denylisted_notification_entities.row_key, denylisted_notification_entities.value, denylisted_notification_entities.version, denylisted_notification_entities.etag from denylisted_notification_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if denylisted_notification_entities_scan.pk is not null or denylisted_notification_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if denylisted_notification_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(denylisted_notification_entities_scan.pk);\n  end if;\n\n  if denylisted_notification_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(denylisted_notification_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by denylisted_notification_entities.partition_key, denylisted_notification_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "notify"
        },
        "github_access_token_table_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into github_access_token_table_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, github_access_token_table_entities_create.version, new_etag)\n    where github_access_token_table_entities.partition_key = github_access_token_table_entities_create.pk and github_access_token_table_entities.row_key = github_access_token_table_entities_create.rk;\n  else\n    insert into github_access_token_table_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "web_server"
        },
        "github_access_token_table_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select github_access_token_table_entities.partition_key, github_access_token_table_entities.row_key, github_access_token_table_entities.value, github_access_token_table_entities.version,\n  github_access_token_table_entities.etag from github_access_token_table_entities\n  where github_access_token_table_entities.partition_key = github_access_token_table_entities_load.partition_key and github_access_token_table_entities.row_key = github_access_token_table_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "web_server"
        },
        "github_access_token_table_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update github_access_token_table_entities\n  set (value, version, etag) = (properties, github_access_token_table_entities_modify.version, new_etag)\n  where github_access_token_table_entities.partition_key = github_access_token_table_entities_modify.partition_key and github_access_token_table_entities.row_key = github_access_token_table_entities_modify.row_key and github_access_token_table_entities.etag = github_access_token_table_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform github_access_token_table_entities.etag from github_access_token_table_entities\n  where github_access_token_table_entities.partition_key = github_access_token_table_entities_modify.partition_key and github_access_token_table_entities.row_key = github_access_token_table_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "web_server"
        },
        "github_access_token_table_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from github_access_token_table_entities\n  where github_access_token_table_entities.partition_key = github_access_token_table_entities_remove.partition_key and github_access_token_table_entities.row_key = github_access_token_table_entities_remove.row_key\n  returning github_access_token_table_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "web_server"
        },
        "github_access_token_table_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select github_access_token_table_entities.partition_key, github_access_token_table_entities.row_key, github_access_token_table_entities.value, github_access_token_table_entities.version, github_access_token_table_entities.etag from github_access_token_table_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if github_access_token_table_entities_scan.pk is not null or github_access_token_table_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if github_access_token_table_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(github_access_token_table_entities_scan.pk);\n  end if;\n\n  if github_access_token_table_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(github_access_token_table_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by github_access_token_table_entities.partition_key, github_access_token_table_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "web_server"
        },
        "hooks_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into hooks_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, hooks_entities_create.version, new_etag)\n    where hooks_entities.partition_key = hooks_entities_create.pk and hooks_entities.row_key = hooks_entities_create.rk;\n  else\n    insert into hooks_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "hooks"
        },
        "hooks_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select hooks_entities.partition_key, hooks_entities.row_key, hooks_entities.value, hooks_entities.version,\n  hooks_entities.etag from hooks_entities\n  where hooks_entities.partition_key = hooks_entities_load.partition_key and hooks_entities.row_key = hooks_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "hooks"
        },
        "hooks_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update hooks_entities\n  set (value, version, etag) = (properties, hooks_entities_modify.version, new_etag)\n  where hooks_entities.partition_key = hooks_entities_modify.partition_key and hooks_entities.row_key = hooks_entities_modify.row_key and hooks_entities.etag = hooks_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform hooks_entities.etag from hooks_entities\n  where hooks_entities.partition_key = hooks_entities_modify.partition_key and hooks_entities.row_key = hooks_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "hooks"
        },
        "hooks_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from hooks_entities\n  where hooks_entities.partition_key = hooks_entities_remove.partition_key and hooks_entities.row_key = hooks_entities_remove.row_key\n  returning hooks_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "hooks"
        },
        "hooks_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select hooks_entities.partition_key, hooks_entities.row_key, hooks_entities.value, hooks_entities.version, hooks_entities.etag from hooks_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if hooks_entities_scan.pk is not null or hooks_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if hooks_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(hooks_entities_scan.pk);\n  end if;\n\n  if hooks_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(hooks_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by hooks_entities.partition_key, hooks_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "hooks"
        },
        "indexed_tasks_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into indexed_tasks_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, indexed_tasks_entities_create.version, new_etag)\n    where indexed_tasks_entities.partition_key = indexed_tasks_entities_create.pk and indexed_tasks_entities.row_key = indexed_tasks_entities_create.rk;\n  else\n    insert into indexed_tasks_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "index"
        },
        "indexed_tasks_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select indexed_tasks_entities.partition_key, indexed_tasks_entities.row_key, indexed_tasks_entities.value, indexed_tasks_entities.version,\n  indexed_tasks_entities.etag from indexed_tasks_entities\n  where indexed_tasks_entities.partition_key = indexed_tasks_entities_load.partition_key and indexed_tasks_entities.row_key = indexed_tasks_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "index"
        },
        "indexed_tasks_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update indexed_tasks_entities\n  set (value, version, etag) = (properties, indexed_tasks_entities_modify.version, new_etag)\n  where indexed_tasks_entities.partition_key = indexed_tasks_entities_modify.partition_key and indexed_tasks_entities.row_key = indexed_tasks_entities_modify.row_key and indexed_tasks_entities.etag = indexed_tasks_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform indexed_tasks_entities.etag from indexed_tasks_entities\n  where indexed_tasks_entities.partition_key = indexed_tasks_entities_modify.partition_key and indexed_tasks_entities.row_key = indexed_tasks_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "index"
        },
        "indexed_tasks_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from indexed_tasks_entities\n  where indexed_tasks_entities.partition_key = indexed_tasks_entities_remove.partition_key and indexed_tasks_entities.row_key = indexed_tasks_entities_remove.row_key\n  returning indexed_tasks_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "index"
        },
        "indexed_tasks_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select indexed_tasks_entities.partition_key, indexed_tasks_entities.row_key, indexed_tasks_entities.value, indexed_tasks_entities.version, indexed_tasks_entities.etag from indexed_tasks_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if indexed_tasks_entities_scan.pk is not null or indexed_tasks_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if indexed_tasks_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(indexed_tasks_entities_scan.pk);\n  end if;\n\n  if indexed_tasks_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(indexed_tasks_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by indexed_tasks_entities.partition_key, indexed_tasks_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "index"
        },
        "last_fire_3_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into last_fire_3_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, last_fire_3_entities_create.version, new_etag)\n    where last_fire_3_entities.partition_key = last_fire_3_entities_create.pk and last_fire_3_entities.row_key = last_fire_3_entities_create.rk;\n  else\n    insert into last_fire_3_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "hooks"
        },
        "last_fire_3_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select last_fire_3_entities.partition_key, last_fire_3_entities.row_key, last_fire_3_entities.value, last_fire_3_entities.version,\n  last_fire_3_entities.etag from last_fire_3_entities\n  where last_fire_3_entities.partition_key = last_fire_3_entities_load.partition_key and last_fire_3_entities.row_key = last_fire_3_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "hooks"
        },
        "last_fire_3_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update last_fire_3_entities\n  set (value, version, etag) = (properties, last_fire_3_entities_modify.version, new_etag)\n  where last_fire_3_entities.partition_key = last_fire_3_entities_modify.partition_key and last_fire_3_entities.row_key = last_fire_3_entities_modify.row_key and last_fire_3_entities.etag = last_fire_3_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform last_fire_3_entities.etag from last_fire_3_entities\n  where last_fire_3_entities.partition_key = last_fire_3_entities_modify.partition_key and last_fire_3_entities.row_key = last_fire_3_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "hooks"
        },
        "last_fire_3_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from last_fire_3_entities\n  where last_fire_3_entities.partition_key = last_fire_3_entities_remove.partition_key and last_fire_3_entities.row_key = last_fire_3_entities_remove.row_key\n  returning last_fire_3_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "hooks"
        },
        "last_fire_3_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select last_fire_3_entities.partition_key, last_fire_3_entities.row_key, last_fire_3_entities.value, last_fire_3_entities.version, last_fire_3_entities.etag from last_fire_3_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if last_fire_3_entities_scan.pk is not null or last_fire_3_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if last_fire_3_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(last_fire_3_entities_scan.pk);\n  end if;\n\n  if last_fire_3_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(last_fire_3_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by last_fire_3_entities.partition_key, last_fire_3_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "hooks"
        },
        "namespaces_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into namespaces_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, namespaces_entities_create.version, new_etag)\n    where namespaces_entities.partition_key = namespaces_entities_create.pk and namespaces_entities.row_key = namespaces_entities_create.rk;\n  else\n    insert into namespaces_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "index"
        },
        "namespaces_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select namespaces_entities.partition_key, namespaces_entities.row_key, namespaces_entities.value, namespaces_entities.version,\n  namespaces_entities.etag from namespaces_entities\n  where namespaces_entities.partition_key = namespaces_entities_load.partition_key and namespaces_entities.row_key = namespaces_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "index"
        },
        "namespaces_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update namespaces_entities\n  set (value, version, etag) = (properties, namespaces_entities_modify.version, new_etag)\n  where namespaces_entities.partition_key = namespaces_entities_modify.partition_key and namespaces_entities.row_key = namespaces_entities_modify.row_key and namespaces_entities.etag = namespaces_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform namespaces_entities.etag from namespaces_entities\n  where namespaces_entities.partition_key = namespaces_entities_modify.partition_key and namespaces_entities.row_key = namespaces_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "index"
        },
        "namespaces_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from namespaces_entities\n  where namespaces_entities.partition_key = namespaces_entities_remove.partition_key and namespaces_entities.row_key = namespaces_entities_remove.row_key\n  returning namespaces_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "index"
        },
        "namespaces_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select namespaces_entities.partition_key, namespaces_entities.row_key, namespaces_entities.value, namespaces_entities.version, namespaces_entities.etag from namespaces_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if namespaces_entities_scan.pk is not null or namespaces_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if namespaces_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(namespaces_entities_scan.pk);\n  end if;\n\n  if namespaces_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(namespaces_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by namespaces_entities.partition_key, namespaces_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "index"
        },
        "queue_artifacts_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into queue_artifacts_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, queue_artifacts_entities_create.version, new_etag)\n    where queue_artifacts_entities.partition_key = queue_artifacts_entities_create.pk and queue_artifacts_entities.row_key = queue_artifacts_entities_create.rk;\n  else\n    insert into queue_artifacts_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "queue_artifacts_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select queue_artifacts_entities.partition_key, queue_artifacts_entities.row_key, queue_artifacts_entities.value, queue_artifacts_entities.version,\n  queue_artifacts_entities.etag from queue_artifacts_entities\n  where queue_artifacts_entities.partition_key = queue_artifacts_entities_load.partition_key and queue_artifacts_entities.row_key = queue_artifacts_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_artifacts_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update queue_artifacts_entities\n  set (value, version, etag) = (properties, queue_artifacts_entities_modify.version, new_etag)\n  where queue_artifacts_entities.partition_key = queue_artifacts_entities_modify.partition_key and queue_artifacts_entities.row_key = queue_artifacts_entities_modify.row_key and queue_artifacts_entities.etag = queue_artifacts_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform queue_artifacts_entities.etag from queue_artifacts_entities\n  where queue_artifacts_entities.partition_key = queue_artifacts_entities_modify.partition_key and queue_artifacts_entities.row_key = queue_artifacts_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_artifacts_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from queue_artifacts_entities\n  where queue_artifacts_entities.partition_key = queue_artifacts_entities_remove.partition_key and queue_artifacts_entities.row_key = queue_artifacts_entities_remove.row_key\n  returning queue_artifacts_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_artifacts_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select queue_artifacts_entities.partition_key, queue_artifacts_entities.row_key, queue_artifacts_entities.value, queue_artifacts_entities.version, queue_artifacts_entities.etag from queue_artifacts_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if queue_artifacts_entities_scan.pk is not null or queue_artifacts_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if queue_artifacts_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(queue_artifacts_entities_scan.pk);\n  end if;\n\n  if queue_artifacts_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(queue_artifacts_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by queue_artifacts_entities.partition_key, queue_artifacts_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_provisioner_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into queue_provisioner_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, queue_provisioner_entities_create.version, new_etag)\n    where queue_provisioner_entities.partition_key = queue_provisioner_entities_create.pk and queue_provisioner_entities.row_key = queue_provisioner_entities_create.rk;\n  else\n    insert into queue_provisioner_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "queue_provisioner_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select queue_provisioner_entities.partition_key, queue_provisioner_entities.row_key, queue_provisioner_entities.value, queue_provisioner_entities.version,\n  queue_provisioner_entities.etag from queue_provisioner_entities\n  where queue_provisioner_entities.partition_key = queue_provisioner_entities_load.partition_key and queue_provisioner_entities.row_key = queue_provisioner_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_provisioner_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update queue_provisioner_entities\n  set (value, version, etag) = (properties, queue_provisioner_entities_modify.version, new_etag)\n  where queue_provisioner_entities.partition_key = queue_provisioner_entities_modify.partition_key and queue_provisioner_entities.row_key = queue_provisioner_entities_modify.row_key and queue_provisioner_entities.etag = queue_provisioner_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform queue_provisioner_entities.etag from queue_provisioner_entities\n  where queue_provisioner_entities.partition_key = queue_provisioner_entities_modify.partition_key and queue_provisioner_entities.row_key = queue_provisioner_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_provisioner_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from queue_provisioner_entities\n  where queue_provisioner_entities.partition_key = queue_provisioner_entities_remove.partition_key and queue_provisioner_entities.row_key = queue_provisioner_entities_remove.row_key\n  returning queue_provisioner_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_provisioner_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select queue_provisioner_entities.partition_key, queue_provisioner_entities.row_key, queue_provisioner_entities.value, queue_provisioner_entities.version, queue_provisioner_entities.etag from queue_provisioner_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if queue_provisioner_entities_scan.pk is not null or queue_provisioner_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if queue_provisioner_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(queue_provisioner_entities_scan.pk);\n  end if;\n\n  if queue_provisioner_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(queue_provisioner_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by queue_provisioner_entities.partition_key, queue_provisioner_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_dependency_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into queue_task_dependency_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, queue_task_dependency_entities_create.version, new_etag)\n    where queue_task_dependency_entities.partition_key = queue_task_dependency_entities_create.pk and queue_task_dependency_entities.row_key = queue_task_dependency_entities_create.rk;\n  else\n    insert into queue_task_dependency_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "queue_task_dependency_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select queue_task_dependency_entities.partition_key, queue_task_dependency_entities.row_key, queue_task_dependency_entities.value, queue_task_dependency_entities.version,\n  queue_task_dependency_entities.etag from queue_task_dependency_entities\n  where queue_task_dependency_entities.partition_key = queue_task_dependency_entities_load.partition_key and queue_task_dependency_entities.row_key = queue_task_dependency_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_dependency_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update queue_task_dependency_entities\n  set (value, version, etag) = (properties, queue_task_dependency_entities_modify.version, new_etag)\n  where queue_task_dependency_entities.partition_key = queue_task_dependency_entities_modify.partition_key and queue_task_dependency_entities.row_key = queue_task_dependency_entities_modify.row_key and queue_task_dependency_entities.etag = queue_task_dependency_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform queue_task_dependency_entities.etag from queue_task_dependency_entities\n  where queue_task_dependency_entities.partition_key = queue_task_dependency_entities_modify.partition_key and queue_task_dependency_entities.row_key = queue_task_dependency_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_dependency_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from queue_task_dependency_entities\n  where queue_task_dependency_entities.partition_key = queue_task_dependency_entities_remove.partition_key and queue_task_dependency_entities.row_key = queue_task_dependency_entities_remove.row_key\n  returning queue_task_dependency_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_dependency_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select queue_task_dependency_entities.partition_key, queue_task_dependency_entities.row_key, queue_task_dependency_entities.value, queue_task_dependency_entities.version, queue_task_dependency_entities.etag from queue_task_dependency_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if queue_task_dependency_entities_scan.pk is not null or queue_task_dependency_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if queue_task_dependency_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(queue_task_dependency_entities_scan.pk);\n  end if;\n\n  if queue_task_dependency_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(queue_task_dependency_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by queue_task_dependency_entities.partition_key, queue_task_dependency_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_group_active_sets_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into queue_task_group_active_sets_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, queue_task_group_active_sets_entities_create.version, new_etag)\n    where queue_task_group_active_sets_entities.partition_key = queue_task_group_active_sets_entities_create.pk and queue_task_group_active_sets_entities.row_key = queue_task_group_active_sets_entities_create.rk;\n  else\n    insert into queue_task_group_active_sets_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "queue_task_group_active_sets_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select queue_task_group_active_sets_entities.partition_key, queue_task_group_active_sets_entities.row_key, queue_task_group_active_sets_entities.value, queue_task_group_active_sets_entities.version,\n  queue_task_group_active_sets_entities.etag from queue_task_group_active_sets_entities\n  where queue_task_group_active_sets_entities.partition_key = queue_task_group_active_sets_entities_load.partition_key and queue_task_group_active_sets_entities.row_key = queue_task_group_active_sets_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_group_active_sets_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update queue_task_group_active_sets_entities\n  set (value, version, etag) = (properties, queue_task_group_active_sets_entities_modify.version, new_etag)\n  where queue_task_group_active_sets_entities.partition_key = queue_task_group_active_sets_entities_modify.partition_key and queue_task_group_active_sets_entities.row_key = queue_task_group_active_sets_entities_modify.row_key and queue_task_group_active_sets_entities.etag = queue_task_group_active_sets_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform queue_task_group_active_sets_entities.etag from queue_task_group_active_sets_entities\n  where queue_task_group_active_sets_entities.partition_key = queue_task_group_active_sets_entities_modify.partition_key and queue_task_group_active_sets_entities.row_key = queue_task_group_active_sets_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_group_active_sets_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from queue_task_group_active_sets_entities\n  where queue_task_group_active_sets_entities.partition_key = queue_task_group_active_sets_entities_remove.partition_key and queue_task_group_active_sets_entities.row_key = queue_task_group_active_sets_entities_remove.row_key\n  returning queue_task_group_active_sets_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_group_active_sets_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select queue_task_group_active_sets_entities.partition_key, queue_task_group_active_sets_entities.row_key, queue_task_group_active_sets_entities.value, queue_task_group_active_sets_entities.version, queue_task_group_active_sets_entities.etag from queue_task_group_active_sets_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if queue_task_group_active_sets_entities_scan.pk is not null or queue_task_group_active_sets_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if queue_task_group_active_sets_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(queue_task_group_active_sets_entities_scan.pk);\n  end if;\n\n  if queue_task_group_active_sets_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(queue_task_group_active_sets_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by queue_task_group_active_sets_entities.partition_key, queue_task_group_active_sets_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_group_members_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into queue_task_group_members_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, queue_task_group_members_entities_create.version, new_etag)\n    where queue_task_group_members_entities.partition_key = queue_task_group_members_entities_create.pk and queue_task_group_members_entities.row_key = queue_task_group_members_entities_create.rk;\n  else\n    insert into queue_task_group_members_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "queue_task_group_members_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select queue_task_group_members_entities.partition_key, queue_task_group_members_entities.row_key, queue_task_group_members_entities.value, queue_task_group_members_entities.version,\n  queue_task_group_members_entities.etag from queue_task_group_members_entities\n  where queue_task_group_members_entities.partition_key = queue_task_group_members_entities_load.partition_key and queue_task_group_members_entities.row_key = queue_task_group_members_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_group_members_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update queue_task_group_members_entities\n  set (value, version, etag) = (properties, queue_task_group_members_entities_modify.version, new_etag)\n  where queue_task_group_members_entities.partition_key = queue_task_group_members_entities_modify.partition_key and queue_task_group_members_entities.row_key = queue_task_group_members_entities_modify.row_key and queue_task_group_members_entities.etag = queue_task_group_members_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform queue_task_group_members_entities.etag from queue_task_group_members_entities\n  where queue_task_group_members_entities.partition_key = queue_task_group_members_entities_modify.partition_key and queue_task_group_members_entities.row_key = queue_task_group_members_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_group_members_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from queue_task_group_members_entities\n  where queue_task_group_members_entities.partition_key = queue_task_group_members_entities_remove.partition_key and queue_task_group_members_entities.row_key = queue_task_group_members_entities_remove.row_key\n  returning queue_task_group_members_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_group_members_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select queue_task_group_members_entities.partition_key, queue_task_group_members_entities.row_key, queue_task_group_members_entities.value, queue_task_group_members_entities.version, queue_task_group_members_entities.etag from queue_task_group_members_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if queue_task_group_members_entities_scan.pk is not null or queue_task_group_members_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if queue_task_group_members_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(queue_task_group_members_entities_scan.pk);\n  end if;\n\n  if queue_task_group_members_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(queue_task_group_members_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by queue_task_group_members_entities.partition_key, queue_task_group_members_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_groups_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into queue_task_groups_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, queue_task_groups_entities_create.version, new_etag)\n    where queue_task_groups_entities.partition_key = queue_task_groups_entities_create.pk and queue_task_groups_entities.row_key = queue_task_groups_entities_create.rk;\n  else\n    insert into queue_task_groups_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "queue_task_groups_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select queue_task_groups_entities.partition_key, queue_task_groups_entities.row_key, queue_task_groups_entities.value, queue_task_groups_entities.version,\n  queue_task_groups_entities.etag from queue_task_groups_entities\n  where queue_task_groups_entities.partition_key = queue_task_groups_entities_load.partition_key and queue_task_groups_entities.row_key = queue_task_groups_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_groups_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update queue_task_groups_entities\n  set (value, version, etag) = (properties, queue_task_groups_entities_modify.version, new_etag)\n  where queue_task_groups_entities.partition_key = queue_task_groups_entities_modify.partition_key and queue_task_groups_entities.row_key = queue_task_groups_entities_modify.row_key and queue_task_groups_entities.etag = queue_task_groups_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform queue_task_groups_entities.etag from queue_task_groups_entities\n  where queue_task_groups_entities.partition_key = queue_task_groups_entities_modify.partition_key and queue_task_groups_entities.row_key = queue_task_groups_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_groups_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from queue_task_groups_entities\n  where queue_task_groups_entities.partition_key = queue_task_groups_entities_remove.partition_key and queue_task_groups_entities.row_key = queue_task_groups_entities_remove.row_key\n  returning queue_task_groups_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_groups_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select queue_task_groups_entities.partition_key, queue_task_groups_entities.row_key, queue_task_groups_entities.value, queue_task_groups_entities.version, queue_task_groups_entities.etag from queue_task_groups_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if queue_task_groups_entities_scan.pk is not null or queue_task_groups_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if queue_task_groups_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(queue_task_groups_entities_scan.pk);\n  end if;\n\n  if queue_task_groups_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(queue_task_groups_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by queue_task_groups_entities.partition_key, queue_task_groups_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_requirement_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into queue_task_requirement_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, queue_task_requirement_entities_create.version, new_etag)\n    where queue_task_requirement_entities.partition_key = queue_task_requirement_entities_create.pk and queue_task_requirement_entities.row_key = queue_task_requirement_entities_create.rk;\n  else\n    insert into queue_task_requirement_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "queue_task_requirement_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select queue_task_requirement_entities.partition_key, queue_task_requirement_entities.row_key, queue_task_requirement_entities.value, queue_task_requirement_entities.version,\n  queue_task_requirement_entities.etag from queue_task_requirement_entities\n  where queue_task_requirement_entities.partition_key = queue_task_requirement_entities_load.partition_key and queue_task_requirement_entities.row_key = queue_task_requirement_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_requirement_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update queue_task_requirement_entities\n  set (value, version, etag) = (properties, queue_task_requirement_entities_modify.version, new_etag)\n  where queue_task_requirement_entities.partition_key = queue_task_requirement_entities_modify.partition_key and queue_task_requirement_entities.row_key = queue_task_requirement_entities_modify.row_key and queue_task_requirement_entities.etag = queue_task_requirement_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform queue_task_requirement_entities.etag from queue_task_requirement_entities\n  where queue_task_requirement_entities.partition_key = queue_task_requirement_entities_modify.partition_key and queue_task_requirement_entities.row_key = queue_task_requirement_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_requirement_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from queue_task_requirement_entities\n  where queue_task_requirement_entities.partition_key = queue_task_requirement_entities_remove.partition_key and queue_task_requirement_entities.row_key = queue_task_requirement_entities_remove.row_key\n  returning queue_task_requirement_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_requirement_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select queue_task_requirement_entities.partition_key, queue_task_requirement_entities.row_key, queue_task_requirement_entities.value, queue_task_requirement_entities.version, queue_task_requirement_entities.etag from queue_task_requirement_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if queue_task_requirement_entities_scan.pk is not null or queue_task_requirement_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if queue_task_requirement_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(queue_task_requirement_entities_scan.pk);\n  end if;\n\n  if queue_task_requirement_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(queue_task_requirement_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by queue_task_requirement_entities.partition_key, queue_task_requirement_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_tasks_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into queue_tasks_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, queue_tasks_entities_create.version, new_etag)\n    where queue_tasks_entities.partition_key = queue_tasks_entities_create.pk and queue_tasks_entities.row_key = queue_tasks_entities_create.rk;\n  else\n    insert into queue_tasks_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "queue_tasks_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select queue_tasks_entities.partition_key, queue_tasks_entities.row_key, queue_tasks_entities.value, queue_tasks_entities.version,\n  queue_tasks_entities.etag from queue_tasks_entities\n  where queue_tasks_entities.partition_key = queue_tasks_entities_load.partition_key and queue_tasks_entities.row_key = queue_tasks_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_tasks_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update queue_tasks_entities\n  set (value, version, etag) = (properties, queue_tasks_entities_modify.version, new_etag)\n  where queue_tasks_entities.partition_key = queue_tasks_entities_modify.partition_key and queue_tasks_entities.row_key = queue_tasks_entities_modify.row_key and queue_tasks_entities.etag = queue_tasks_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform queue_tasks_entities.etag from queue_tasks_entities\n  where queue_tasks_entities.partition_key = queue_tasks_entities_modify.partition_key and queue_tasks_entities.row_key = queue_tasks_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_tasks_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from queue_tasks_entities\n  where queue_tasks_entities.partition_key = queue_tasks_entities_remove.partition_key and queue_tasks_entities.row_key = queue_tasks_entities_remove.row_key\n  returning queue_tasks_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_tasks_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select queue_tasks_entities.partition_key, queue_tasks_entities.row_key, queue_tasks_entities.value, queue_tasks_entities.version, queue_tasks_entities.etag from queue_tasks_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if queue_tasks_entities_scan.pk is not null or queue_tasks_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if queue_tasks_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(queue_tasks_entities_scan.pk);\n  end if;\n\n  if queue_tasks_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(queue_tasks_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by queue_tasks_entities.partition_key, queue_tasks_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_worker_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into queue_worker_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, queue_worker_entities_create.version, new_etag)\n    where queue_worker_entities.partition_key = queue_worker_entities_create.pk and queue_worker_entities.row_key = queue_worker_entities_create.rk;\n  else\n    insert into queue_worker_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "queue_worker_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select queue_worker_entities.partition_key, queue_worker_entities.row_key, queue_worker_entities.value, queue_worker_entities.version,\n  queue_worker_entities.etag from queue_worker_entities\n  where queue_worker_entities.partition_key = queue_worker_entities_load.partition_key and queue_worker_entities.row_key = queue_worker_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_worker_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update queue_worker_entities\n  set (value, version, etag) = (properties, queue_worker_entities_modify.version, new_etag)\n  where queue_worker_entities.partition_key = queue_worker_entities_modify.partition_key and queue_worker_entities.row_key = queue_worker_entities_modify.row_key and queue_worker_entities.etag = queue_worker_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform queue_worker_entities.etag from queue_worker_entities\n  where queue_worker_entities.partition_key = queue_worker_entities_modify.partition_key and queue_worker_entities.row_key = queue_worker_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_worker_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from queue_worker_entities\n  where queue_worker_entities.partition_key = queue_worker_entities_remove.partition_key and queue_worker_entities.row_key = queue_worker_entities_remove.row_key\n  returning queue_worker_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_worker_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select queue_worker_entities.partition_key, queue_worker_entities.row_key, queue_worker_entities.value, queue_worker_entities.version, queue_worker_entities.etag from queue_worker_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if queue_worker_entities_scan.pk is not null or queue_worker_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if queue_worker_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(queue_worker_entities_scan.pk);\n  end if;\n\n  if queue_worker_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(queue_worker_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by queue_worker_entities.partition_key, queue_worker_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_worker_type_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into queue_worker_type_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, queue_worker_type_entities_create.version, new_etag)\n    where queue_worker_type_entities.partition_key = queue_worker_type_entities_create.pk and queue_worker_type_entities.row_key = queue_worker_type_entities_create.rk;\n  else\n    insert into queue_worker_type_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "queue_worker_type_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select queue_worker_type_entities.partition_key, queue_worker_type_entities.row_key, queue_worker_type_entities.value, queue_worker_type_entities.version,\n  queue_worker_type_entities.etag from queue_worker_type_entities\n  where queue_worker_type_entities.partition_key = queue_worker_type_entities_load.partition_key and queue_worker_type_entities.row_key = queue_worker_type_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_worker_type_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update queue_worker_type_entities\n  set (value, version, etag) = (properties, queue_worker_type_entities_modify.version, new_etag)\n  where queue_worker_type_entities.partition_key = queue_worker_type_entities_modify.partition_key and queue_worker_type_entities.row_key = queue_worker_type_entities_modify.row_key and queue_worker_type_entities.etag = queue_worker_type_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform queue_worker_type_entities.etag from queue_worker_type_entities\n  where queue_worker_type_entities.partition_key = queue_worker_type_entities_modify.partition_key and queue_worker_type_entities.row_key = queue_worker_type_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_worker_type_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from queue_worker_type_entities\n  where queue_worker_type_entities.partition_key = queue_worker_type_entities_remove.partition_key and queue_worker_type_entities.row_key = queue_worker_type_entities_remove.row_key\n  returning queue_worker_type_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_worker_type_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select queue_worker_type_entities.partition_key, queue_worker_type_entities.row_key, queue_worker_type_entities.value, queue_worker_type_entities.version, queue_worker_type_entities.etag from queue_worker_type_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if queue_worker_type_entities_scan.pk is not null or queue_worker_type_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if queue_worker_type_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(queue_worker_type_entities_scan.pk);\n  end if;\n\n  if queue_worker_type_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(queue_worker_type_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by queue_worker_type_entities.partition_key, queue_worker_type_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queues_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into queues_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, queues_entities_create.version, new_etag)\n    where queues_entities.partition_key = queues_entities_create.pk and queues_entities.row_key = queues_entities_create.rk;\n  else\n    insert into queues_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "hooks"
        },
        "queues_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select queues_entities.partition_key, queues_entities.row_key, queues_entities.value, queues_entities.version,\n  queues_entities.etag from queues_entities\n  where queues_entities.partition_key = queues_entities_load.partition_key and queues_entities.row_key = queues_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "hooks"
        },
        "queues_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update queues_entities\n  set (value, version, etag) = (properties, queues_entities_modify.version, new_etag)\n  where queues_entities.partition_key = queues_entities_modify.partition_key and queues_entities.row_key = queues_entities_modify.row_key and queues_entities.etag = queues_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform queues_entities.etag from queues_entities\n  where queues_entities.partition_key = queues_entities_modify.partition_key and queues_entities.row_key = queues_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "hooks"
        },
        "queues_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from queues_entities\n  where queues_entities.partition_key = queues_entities_remove.partition_key and queues_entities.row_key = queues_entities_remove.row_key\n  returning queues_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "hooks"
        },
        "queues_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select queues_entities.partition_key, queues_entities.row_key, queues_entities.value, queues_entities.version, queues_entities.etag from queues_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if queues_entities_scan.pk is not null or queues_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if queues_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(queues_entities_scan.pk);\n  end if;\n\n  if queues_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(queues_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by queues_entities.partition_key, queues_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "hooks"
        },
        "roles_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into roles_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, roles_entities_create.version, new_etag)\n    where roles_entities.partition_key = roles_entities_create.pk and roles_entities.row_key = roles_entities_create.rk;\n  else\n    insert into roles_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "auth"
        },
        "roles_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select roles_entities.partition_key, roles_entities.row_key, roles_entities.value, roles_entities.version,\n  roles_entities.etag from roles_entities\n  where roles_entities.partition_key = roles_entities_load.partition_key and roles_entities.row_key = roles_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "auth"
        },
        "roles_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update roles_entities\n  set (value, version, etag) = (properties, roles_entities_modify.version, new_etag)\n  where roles_entities.partition_key = roles_entities_modify.partition_key and roles_entities.row_key = roles_entities_modify.row_key and roles_entities.etag = roles_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform roles_entities.etag from roles_entities\n  where roles_entities.partition_key = roles_entities_modify.partition_key and roles_entities.row_key = roles_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "auth"
        },
        "roles_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from roles_entities\n  where roles_entities.partition_key = roles_entities_remove.partition_key and roles_entities.row_key = roles_entities_remove.row_key\n  returning roles_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "auth"
        },
        "roles_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select roles_entities.partition_key, roles_entities.row_key, roles_entities.value, roles_entities.version, roles_entities.etag from roles_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if roles_entities_scan.pk is not null or roles_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if roles_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(roles_entities_scan.pk);\n  end if;\n\n  if roles_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(roles_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by roles_entities.partition_key, roles_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "auth"
        },
        "secrets_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into secrets_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, secrets_entities_create.version, new_etag)\n    where secrets_entities.partition_key = secrets_entities_create.pk and secrets_entities.row_key = secrets_entities_create.rk;\n  else\n    insert into secrets_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "secrets"
        },
        "secrets_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select secrets_entities.partition_key, secrets_entities.row_key, secrets_entities.value, secrets_entities.version,\n  secrets_entities.etag from secrets_entities\n  where secrets_entities.partition_key = secrets_entities_load.partition_key and secrets_entities.row_key = secrets_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "secrets"
        },
        "secrets_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update secrets_entities\n  set (value, version, etag) = (properties, secrets_entities_modify.version, new_etag)\n  where secrets_entities.partition_key = secrets_entities_modify.partition_key and secrets_entities.row_key = secrets_entities_modify.row_key and secrets_entities.etag = secrets_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform secrets_entities.etag from secrets_entities\n  where secrets_entities.partition_key = secrets_entities_modify.partition_key and secrets_entities.row_key = secrets_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "secrets"
        },
        "secrets_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from secrets_entities\n  where secrets_entities.partition_key = secrets_entities_remove.partition_key and secrets_entities.row_key = secrets_entities_remove.row_key\n  returning secrets_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "secrets"
        },
        "secrets_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select secrets_entities.partition_key, secrets_entities.row_key, secrets_entities.value, secrets_entities.version, secrets_entities.etag from secrets_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if secrets_entities_scan.pk is not null or secrets_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if secrets_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(secrets_entities_scan.pk);\n  end if;\n\n  if secrets_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(secrets_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by secrets_entities.partition_key, secrets_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "secrets"
        },
        "session_storage_table_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into session_storage_table_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, session_storage_table_entities_create.version, new_etag)\n    where session_storage_table_entities.partition_key = session_storage_table_entities_create.pk and session_storage_table_entities.row_key = session_storage_table_entities_create.rk;\n  else\n    insert into session_storage_table_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "web_server"
        },
        "session_storage_table_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select session_storage_table_entities.partition_key, session_storage_table_entities.row_key, session_storage_table_entities.value, session_storage_table_entities.version,\n  session_storage_table_entities.etag from session_storage_table_entities\n  where session_storage_table_entities.partition_key = session_storage_table_entities_load.partition_key and session_storage_table_entities.row_key = session_storage_table_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "web_server"
        },
        "session_storage_table_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update session_storage_table_entities\n  set (value, version, etag) = (properties, session_storage_table_entities_modify.version, new_etag)\n  where session_storage_table_entities.partition_key = session_storage_table_entities_modify.partition_key and session_storage_table_entities.row_key = session_storage_table_entities_modify.row_key and session_storage_table_entities.etag = session_storage_table_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform session_storage_table_entities.etag from session_storage_table_entities\n  where session_storage_table_entities.partition_key = session_storage_table_entities_modify.partition_key and session_storage_table_entities.row_key = session_storage_table_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "web_server"
        },
        "session_storage_table_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from session_storage_table_entities\n  where session_storage_table_entities.partition_key = session_storage_table_entities_remove.partition_key and session_storage_table_entities.row_key = session_storage_table_entities_remove.row_key\n  returning session_storage_table_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "web_server"
        },
        "session_storage_table_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select session_storage_table_entities.partition_key, session_storage_table_entities.row_key, session_storage_table_entities.value, session_storage_table_entities.version, session_storage_table_entities.etag from session_storage_table_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if session_storage_table_entities_scan.pk is not null or session_storage_table_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if session_storage_table_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(session_storage_table_entities_scan.pk);\n  end if;\n\n  if session_storage_table_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(session_storage_table_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by session_storage_table_entities.partition_key, session_storage_table_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "web_server"
        },
        "taskcluster_check_runs_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into taskcluster_check_runs_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, taskcluster_check_runs_entities_create.version, new_etag)\n    where taskcluster_check_runs_entities.partition_key = taskcluster_check_runs_entities_create.pk and taskcluster_check_runs_entities.row_key = taskcluster_check_runs_entities_create.rk;\n  else\n    insert into taskcluster_check_runs_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "github"
        },
        "taskcluster_check_runs_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select taskcluster_check_runs_entities.partition_key, taskcluster_check_runs_entities.row_key, taskcluster_check_runs_entities.value, taskcluster_check_runs_entities.version,\n  taskcluster_check_runs_entities.etag from taskcluster_check_runs_entities\n  where taskcluster_check_runs_entities.partition_key = taskcluster_check_runs_entities_load.partition_key and taskcluster_check_runs_entities.row_key = taskcluster_check_runs_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_check_runs_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update taskcluster_check_runs_entities\n  set (value, version, etag) = (properties, taskcluster_check_runs_entities_modify.version, new_etag)\n  where taskcluster_check_runs_entities.partition_key = taskcluster_check_runs_entities_modify.partition_key and taskcluster_check_runs_entities.row_key = taskcluster_check_runs_entities_modify.row_key and taskcluster_check_runs_entities.etag = taskcluster_check_runs_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform taskcluster_check_runs_entities.etag from taskcluster_check_runs_entities\n  where taskcluster_check_runs_entities.partition_key = taskcluster_check_runs_entities_modify.partition_key and taskcluster_check_runs_entities.row_key = taskcluster_check_runs_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_check_runs_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from taskcluster_check_runs_entities\n  where taskcluster_check_runs_entities.partition_key = taskcluster_check_runs_entities_remove.partition_key and taskcluster_check_runs_entities.row_key = taskcluster_check_runs_entities_remove.row_key\n  returning taskcluster_check_runs_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_check_runs_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select taskcluster_check_runs_entities.partition_key, taskcluster_check_runs_entities.row_key, taskcluster_check_runs_entities.value, taskcluster_check_runs_entities.version, taskcluster_check_runs_entities.etag from taskcluster_check_runs_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if taskcluster_check_runs_entities_scan.pk is not null or taskcluster_check_runs_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if taskcluster_check_runs_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(taskcluster_check_runs_entities_scan.pk);\n  end if;\n\n  if taskcluster_check_runs_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(taskcluster_check_runs_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by taskcluster_check_runs_entities.partition_key, taskcluster_check_runs_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_checks_to_tasks_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into taskcluster_checks_to_tasks_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, taskcluster_checks_to_tasks_entities_create.version, new_etag)\n    where taskcluster_checks_to_tasks_entities.partition_key = taskcluster_checks_to_tasks_entities_create.pk and taskcluster_checks_to_tasks_entities.row_key = taskcluster_checks_to_tasks_entities_create.rk;\n  else\n    insert into taskcluster_checks_to_tasks_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "github"
        },
        "taskcluster_checks_to_tasks_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select taskcluster_checks_to_tasks_entities.partition_key, taskcluster_checks_to_tasks_entities.row_key, taskcluster_checks_to_tasks_entities.value, taskcluster_checks_to_tasks_entities.version,\n  taskcluster_checks_to_tasks_entities.etag from taskcluster_checks_to_tasks_entities\n  where taskcluster_checks_to_tasks_entities.partition_key = taskcluster_checks_to_tasks_entities_load.partition_key and taskcluster_checks_to_tasks_entities.row_key = taskcluster_checks_to_tasks_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_checks_to_tasks_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update taskcluster_checks_to_tasks_entities\n  set (value, version, etag) = (properties, taskcluster_checks_to_tasks_entities_modify.version, new_etag)\n  where taskcluster_checks_to_tasks_entities.partition_key = taskcluster_checks_to_tasks_entities_modify.partition_key and taskcluster_checks_to_tasks_entities.row_key = taskcluster_checks_to_tasks_entities_modify.row_key and taskcluster_checks_to_tasks_entities.etag = taskcluster_checks_to_tasks_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform taskcluster_checks_to_tasks_entities.etag from taskcluster_checks_to_tasks_entities\n  where taskcluster_checks_to_tasks_entities.partition_key = taskcluster_checks_to_tasks_entities_modify.partition_key and taskcluster_checks_to_tasks_entities.row_key = taskcluster_checks_to_tasks_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_checks_to_tasks_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from taskcluster_checks_to_tasks_entities\n  where taskcluster_checks_to_tasks_entities.partition_key = taskcluster_checks_to_tasks_entities_remove.partition_key and taskcluster_checks_to_tasks_entities.row_key = taskcluster_checks_to_tasks_entities_remove.row_key\n  returning taskcluster_checks_to_tasks_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_checks_to_tasks_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select taskcluster_checks_to_tasks_entities.partition_key, taskcluster_checks_to_tasks_entities.row_key, taskcluster_checks_to_tasks_entities.value, taskcluster_checks_to_tasks_entities.version, taskcluster_checks_to_tasks_entities.etag from taskcluster_checks_to_tasks_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if taskcluster_checks_to_tasks_entities_scan.pk is not null or taskcluster_checks_to_tasks_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if taskcluster_checks_to_tasks_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(taskcluster_checks_to_tasks_entities_scan.pk);\n  end if;\n\n  if taskcluster_checks_to_tasks_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(taskcluster_checks_to_tasks_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by taskcluster_checks_to_tasks_entities.partition_key, taskcluster_checks_to_tasks_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_github_builds_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into taskcluster_github_builds_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, taskcluster_github_builds_entities_create.version, new_etag)\n    where taskcluster_github_builds_entities.partition_key = taskcluster_github_builds_entities_create.pk and taskcluster_github_builds_entities.row_key = taskcluster_github_builds_entities_create.rk;\n  else\n    insert into taskcluster_github_builds_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "github"
        },
        "taskcluster_github_builds_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select taskcluster_github_builds_entities.partition_key, taskcluster_github_builds_entities.row_key, taskcluster_github_builds_entities.value, taskcluster_github_builds_entities.version,\n  taskcluster_github_builds_entities.etag from taskcluster_github_builds_entities\n  where taskcluster_github_builds_entities.partition_key = taskcluster_github_builds_entities_load.partition_key and taskcluster_github_builds_entities.row_key = taskcluster_github_builds_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_github_builds_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update taskcluster_github_builds_entities\n  set (value, version, etag) = (properties, taskcluster_github_builds_entities_modify.version, new_etag)\n  where taskcluster_github_builds_entities.partition_key = taskcluster_github_builds_entities_modify.partition_key and taskcluster_github_builds_entities.row_key = taskcluster_github_builds_entities_modify.row_key and taskcluster_github_builds_entities.etag = taskcluster_github_builds_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform taskcluster_github_builds_entities.etag from taskcluster_github_builds_entities\n  where taskcluster_github_builds_entities.partition_key = taskcluster_github_builds_entities_modify.partition_key and taskcluster_github_builds_entities.row_key = taskcluster_github_builds_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_github_builds_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from taskcluster_github_builds_entities\n  where taskcluster_github_builds_entities.partition_key = taskcluster_github_builds_entities_remove.partition_key and taskcluster_github_builds_entities.row_key = taskcluster_github_builds_entities_remove.row_key\n  returning taskcluster_github_builds_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_github_builds_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select taskcluster_github_builds_entities.partition_key, taskcluster_github_builds_entities.row_key, taskcluster_github_builds_entities.value, taskcluster_github_builds_entities.version, taskcluster_github_builds_entities.etag from taskcluster_github_builds_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if taskcluster_github_builds_entities_scan.pk is not null or taskcluster_github_builds_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if taskcluster_github_builds_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(taskcluster_github_builds_entities_scan.pk);\n  end if;\n\n  if taskcluster_github_builds_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(taskcluster_github_builds_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by taskcluster_github_builds_entities.partition_key, taskcluster_github_builds_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_integration_owners_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into taskcluster_integration_owners_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, taskcluster_integration_owners_entities_create.version, new_etag)\n    where taskcluster_integration_owners_entities.partition_key = taskcluster_integration_owners_entities_create.pk and taskcluster_integration_owners_entities.row_key = taskcluster_integration_owners_entities_create.rk;\n  else\n    insert into taskcluster_integration_owners_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "github"
        },
        "taskcluster_integration_owners_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select taskcluster_integration_owners_entities.partition_key, taskcluster_integration_owners_entities.row_key, taskcluster_integration_owners_entities.value, taskcluster_integration_owners_entities.version,\n  taskcluster_integration_owners_entities.etag from taskcluster_integration_owners_entities\n  where taskcluster_integration_owners_entities.partition_key = taskcluster_integration_owners_entities_load.partition_key and taskcluster_integration_owners_entities.row_key = taskcluster_integration_owners_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_integration_owners_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update taskcluster_integration_owners_entities\n  set (value, version, etag) = (properties, taskcluster_integration_owners_entities_modify.version, new_etag)\n  where taskcluster_integration_owners_entities.partition_key = taskcluster_integration_owners_entities_modify.partition_key and taskcluster_integration_owners_entities.row_key = taskcluster_integration_owners_entities_modify.row_key and taskcluster_integration_owners_entities.etag = taskcluster_integration_owners_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform taskcluster_integration_owners_entities.etag from taskcluster_integration_owners_entities\n  where taskcluster_integration_owners_entities.partition_key = taskcluster_integration_owners_entities_modify.partition_key and taskcluster_integration_owners_entities.row_key = taskcluster_integration_owners_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_integration_owners_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from taskcluster_integration_owners_entities\n  where taskcluster_integration_owners_entities.partition_key = taskcluster_integration_owners_entities_remove.partition_key and taskcluster_integration_owners_entities.row_key = taskcluster_integration_owners_entities_remove.row_key\n  returning taskcluster_integration_owners_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_integration_owners_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select taskcluster_integration_owners_entities.partition_key, taskcluster_integration_owners_entities.row_key, taskcluster_integration_owners_entities.value, taskcluster_integration_owners_entities.version, taskcluster_integration_owners_entities.etag from taskcluster_integration_owners_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if taskcluster_integration_owners_entities_scan.pk is not null or taskcluster_integration_owners_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if taskcluster_integration_owners_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(taskcluster_integration_owners_entities_scan.pk);\n  end if;\n\n  if taskcluster_integration_owners_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(taskcluster_integration_owners_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by taskcluster_integration_owners_entities.partition_key, taskcluster_integration_owners_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "github"
        },
        "wmworker_pool_errors_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into wmworker_pool_errors_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, wmworker_pool_errors_entities_create.version, new_etag)\n    where wmworker_pool_errors_entities.partition_key = wmworker_pool_errors_entities_create.pk and wmworker_pool_errors_entities.row_key = wmworker_pool_errors_entities_create.rk;\n  else\n    insert into wmworker_pool_errors_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "worker_manager"
        },
        "wmworker_pool_errors_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select wmworker_pool_errors_entities.partition_key, wmworker_pool_errors_entities.row_key, wmworker_pool_errors_entities.value, wmworker_pool_errors_entities.version,\n  wmworker_pool_errors_entities.etag from wmworker_pool_errors_entities\n  where wmworker_pool_errors_entities.partition_key = wmworker_pool_errors_entities_load.partition_key and wmworker_pool_errors_entities.row_key = wmworker_pool_errors_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworker_pool_errors_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update wmworker_pool_errors_entities\n  set (value, version, etag) = (properties, wmworker_pool_errors_entities_modify.version, new_etag)\n  where wmworker_pool_errors_entities.partition_key = wmworker_pool_errors_entities_modify.partition_key and wmworker_pool_errors_entities.row_key = wmworker_pool_errors_entities_modify.row_key and wmworker_pool_errors_entities.etag = wmworker_pool_errors_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform wmworker_pool_errors_entities.etag from wmworker_pool_errors_entities\n  where wmworker_pool_errors_entities.partition_key = wmworker_pool_errors_entities_modify.partition_key and wmworker_pool_errors_entities.row_key = wmworker_pool_errors_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworker_pool_errors_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from wmworker_pool_errors_entities\n  where wmworker_pool_errors_entities.partition_key = wmworker_pool_errors_entities_remove.partition_key and wmworker_pool_errors_entities.row_key = wmworker_pool_errors_entities_remove.row_key\n  returning wmworker_pool_errors_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworker_pool_errors_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select wmworker_pool_errors_entities.partition_key, wmworker_pool_errors_entities.row_key, wmworker_pool_errors_entities.value, wmworker_pool_errors_entities.version, wmworker_pool_errors_entities.etag from wmworker_pool_errors_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if wmworker_pool_errors_entities_scan.pk is not null or wmworker_pool_errors_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if wmworker_pool_errors_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(wmworker_pool_errors_entities_scan.pk);\n  end if;\n\n  if wmworker_pool_errors_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(wmworker_pool_errors_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by wmworker_pool_errors_entities.partition_key, wmworker_pool_errors_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworker_pools_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into wmworker_pools_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, wmworker_pools_entities_create.version, new_etag)\n    where wmworker_pools_entities.partition_key = wmworker_pools_entities_create.pk and wmworker_pools_entities.row_key = wmworker_pools_entities_create.rk;\n  else\n    insert into wmworker_pools_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "worker_manager"
        },
        "wmworker_pools_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select wmworker_pools_entities.partition_key, wmworker_pools_entities.row_key, wmworker_pools_entities.value, wmworker_pools_entities.version,\n  wmworker_pools_entities.etag from wmworker_pools_entities\n  where wmworker_pools_entities.partition_key = wmworker_pools_entities_load.partition_key and wmworker_pools_entities.row_key = wmworker_pools_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworker_pools_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update wmworker_pools_entities\n  set (value, version, etag) = (properties, wmworker_pools_entities_modify.version, new_etag)\n  where wmworker_pools_entities.partition_key = wmworker_pools_entities_modify.partition_key and wmworker_pools_entities.row_key = wmworker_pools_entities_modify.row_key and wmworker_pools_entities.etag = wmworker_pools_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform wmworker_pools_entities.etag from wmworker_pools_entities\n  where wmworker_pools_entities.partition_key = wmworker_pools_entities_modify.partition_key and wmworker_pools_entities.row_key = wmworker_pools_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworker_pools_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from wmworker_pools_entities\n  where wmworker_pools_entities.partition_key = wmworker_pools_entities_remove.partition_key and wmworker_pools_entities.row_key = wmworker_pools_entities_remove.row_key\n  returning wmworker_pools_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworker_pools_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select wmworker_pools_entities.partition_key, wmworker_pools_entities.row_key, wmworker_pools_entities.value, wmworker_pools_entities.version, wmworker_pools_entities.etag from wmworker_pools_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if wmworker_pools_entities_scan.pk is not null or wmworker_pools_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if wmworker_pools_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(wmworker_pools_entities_scan.pk);\n  end if;\n\n  if wmworker_pools_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(wmworker_pools_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by wmworker_pools_entities.partition_key, wmworker_pools_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworkers_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  if overwrite then\n    insert into wmworkers_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    ) on conflict (partition_key, row_key) do\n    update\n    set (value, version, etag) = (properties, wmworkers_entities_create.version, new_etag)\n    where wmworkers_entities.partition_key = wmworkers_entities_create.pk and wmworkers_entities.row_key = wmworkers_entities_create.rk;\n  else\n    insert into wmworkers_entities(partition_key, row_key, value, version, etag)\n    values (\n      pk,\n      rk,\n      properties,\n      version,\n      new_etag\n    );\n  end if;\n  return new_etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "worker_manager"
        },
        "wmworkers_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select wmworkers_entities.partition_key, wmworkers_entities.row_key, wmworkers_entities.value, wmworkers_entities.version,\n  wmworkers_entities.etag from wmworkers_entities\n  where wmworkers_entities.partition_key = wmworkers_entities_load.partition_key and wmworkers_entities.row_key = wmworkers_entities_load.row_key;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworkers_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  update wmworkers_entities\n  set (value, version, etag) = (properties, wmworkers_entities_modify.version, new_etag)\n  where wmworkers_entities.partition_key = wmworkers_entities_modify.partition_key and wmworkers_entities.row_key = wmworkers_entities_modify.row_key and wmworkers_entities.etag = wmworkers_entities_modify.old_etag;\n\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n\n  perform wmworkers_entities.etag from wmworkers_entities\n  where wmworkers_entities.partition_key = wmworkers_entities_modify.partition_key and wmworkers_entities.row_key = wmworkers_entities_modify.row_key;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworkers_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from wmworkers_entities\n  where wmworkers_entities.partition_key = wmworkers_entities_remove.partition_key and wmworkers_entities.row_key = wmworkers_entities_remove.row_key\n  returning wmworkers_entities.etag;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworkers_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select wmworkers_entities.partition_key, wmworkers_entities.row_key, wmworkers_entities.value, wmworkers_entities.version, wmworkers_entities.etag from wmworkers_entities';\n  partition_key_var text;\n  row_key_var text;\n  page_offset integer;\nbegin\n  if wmworkers_entities_scan.pk is not null or wmworkers_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if wmworkers_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(wmworkers_entities_scan.pk);\n  end if;\n\n  if wmworkers_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(wmworkers_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by wmworkers_entities.partition_key, wmworkers_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      page_offset := size * (page - 1);\n      sql := sql || ' offset ' || page_offset;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "worker_manager"
        }
      },
      "migrationScript": "begin\n  create table clients_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table clients_entities add primary key (partition_key, row_key);\n\n  create table roles_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table roles_entities add primary key (partition_key, row_key);\n\n  create table taskcluster_github_builds_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table taskcluster_github_builds_entities add primary key (partition_key, row_key);\n\n  create table taskcluster_integration_owners_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table taskcluster_integration_owners_entities add primary key (partition_key, row_key);\n\n  create table taskcluster_checks_to_tasks_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table taskcluster_checks_to_tasks_entities add primary key (partition_key, row_key);\n\n  create table taskcluster_check_runs_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table taskcluster_check_runs_entities add primary key (partition_key, row_key);\n\n  create table hooks_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table hooks_entities add primary key (partition_key, row_key);\n\n  create table queues_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table queues_entities add primary key (partition_key, row_key);\n\n  create table last_fire_3_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table last_fire_3_entities add primary key (partition_key, row_key);\n\n  create table indexed_tasks_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table indexed_tasks_entities add primary key (partition_key, row_key);\n\n  create table namespaces_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table namespaces_entities add primary key (partition_key, row_key);\n\n  create table denylisted_notification_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table denylisted_notification_entities add primary key (partition_key, row_key);\n\n  create table cache_purges_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table cache_purges_entities add primary key (partition_key, row_key);\n\n  create table queue_tasks_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table queue_tasks_entities add primary key (partition_key, row_key);\n\n  create table queue_artifacts_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table queue_artifacts_entities add primary key (partition_key, row_key);\n\n  create table queue_task_groups_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table queue_task_groups_entities add primary key (partition_key, row_key);\n\n  create table queue_task_group_members_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table queue_task_group_members_entities add primary key (partition_key, row_key);\n\n  create table queue_task_group_active_sets_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table queue_task_group_active_sets_entities add primary key (partition_key, row_key);\n\n  create table queue_task_requirement_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table queue_task_requirement_entities add primary key (partition_key, row_key);\n\n  create table queue_task_dependency_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table queue_task_dependency_entities add primary key (partition_key, row_key);\n\n  create table queue_worker_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table queue_worker_entities add primary key (partition_key, row_key);\n\n  create table queue_worker_type_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table queue_worker_type_entities add primary key (partition_key, row_key);\n\n  create table queue_provisioner_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table queue_provisioner_entities add primary key (partition_key, row_key);\n\n  create table secrets_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table secrets_entities add primary key (partition_key, row_key);\n\n  create table authorization_codes_table_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table authorization_codes_table_entities add primary key (partition_key, row_key);\n\n  create table access_token_table_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table access_token_table_entities add primary key (partition_key, row_key);\n\n  create table session_storage_table_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table session_storage_table_entities add primary key (partition_key, row_key);\n\n  create table github_access_token_table_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table github_access_token_table_entities add primary key (partition_key, row_key);\n\n  create table wmworkers_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table wmworkers_entities add primary key (partition_key, row_key);\n\n  create table wmworker_pools_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table wmworker_pools_entities add primary key (partition_key, row_key);\n\n  create table wmworker_pool_errors_entities(partition_key text, row_key text, value jsonb not null, version integer not null, etag uuid default public.gen_random_uuid());\n  alter table wmworker_pool_errors_entities add primary key (partition_key, row_key);\n\n  grant select, insert, update, delete on clients_entities to $db_user_prefix$_auth;\n  grant select, insert, update, delete on roles_entities to $db_user_prefix$_auth;\n\n  grant select, insert, update, delete on taskcluster_check_runs_entities to $db_user_prefix$_github;\n  grant select, insert, update, delete on taskcluster_checks_to_tasks_entities to $db_user_prefix$_github;\n  grant select, insert, update, delete on taskcluster_github_builds_entities to $db_user_prefix$_github;\n  grant select, insert, update, delete on taskcluster_integration_owners_entities to $db_user_prefix$_github;\n\n  grant select, insert, update, delete on hooks_entities to $db_user_prefix$_hooks;\n  grant select, insert, update, delete on last_fire_3_entities to $db_user_prefix$_hooks;\n  grant select, insert, update, delete on queues_entities to $db_user_prefix$_hooks;\n\n  grant select, insert, update, delete on indexed_tasks_entities to $db_user_prefix$_index;\n  grant select, insert, update, delete on namespaces_entities to $db_user_prefix$_index;\n\n  grant select, insert, update, delete on denylisted_notification_entities to $db_user_prefix$_notify;\n\n  grant select, insert, update, delete on cache_purges_entities to $db_user_prefix$_purge_cache;\n\n  grant select, insert, update, delete on secrets_entities to $db_user_prefix$_secrets;\n\n  grant select, insert, update, delete on access_token_table_entities to $db_user_prefix$_web_server;\n  grant select, insert, update, delete on authorization_codes_table_entities to $db_user_prefix$_web_server;\n  grant select, insert, update, delete on github_access_token_table_entities to $db_user_prefix$_web_server;\n  grant select, insert, update, delete on session_storage_table_entities to $db_user_prefix$_web_server;\n\n  grant select, insert, update, delete on wmworker_pool_errors_entities to $db_user_prefix$_worker_manager;\n  grant select, insert, update, delete on wmworker_pools_entities to $db_user_prefix$_worker_manager;\n  grant select, insert, update, delete on wmworkers_entities to $db_user_prefix$_worker_manager;\n\n  grant select, insert, update, delete on queue_tasks_entities to $db_user_prefix$_queue;\n  grant select, insert, update, delete on queue_artifacts_entities to $db_user_prefix$_queue;\n  grant select, insert, update, delete on queue_task_groups_entities to $db_user_prefix$_queue;\n  grant select, insert, update, delete on queue_task_group_members_entities to $db_user_prefix$_queue;\n  grant select, insert, update, delete on queue_task_group_active_sets_entities to $db_user_prefix$_queue;\n  grant select, insert, update, delete on queue_task_requirement_entities to $db_user_prefix$_queue;\n  grant select, insert, update, delete on queue_task_dependency_entities to $db_user_prefix$_queue;\n  grant select, insert, update, delete on queue_worker_entities to $db_user_prefix$_queue;\n  grant select, insert, update, delete on queue_worker_type_entities to $db_user_prefix$_queue;\n  grant select, insert, update, delete on queue_provisioner_entities to $db_user_prefix$_queue;\nend",
      "version": 2
    },
    {
      "description": "define all taskcluster-lib-azqueue tables and functions",
      "downgradeScript": "begin\n  revoke select, insert, update, delete on azure_queue_messages from $db_user_prefix$_queue;\n  drop table azure_queue_messages;\nend",
      "methods": {
        "azure_queue_count": {
          "args": "queue_name text",
          "body": "begin\n  return (select count(*)\n  from azure_queue_messages msgs\n  where msgs.queue_name = azure_queue_count.queue_name);\nend",
          "deprecated": false,
          "description": "Count messages in the named queue.\n",
          "mode": "read",
          "returns": "integer",
          "serviceName": "queue"
        },
        "azure_queue_delete": {
          "args": "queue_name text, message_id uuid, pop_receipt uuid",
          "body": "begin\n  delete from azure_queue_messages msgs\n    where msgs.queue_name = azure_queue_delete.queue_name\n      and msgs.message_id = azure_queue_delete.message_id\n      and msgs.pop_receipt = azure_queue_delete.pop_receipt;\nend",
          "deprecated": false,
          "description": "Delete the message identified by the given `queue_name`, `message_id` and\n`pop_receipt`.\n",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "azure_queue_delete_expired": {
          "args": "",
          "body": "begin\n  delete from azure_queue_messages msgs\n    where msgs.expires <= now();\nend",
          "deprecated": false,
          "description": "Delete all expired messages.  This is a maintenance task that should occur\nabout once an hour.\n",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "azure_queue_get": {
          "args": "queue_name text, visible timestamp, count integer",
          "body": "begin\n  return query update azure_queue_messages m1\n    set\n      pop_receipt = public.gen_random_uuid(),\n      visible = azure_queue_get.visible\n    where\n      m1.message_id in (\n        select m2.message_id from azure_queue_messages m2\n        where m2.queue_name = azure_queue_get.queue_name\n          and m2.visible <= now()\n          and m2.expires > now()\n        order by m2.inserted\n        for update skip locked\n        limit count\n    )\n    returning m1.message_id, m1.message_text, m1.pop_receipt;\nend",
          "deprecated": false,
          "description": "Get up to `count` messages from the given queue, setting the `visible`\ncolumn of each to the given value.  Returns a `message_id` and\n`pop_receipt` for each one, for use with `azure_queue_delete` and\n`azure_queue_update`.\n",
          "mode": "write",
          "returns": "table (message_id uuid, message_text text, pop_receipt uuid)",
          "serviceName": "queue"
        },
        "azure_queue_put": {
          "args": "queue_name text, message_text text, visible timestamp, expires timestamp",
          "body": "begin\n  insert into azure_queue_messages (\n      queue_name,\n      message_id,\n      message_text,\n      inserted,\n      visible,\n      expires\n    ) values (\n      azure_queue_put.queue_name,\n      public.gen_random_uuid(),\n      azure_queue_put.message_text,\n      now(),\n      azure_queue_put.visible,\n      azure_queue_put.expires\n    );\n  execute 'notify ' || quote_ident(queue_name);\nend",
          "deprecated": false,
          "description": "Put the given message into the given queue.  The message will not be visible until\nafter the visible timestamp, and will disappear after the expires timestamp.\n",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "azure_queue_update": {
          "args": "queue_name text, message_text text, message_id uuid, pop_receipt uuid, visible timestamp",
          "body": "begin\n  update azure_queue_messages msgs\n    set message_text = azure_queue_update.message_text,\n      visible = azure_queue_update.visible\n    where msgs.queue_name = azure_queue_update.queue_name\n      and msgs.message_id = azure_queue_update.message_id\n      and msgs.pop_receipt = azure_queue_update.pop_receipt;\nend",
          "deprecated": false,
          "description": "Update the message identified by the given `queue_name`, `message_id` and\n`pop_receipt`, setting its `visible` and `message_text` properties as\ngiven.\n",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        }
      },
      "migrationScript": "begin\n  create table azure_queue_messages (\n    message_id uuid not null primary key,\n    queue_name text not null,\n    message_text text not null,\n    inserted timestamp not null,\n    visible timestamp not null, -- visible after this time\n    expires timestamp not null,  -- expired after this time\n    pop_receipt uuid -- null means not popped\n  );\n  -- 'get' operations sort by inserted within a queue\n  create index azure_queue_messages_inserted on azure_queue_messages(queue_name, inserted);\n  grant select, insert, update, delete on azure_queue_messages to $db_user_prefix$_queue;\nend",
      "version": 3
    },
    {
      "description": "fix typestamps in `azure_queue_messages`",
      "downgradeScript": "begin\n  alter table azure_queue_messages\n    alter column inserted type timestamp,\n    alter column visible type timestamp,\n    alter column expires type timestamp;\nend",
      "methods": {
      },
      "migrationScript": "begin\n  alter table azure_queue_messages\n    alter column inserted type timestamptz,\n    alter column visible type timestamptz,\n    alter column expires type timestamptz;\nend",
      "version": 4
    },
    {
      "description": "fix ordering of results returned from `azure_queue_get`",
      "methods": {
        "azure_queue_get": {
          "args": "queue_name text, visible timestamp, count integer",
          "body": "begin\n  return query\n    with updated as (\n      update azure_queue_messages m\n      set\n        pop_receipt = public.gen_random_uuid(),\n        visible = azure_queue_get.visible\n      where\n        m.message_id in (\n          select m2.message_id from azure_queue_messages m2\n          where m2.queue_name = azure_queue_get.queue_name\n            and m2.visible <= now()\n            and m2.expires > now()\n          order by m2.inserted\n          for update skip locked\n          limit count\n      )\n      returning m.inserted, m.message_id, m.message_text, m.pop_receipt\n    )\n    select\n      u.message_id, u.message_text, u.pop_receipt\n    from updated as u\n    order by u.inserted;\nend",
          "deprecated": false,
          "description": "Get up to `count` messages from the given queue, setting the `visible`\ncolumn of each to the given value.  Returns a `message_id` and\n`pop_receipt` for each one, for use with `azure_queue_delete` and\n`azure_queue_update`.\n",
          "mode": "write",
          "returns": "table (message_id uuid, message_text text, pop_receipt uuid)",
          "serviceName": "queue"
        }
      },
      "version": 5
    },
    {
      "description": "update `azure_queue_count` to count only non-expired rows",
      "methods": {
        "azure_queue_count": {
          "args": "queue_name text",
          "body": "begin\n  return (\n    select\n      count(*)\n    from\n      azure_queue_messages msgs\n    where\n      msgs.queue_name = azure_queue_count.queue_name\n      and msgs.expires > now()\n  );\nend",
          "deprecated": false,
          "description": "Count non-expired messages in the named queue.\n",
          "mode": "read",
          "returns": "integer",
          "serviceName": "queue"
        }
      },
      "version": 6
    },
    {
      "description": "bugfix for `.._scan` taskcluster-lib-entities methods",
      "methods": {
        "access_token_table_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select access_token_table_entities.partition_key, access_token_table_entities.row_key, access_token_table_entities.value, access_token_table_entities.version, access_token_table_entities.etag from access_token_table_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if access_token_table_entities_scan.pk is not null or access_token_table_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if access_token_table_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(access_token_table_entities_scan.pk);\n  end if;\n\n  if access_token_table_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(access_token_table_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by access_token_table_entities.partition_key, access_token_table_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "web_server"
        },
        "authorization_codes_table_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select authorization_codes_table_entities.partition_key, authorization_codes_table_entities.row_key, authorization_codes_table_entities.value, authorization_codes_table_entities.version, authorization_codes_table_entities.etag from authorization_codes_table_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if authorization_codes_table_entities_scan.pk is not null or authorization_codes_table_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if authorization_codes_table_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(authorization_codes_table_entities_scan.pk);\n  end if;\n\n  if authorization_codes_table_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(authorization_codes_table_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by authorization_codes_table_entities.partition_key, authorization_codes_table_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "web_server"
        },
        "cache_purges_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select cache_purges_entities.partition_key, cache_purges_entities.row_key, cache_purges_entities.value, cache_purges_entities.version, cache_purges_entities.etag from cache_purges_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if cache_purges_entities_scan.pk is not null or cache_purges_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if cache_purges_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(cache_purges_entities_scan.pk);\n  end if;\n\n  if cache_purges_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(cache_purges_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by cache_purges_entities.partition_key, cache_purges_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "purge_cache"
        },
        "clients_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select clients_entities.partition_key, clients_entities.row_key, clients_entities.value, clients_entities.version, clients_entities.etag from clients_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if clients_entities_scan.pk is not null or clients_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if clients_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(clients_entities_scan.pk);\n  end if;\n\n  if clients_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(clients_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by clients_entities.partition_key, clients_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "auth"
        },
        "denylisted_notification_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select denylisted_notification_entities.partition_key, denylisted_notification_entities.row_key, denylisted_notification_entities.value, denylisted_notification_entities.version, denylisted_notification_entities.etag from denylisted_notification_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if denylisted_notification_entities_scan.pk is not null or denylisted_notification_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if denylisted_notification_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(denylisted_notification_entities_scan.pk);\n  end if;\n\n  if denylisted_notification_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(denylisted_notification_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by denylisted_notification_entities.partition_key, denylisted_notification_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "notify"
        },
        "github_access_token_table_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select github_access_token_table_entities.partition_key, github_access_token_table_entities.row_key, github_access_token_table_entities.value, github_access_token_table_entities.version, github_access_token_table_entities.etag from github_access_token_table_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if github_access_token_table_entities_scan.pk is not null or github_access_token_table_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if github_access_token_table_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(github_access_token_table_entities_scan.pk);\n  end if;\n\n  if github_access_token_table_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(github_access_token_table_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by github_access_token_table_entities.partition_key, github_access_token_table_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "web_server"
        },
        "hooks_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select hooks_entities.partition_key, hooks_entities.row_key, hooks_entities.value, hooks_entities.version, hooks_entities.etag from hooks_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if hooks_entities_scan.pk is not null or hooks_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if hooks_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(hooks_entities_scan.pk);\n  end if;\n\n  if hooks_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(hooks_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by hooks_entities.partition_key, hooks_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "hooks"
        },
        "indexed_tasks_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select indexed_tasks_entities.partition_key, indexed_tasks_entities.row_key, indexed_tasks_entities.value, indexed_tasks_entities.version, indexed_tasks_entities.etag from indexed_tasks_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if indexed_tasks_entities_scan.pk is not null or indexed_tasks_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if indexed_tasks_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(indexed_tasks_entities_scan.pk);\n  end if;\n\n  if indexed_tasks_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(indexed_tasks_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by indexed_tasks_entities.partition_key, indexed_tasks_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "index"
        },
        "last_fire_3_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select last_fire_3_entities.partition_key, last_fire_3_entities.row_key, last_fire_3_entities.value, last_fire_3_entities.version, last_fire_3_entities.etag from last_fire_3_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if last_fire_3_entities_scan.pk is not null or last_fire_3_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if last_fire_3_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(last_fire_3_entities_scan.pk);\n  end if;\n\n  if last_fire_3_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(last_fire_3_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by last_fire_3_entities.partition_key, last_fire_3_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "hooks"
        },
        "namespaces_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select namespaces_entities.partition_key, namespaces_entities.row_key, namespaces_entities.value, namespaces_entities.version, namespaces_entities.etag from namespaces_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if namespaces_entities_scan.pk is not null or namespaces_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if namespaces_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(namespaces_entities_scan.pk);\n  end if;\n\n  if namespaces_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(namespaces_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by namespaces_entities.partition_key, namespaces_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "index"
        },
        "queue_artifacts_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select queue_artifacts_entities.partition_key, queue_artifacts_entities.row_key, queue_artifacts_entities.value, queue_artifacts_entities.version, queue_artifacts_entities.etag from queue_artifacts_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if queue_artifacts_entities_scan.pk is not null or queue_artifacts_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if queue_artifacts_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(queue_artifacts_entities_scan.pk);\n  end if;\n\n  if queue_artifacts_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(queue_artifacts_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by queue_artifacts_entities.partition_key, queue_artifacts_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_provisioner_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select queue_provisioner_entities.partition_key, queue_provisioner_entities.row_key, queue_provisioner_entities.value, queue_provisioner_entities.version, queue_provisioner_entities.etag from queue_provisioner_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if queue_provisioner_entities_scan.pk is not null or queue_provisioner_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if queue_provisioner_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(queue_provisioner_entities_scan.pk);\n  end if;\n\n  if queue_provisioner_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(queue_provisioner_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by queue_provisioner_entities.partition_key, queue_provisioner_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_dependency_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select queue_task_dependency_entities.partition_key, queue_task_dependency_entities.row_key, queue_task_dependency_entities.value, queue_task_dependency_entities.version, queue_task_dependency_entities.etag from queue_task_dependency_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if queue_task_dependency_entities_scan.pk is not null or queue_task_dependency_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if queue_task_dependency_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(queue_task_dependency_entities_scan.pk);\n  end if;\n\n  if queue_task_dependency_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(queue_task_dependency_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by queue_task_dependency_entities.partition_key, queue_task_dependency_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_group_active_sets_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select queue_task_group_active_sets_entities.partition_key, queue_task_group_active_sets_entities.row_key, queue_task_group_active_sets_entities.value, queue_task_group_active_sets_entities.version, queue_task_group_active_sets_entities.etag from queue_task_group_active_sets_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if queue_task_group_active_sets_entities_scan.pk is not null or queue_task_group_active_sets_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if queue_task_group_active_sets_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(queue_task_group_active_sets_entities_scan.pk);\n  end if;\n\n  if queue_task_group_active_sets_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(queue_task_group_active_sets_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by queue_task_group_active_sets_entities.partition_key, queue_task_group_active_sets_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_group_members_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select queue_task_group_members_entities.partition_key, queue_task_group_members_entities.row_key, queue_task_group_members_entities.value, queue_task_group_members_entities.version, queue_task_group_members_entities.etag from queue_task_group_members_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if queue_task_group_members_entities_scan.pk is not null or queue_task_group_members_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if queue_task_group_members_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(queue_task_group_members_entities_scan.pk);\n  end if;\n\n  if queue_task_group_members_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(queue_task_group_members_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by queue_task_group_members_entities.partition_key, queue_task_group_members_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_groups_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select queue_task_groups_entities.partition_key, queue_task_groups_entities.row_key, queue_task_groups_entities.value, queue_task_groups_entities.version, queue_task_groups_entities.etag from queue_task_groups_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if queue_task_groups_entities_scan.pk is not null or queue_task_groups_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if queue_task_groups_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(queue_task_groups_entities_scan.pk);\n  end if;\n\n  if queue_task_groups_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(queue_task_groups_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by queue_task_groups_entities.partition_key, queue_task_groups_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_requirement_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select queue_task_requirement_entities.partition_key, queue_task_requirement_entities.row_key, queue_task_requirement_entities.value, queue_task_requirement_entities.version, queue_task_requirement_entities.etag from queue_task_requirement_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if queue_task_requirement_entities_scan.pk is not null or queue_task_requirement_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if queue_task_requirement_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(queue_task_requirement_entities_scan.pk);\n  end if;\n\n  if queue_task_requirement_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(queue_task_requirement_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by queue_task_requirement_entities.partition_key, queue_task_requirement_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_tasks_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select queue_tasks_entities.partition_key, queue_tasks_entities.row_key, queue_tasks_entities.value, queue_tasks_entities.version, queue_tasks_entities.etag from queue_tasks_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if queue_tasks_entities_scan.pk is not null or queue_tasks_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if queue_tasks_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(queue_tasks_entities_scan.pk);\n  end if;\n\n  if queue_tasks_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(queue_tasks_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by queue_tasks_entities.partition_key, queue_tasks_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_worker_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select queue_worker_entities.partition_key, queue_worker_entities.row_key, queue_worker_entities.value, queue_worker_entities.version, queue_worker_entities.etag from queue_worker_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if queue_worker_entities_scan.pk is not null or queue_worker_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if queue_worker_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(queue_worker_entities_scan.pk);\n  end if;\n\n  if queue_worker_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(queue_worker_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by queue_worker_entities.partition_key, queue_worker_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_worker_type_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select queue_worker_type_entities.partition_key, queue_worker_type_entities.row_key, queue_worker_type_entities.value, queue_worker_type_entities.version, queue_worker_type_entities.etag from queue_worker_type_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if queue_worker_type_entities_scan.pk is not null or queue_worker_type_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if queue_worker_type_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(queue_worker_type_entities_scan.pk);\n  end if;\n\n  if queue_worker_type_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(queue_worker_type_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by queue_worker_type_entities.partition_key, queue_worker_type_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queues_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select queues_entities.partition_key, queues_entities.row_key, queues_entities.value, queues_entities.version, queues_entities.etag from queues_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if queues_entities_scan.pk is not null or queues_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if queues_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(queues_entities_scan.pk);\n  end if;\n\n  if queues_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(queues_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by queues_entities.partition_key, queues_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "hooks"
        },
        "roles_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select roles_entities.partition_key, roles_entities.row_key, roles_entities.value, roles_entities.version, roles_entities.etag from roles_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if roles_entities_scan.pk is not null or roles_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if roles_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(roles_entities_scan.pk);\n  end if;\n\n  if roles_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(roles_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by roles_entities.partition_key, roles_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "auth"
        },
        "secrets_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select secrets_entities.partition_key, secrets_entities.row_key, secrets_entities.value, secrets_entities.version, secrets_entities.etag from secrets_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if secrets_entities_scan.pk is not null or secrets_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if secrets_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(secrets_entities_scan.pk);\n  end if;\n\n  if secrets_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(secrets_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by secrets_entities.partition_key, secrets_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "secrets"
        },
        "session_storage_table_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select session_storage_table_entities.partition_key, session_storage_table_entities.row_key, session_storage_table_entities.value, session_storage_table_entities.version, session_storage_table_entities.etag from session_storage_table_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if session_storage_table_entities_scan.pk is not null or session_storage_table_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if session_storage_table_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(session_storage_table_entities_scan.pk);\n  end if;\n\n  if session_storage_table_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(session_storage_table_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by session_storage_table_entities.partition_key, session_storage_table_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "web_server"
        },
        "taskcluster_check_runs_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select taskcluster_check_runs_entities.partition_key, taskcluster_check_runs_entities.row_key, taskcluster_check_runs_entities.value, taskcluster_check_runs_entities.version, taskcluster_check_runs_entities.etag from taskcluster_check_runs_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if taskcluster_check_runs_entities_scan.pk is not null or taskcluster_check_runs_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if taskcluster_check_runs_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(taskcluster_check_runs_entities_scan.pk);\n  end if;\n\n  if taskcluster_check_runs_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(taskcluster_check_runs_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by taskcluster_check_runs_entities.partition_key, taskcluster_check_runs_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_checks_to_tasks_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select taskcluster_checks_to_tasks_entities.partition_key, taskcluster_checks_to_tasks_entities.row_key, taskcluster_checks_to_tasks_entities.value, taskcluster_checks_to_tasks_entities.version, taskcluster_checks_to_tasks_entities.etag from taskcluster_checks_to_tasks_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if taskcluster_checks_to_tasks_entities_scan.pk is not null or taskcluster_checks_to_tasks_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if taskcluster_checks_to_tasks_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(taskcluster_checks_to_tasks_entities_scan.pk);\n  end if;\n\n  if taskcluster_checks_to_tasks_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(taskcluster_checks_to_tasks_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by taskcluster_checks_to_tasks_entities.partition_key, taskcluster_checks_to_tasks_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_github_builds_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select taskcluster_github_builds_entities.partition_key, taskcluster_github_builds_entities.row_key, taskcluster_github_builds_entities.value, taskcluster_github_builds_entities.version, taskcluster_github_builds_entities.etag from taskcluster_github_builds_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if taskcluster_github_builds_entities_scan.pk is not null or taskcluster_github_builds_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if taskcluster_github_builds_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(taskcluster_github_builds_entities_scan.pk);\n  end if;\n\n  if taskcluster_github_builds_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(taskcluster_github_builds_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by taskcluster_github_builds_entities.partition_key, taskcluster_github_builds_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_integration_owners_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select taskcluster_integration_owners_entities.partition_key, taskcluster_integration_owners_entities.row_key, taskcluster_integration_owners_entities.value, taskcluster_integration_owners_entities.version, taskcluster_integration_owners_entities.etag from taskcluster_integration_owners_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if taskcluster_integration_owners_entities_scan.pk is not null or taskcluster_integration_owners_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if taskcluster_integration_owners_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(taskcluster_integration_owners_entities_scan.pk);\n  end if;\n\n  if taskcluster_integration_owners_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(taskcluster_integration_owners_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by taskcluster_integration_owners_entities.partition_key, taskcluster_integration_owners_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "github"
        },
        "wmworker_pool_errors_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select wmworker_pool_errors_entities.partition_key, wmworker_pool_errors_entities.row_key, wmworker_pool_errors_entities.value, wmworker_pool_errors_entities.version, wmworker_pool_errors_entities.etag from wmworker_pool_errors_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if wmworker_pool_errors_entities_scan.pk is not null or wmworker_pool_errors_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if wmworker_pool_errors_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(wmworker_pool_errors_entities_scan.pk);\n  end if;\n\n  if wmworker_pool_errors_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(wmworker_pool_errors_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by wmworker_pool_errors_entities.partition_key, wmworker_pool_errors_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworker_pools_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select wmworker_pools_entities.partition_key, wmworker_pools_entities.row_key, wmworker_pools_entities.value, wmworker_pools_entities.version, wmworker_pools_entities.etag from wmworker_pools_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if wmworker_pools_entities_scan.pk is not null or wmworker_pools_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if wmworker_pools_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(wmworker_pools_entities_scan.pk);\n  end if;\n\n  if wmworker_pools_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(wmworker_pools_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by wmworker_pools_entities.partition_key, wmworker_pools_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworkers_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  sql text := 'select wmworkers_entities.partition_key, wmworkers_entities.row_key, wmworkers_entities.value, wmworkers_entities.version, wmworkers_entities.etag from wmworkers_entities';\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if wmworkers_entities_scan.pk is not null or wmworkers_entities_scan.rk is not null or condition is not null then\n    sql := sql || ' where ';\n  end if;\n\n  if wmworkers_entities_scan.pk is not null then\n    partition_key_var := 'partition_key = ' || quote_literal(wmworkers_entities_scan.pk);\n  end if;\n\n  if wmworkers_entities_scan.rk is not null then\n    row_key_var := 'row_key = ' || quote_literal(wmworkers_entities_scan.rk);\n  end if;\n\n  sql := sql || concat_ws(' and ', partition_key_var, row_key_var, condition);\n  sql := sql || ' order by wmworkers_entities.partition_key, wmworkers_entities.row_key';\n\n  if size is not null and size > 0 then\n    sql := sql || ' limit ' || size + 1;\n\n    if page is not null and page > 0 then\n      sql := sql || ' offset ' || page;\n    end if;\n  end if;\n\n  return query execute sql;\nend",
          "deprecated": false,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "worker_manager"
        }
      },
      "version": 7
    },
    {
      "description": "introduce `encode/decode_composite_key`, `encode/decode_string_key`, and `entity_buf_encode/decode`",
      "downgradeScript": "begin\n  drop function entity_buf_decode(value JSONB, name text);\n  drop function entity_buf_encode(value JSONB, name text, data text);\n  drop function encode_string_key(in_str text, OUT _result text);\n  drop function decode_string_key(in_str text, OUT _result text);\n  drop function encode_composite_key(key1 text, key2 text);\n  drop function decode_composite_key(encoded_key text);\nend\n",
      "methods": {
      },
      "migrationScript": "begin\n  -- define useful functions for migrations of tc-lib-entities tables.  These will persist and can\n  -- be used by future version files and downgrade scripts\n\n  -- given two simple keys, create a composite key like Entity.CompositeKey.\n  create or replace function encode_composite_key(key1 text, key2 text) RETURNS text\n  as $$\n      begin\n          return encode_string_key(key1) || '~' || encode_string_key(key2);\n      end;\n  $$\n  language plpgSQL\n  strict immutable;\n\n  -- Reverse the effect of encode_composite_key.  Note that SQL arrays are 1-indexed!\n  create or replace function decode_composite_key(encoded_key text) RETURNS text[]\n  as $$\n      begin\n       return array[decode_string_key(split_part(encoded_key, '~', 1)), decode_string_key(split_part(encoded_key, '~', 2))];\n      end;\n  $$\n  language plpgSQL\n  strict immutable;\n\n  -- decode the __buf encoding defined in tc-lib-entities entitytypes.js\n  create or replace function entity_buf_decode(value JSONB, name text) RETURNS text\n  as $$\n      declare\n          buffer text = '';\n          chunks integer;\n          chunk integer = 0;\n      begin\n          chunks = (value ->> ('__bufchunks_' || name))::integer;\n          loop\n              exit when chunks is null or chunk >= chunks;\n              buffer = buffer || (value ->> ('__buf' || chunk || '_' || name))::text;\n              chunk = chunk + 1;\n          end loop;\n          return convert_from(decode(buffer, 'base64'), 'utf8');\n      end;\n  $$\n  language plpgSQL\n  strict immutable;\n\n  -- encode the __buf encoding defined in tc-lib-entities entitytypes.js.  This uses a single\n  -- buffer unconditionally\n  create or replace function entity_buf_encode(value JSONB, name text, data text) RETURNS jsonb\n  as $$\n      declare\n        bytes bytea;\n      begin\n        value = jsonb_set(value,\n            ('{__bufchunks_' || name || '}')::text[],\n            to_jsonb(1));\n        bytes = convert_to(data, 'utf8');\n        value = jsonb_set(value,\n            ('{__buf0_' || name || '}')::text[],\n            to_jsonb(replace(encode(bytes, 'base64'), E'\\n', '')));\n        return value;\n      end;\n  $$\n  language plpgSQL\n  strict immutable;\n\n  -- SQL implementation of the tc-lib-entities encodeStringKey function, with credit to Nick in\n  -- https://stackoverflow.com/questions/341074/urlencode-with-only-built-in-functions\n  create or replace function encode_string_key(in_str text) returns text\n  as $$\n    select\n      case when in_str = '' then '!'\n      else\n        string_agg(\n          case\n            when ch = E'\\x5C' then '!5C'\n            when ch = '~' then '!7e'\n            when ol>1 or ch !~ '[-''()*.0-9A-Z_a-z]'\n                then regexp_replace(upper(substring(ch::bytea::text, 3)), '(..)', E'!\\\\1', 'g')\n            else ch\n          end,\n          ''\n        )\n      end\n    from (\n      select ch, octet_length(ch) as ol\n      from regexp_split_to_table($1, '') as ch\n    ) as s;\n  $$\n  language sql\n  strict immutable;\n\n  -- inverse of encode_string_key\n  -- based on https://www.postgresql.org/message-id/B6F6FD62F2624C4C9916AC0175D56D880CE1C118@jenmbs01.ad.intershop.net\n  -- with some major revisions to handle corner cases\n  create or replace function decode_string_key(in_str text) returns text\n  as $$\n    declare\n      ret text;\n      t text[];\n    begin\n      -- '!' is a special-case encoding of an empty string\n      if in_str = '!' then\n        return '';\n      end if;\n\n      with str as (\n        select\n          -- `plain` is an array of all non-encoded substrings, including a blank first string\n          -- if necessary so that we can assume the first component is blank\n          case when in_str ~ '^![0-9a-fA-F][0-9a-fA-F]'\n            then '{}'|| regexp_split_to_array (in_str,'(![0-9a-fA-F][0-9a-fA-F])+', 'i')\n            else regexp_split_to_array (in_str,'(![0-9a-fA-F][0-9a-fA-F])+', 'i')\n           end plain,\n\n          -- `encoded` is the opposite, all encoded substrings\n          array(select (regexp_matches (in_str,'((?:![0-9a-fA-F][0-9a-fA-F])+)', 'gi'))[1]) encoded\n      )\n      -- concatentate pairs of `plain` and decoded `encoded`\n      select string_agg(plain[i] || coalesce( convert_from(decode(replace(encoded[i], '!',''), 'hex'), 'utf8'),''),'')\n          from str,\n                (select  generate_series(1, greatest(0, array_upper(encoded,1))+2) i FROM str) as i\n      into ret;\n      return ret;\n    end;\n  $$\n  language plpgsql\n  strict immutable;\nend\n",
      "version": 8
    },
    {
      "description": "purge-cache phase 2",
      "downgradeScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.\n  lock table cache_purges;\n\n  create table cache_purges_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n  alter table cache_purges_entities add primary key (partition_key, row_key);\n\n  insert into cache_purges_entities\n  select\n    encode_composite_key(provisioner_id, worker_type) as partition_key,\n    encode_string_key(cache_name) as row_key,\n    jsonb_build_object(\n      'PartitionKey', encode_composite_key(provisioner_id, worker_type),\n      'RowKey', encode_string_key(cache_name),\n      'provisionerId', provisioner_id,\n      'workerType', worker_type,\n      'cacheName', cache_name,\n      'before', before,\n      'expires', expires) as value,\n    1 as version,\n    etag\n  from cache_purges;\n\n  revoke select, insert, update, delete on cache_purges from $db_user_prefix$_purge_cache;\n  drop table cache_purges;\n  grant select, insert, update, delete on cache_purges_entities to $db_user_prefix$_purge_cache;\n\n  drop function get_page_limit(page_size integer);\n  drop function get_page_offset(page_offset integer);\nend\n",
      "methods": {
        "all_purge_requests": {
          "args": "page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query select cache_purges.provisioner_id, cache_purges.worker_type, cache_purges.cache_name, cache_purges.before\n  from cache_purges\n  order by cache_purges.provisioner_id, cache_purges.worker_type, cache_purges.cache_name\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "View all active purge requests.",
          "mode": "read",
          "returns": "table (provisioner_id text, worker_type text, cache_name text, before timestamptz)",
          "serviceName": "purge_cache"
        },
        "cache_purges_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_row cache_purges%ROWTYPE;\nbegin\n  select\n    (properties ->> 'provisionerId')::text as provisioner_id,\n    (properties ->> 'workerType')::text as worker_type,\n    (properties ->> 'cacheName')::text as cache_name,\n    (properties ->> 'before')::timestamptz as before,\n    (properties ->> 'expires')::timestamptz as expires,\n    public.gen_random_uuid() as etag\n  into new_row;\n  if overwrite then\n    raise exception 'overwrite not implemented';\n  else\n    execute 'insert into cache_purges select $1.*' using new_row;\n  end if;\n  return new_row.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "purge_cache"
        },
        "cache_purges_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "declare\n  decoded_composite_key text[];\nbegin\n  decoded_composite_key := decode_composite_key(cache_purges_entities_load.partition_key);\n  return query\n  select\n    cache_purges_entities_load.partition_key,\n    cache_purges_entities_load.row_key,\n    jsonb_build_object(\n      'PartitionKey', cache_purges_entities_load.partition_key,\n      'RowKey', cache_purges_entities_load.row_key,\n      'provisionerId', provisioner_id,\n      'workerType', worker_type,\n      'cacheName', cache_name,\n      'before', before,\n      'expires', expires) as value,\n    1 as version,\n    cache_purges.etag as etag\n  from cache_purges\n  where\n    cache_purges.provisioner_id = decoded_composite_key[1] and cache_purges.worker_type = decoded_composite_key[2] and cache_purges.cache_name = decode_string_key(cache_purges_entities_load.row_key);\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "purge_cache"
        },
        "cache_purges_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_row cache_purges%ROWTYPE;\n  decoded_composite_key text[];\nbegin\n  decoded_composite_key := decode_composite_key(cache_purges_entities_modify.partition_key);\n  select\n    (properties ->> 'provisionerId')::text as provisioner_id,\n    (properties ->> 'workerType')::text as worker_type,\n    (properties ->> 'cacheName')::text as cache_name,\n    (properties ->> 'before')::timestamptz as before,\n    (properties ->> 'expires')::timestamptz as expires,\n    public.gen_random_uuid() as etag\n  into new_row;\n  update cache_purges\n  set (\n    provisioner_id,\n    worker_type,\n    cache_name,\n    before,\n    expires,\n    etag\n  ) = (\n    new_row.provisioner_id,\n    new_row.worker_type,\n    new_row.cache_name,\n    new_row.before,\n    new_row.expires,\n    new_row.etag\n  )\n  where\n    cache_purges.provisioner_id = decoded_composite_key[1] and\n    cache_purges.worker_type = decoded_composite_key[2] and\n    cache_purges.cache_name = decode_string_key(cache_purges_entities_modify.row_key) and\n    cache_purges.etag = cache_purges_entities_modify.old_etag;\n  if found then\n    return query select new_row.etag;\n    return;\n  end if;\n  perform cache_purges.etag from cache_purges\n  where\n    cache_purges.provisioner_id = decoded_composite_key[1] and\n    cache_purges.worker_type = decoded_composite_key[2] and\n    cache_purges.cache_name = decode_string_key(cache_purges_entities_modify.row_key);\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "purge_cache"
        },
        "cache_purges_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "declare\n  decoded_composite_key text[];\nbegin\n  decoded_composite_key := decode_composite_key(cache_purges_entities_remove.partition_key);\n  return query delete from cache_purges\n  where\n    cache_purges.provisioner_id = decoded_composite_key[1] and cache_purges.worker_type = decoded_composite_key[2] and cache_purges.cache_name = decode_string_key(cache_purges_entities_remove.row_key)\n  returning cache_purges.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "purge_cache"
        },
        "cache_purges_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  cond text[];\n  exp_cond_operator text;\n  exp_cond_operand timestamptz;\n  partition_key_var text;\n  row_key_var text;\n  decoded_composite_key text[];\nbegin\n  decoded_composite_key := decode_composite_key(cache_purges_entities_scan.pk);\n  if not condition is null then\n    cond := regexp_split_to_array(condition, '\\s+');\n    exp_cond_operator := cond[4];\n    exp_cond_operand := cond[5] :: timestamptz;\n\n    return query select\n      encode_composite_key(cache_purges.provisioner_id, cache_purges.worker_type) as partition_key,\n      encode_string_key(cache_purges.cache_name) as row_key,\n      jsonb_build_object(\n        'PartitionKey', encode_composite_key(cache_purges.provisioner_id, cache_purges.worker_type),\n        'RowKey', cache_name,\n        'provisionerId', provisioner_id,\n        'workerType', worker_type,\n        'cacheName', cache_name,\n        'before', before,\n        'expires', expires\n      ) as value,\n      1 as version,\n      cache_purges.etag as etag from cache_purges\n    where\n      (cache_purges_entities_scan.pk is null or cache_purges_entities_scan.pk = decoded_composite_key[1] || '~' || decoded_composite_key[2]) and\n      (cache_purges_entities_scan.rk is null or cache_purges_entities_scan.rk = cache_name) and\n      case\n        when exp_cond_operator = '=' then expires = exp_cond_operand\n        when exp_cond_operator = '<' then expires < exp_cond_operand\n        when exp_cond_operator = '<=' then expires <= exp_cond_operand\n        when exp_cond_operator = '>' then expires > exp_cond_operand\n        when exp_cond_operator = '>=' then expires >= exp_cond_operand\n        else expires <> exp_cond_operand\n      end\n    order by cache_purges.provisioner_id, cache_purges.worker_type, cache_purges.cache_name\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (page is not null and page > 0) then page\n      else 0\n    end;\n  else\n    return query select\n      encode_composite_key(cache_purges.provisioner_id, cache_purges.worker_type) as partition_key,\n      encode_string_key(cache_purges.cache_name) as row_key,\n      jsonb_build_object(\n        'PartitionKey', encode_composite_key(cache_purges.provisioner_id, cache_purges.worker_type),\n        'RowKey', encode_string_key(cache_name),\n        'provisionerId', provisioner_id,\n        'workerType', worker_type,\n        'cacheName', cache_name,\n        'before', before,\n        'expires', expires\n      ) as value,\n      1 as version,\n      cache_purges.etag as etag from cache_purges\n    where\n      (cache_purges_entities_scan.pk is null or (cache_purges.provisioner_id = decoded_composite_key[1] and cache_purges.worker_type = decoded_composite_key[2])) and\n      (cache_purges_entities_scan.rk is null or cache_purges_entities_scan.rk = cache_name)\n    order by cache_purges.provisioner_id, cache_purges.worker_type, cache_purges.cache_name\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (size is not null and size > 0 and page is not null and page > 0) then page\n      else 0\n    end;\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "purge_cache"
        },
        "expire_cache_purges": {
          "args": "expires_in timestamptz",
          "body": "declare\n  count integer;\nbegin\n  delete from cache_purges where cache_purges.expires < expires_in;\n  if found then\n    get diagnostics count = row_count;\n    return count;\n  end if;\n  return 0;\nend",
          "deprecated": false,
          "description": "Expire cache purges that come before `expires_in`.\nReturns a count of rows that have been deleted.",
          "mode": "write",
          "returns": "integer",
          "serviceName": "purge_cache"
        },
        "purge_cache": {
          "args": "provisioner_id_in text, worker_type_in text, cache_name_in text, before_in timestamptz, expires_in timestamptz",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  insert into cache_purges(provisioner_id, worker_type, cache_name, before, expires, etag)\n  values (\n    provisioner_id_in,\n    worker_type_in,\n    cache_name_in,\n    before_in,\n    expires_in,\n    new_etag\n  ) on conflict (provisioner_id, worker_type, cache_name) do\n  update\n  set (before, expires, etag) = (before_in, expires_in, new_etag)\n  where cache_purges.provisioner_id = provisioner_id_in and cache_purges.worker_type = worker_type_in and cache_purges.cache_name = cache_name_in;\nend",
          "deprecated": false,
          "description": "Publish a request to purge caches with name `cache_name_in`\non `provisioner_id_in`/`worker_type_in` workers.",
          "mode": "write",
          "returns": "void",
          "serviceName": "purge_cache"
        },
        "purge_requests": {
          "args": "provisioner_id_in text, worker_type_in text",
          "body": "begin\n  return query\n  select cache_purges.provisioner_id, cache_purges.worker_type, cache_purges.cache_name, cache_purges.before from cache_purges\n  where cache_purges.provisioner_id = provisioner_id_in and cache_purges.worker_type = worker_type_in;\nend",
          "deprecated": false,
          "description": "List the caches for this `provisioner_id_in`/`worker_type_in`.",
          "mode": "read",
          "returns": "table (provisioner_id text, worker_type text, cache_name text, before timestamptz)",
          "serviceName": "purge_cache"
        }
      },
      "migrationScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.  Note that this may lead to concurrent\n  -- updates failing; the important thing is that they not succeed without\n  -- taking effect.  Failed updates will be retried.\n  lock table cache_purges_entities;\n\n  create table cache_purges\n  as\n    select\n      (value ->> 'provisionerId')::text as provisioner_id,\n      (value ->> 'workerType')::text as worker_type,\n      (value ->> 'cacheName')::text as cache_name,\n      (value ->> 'before')::timestamptz as before,\n      (value ->> 'expires')::timestamptz as expires,\n      etag\n    from cache_purges_entities;\n  alter table cache_purges add primary key (provisioner_id, worker_type, cache_name);\n  alter table cache_purges\n    alter column provisioner_id set not null,\n    alter column worker_type set not null,\n    alter column cache_name set not null,\n    alter column before set not null,\n    alter column expires set not null,\n    alter column etag set not null,\n    alter column etag set default public.gen_random_uuid();\n\n  revoke select, insert, update, delete on cache_purges_entities from $db_user_prefix$_purge_cache;\n  drop table cache_purges_entities;\n  grant select, insert, update, delete on cache_purges to $db_user_prefix$_purge_cache;\n\n  -- Given a page size it returns the limit to use on a paginated db function.\n  create or replace function get_page_limit(page_size integer) RETURNS integer\n  as $$\n      begin\n        return case\n          when (page_size is not null and page_size > 0) then page_size\n          else null\n        end;\n      end;\n  $$\n  language plpgSQL\n  strict immutable;\n\n  -- Given a page offset it returns the offset to use on a paginated db function.\n  create or replace function get_page_offset(page_offset integer) RETURNS integer\n  as $$\n      begin\n        return case\n          when (page_offset is not null and page_offset > 0) then page_offset\n          else 0\n        end;\n      end;\n  $$\n  language plpgSQL\n  strict immutable;\nend\n",
      "version": 9
    },
    {
      "description": "worker-manager worker pools phase 2",
      "downgradeScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.\n  lock table worker_pools;\n\n  create table wmworker_pools_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n  alter table wmworker_pools_entities add primary key (partition_key, row_key);\n\n  insert into wmworker_pools_entities\n  select\n    encode_string_key(worker_pool_id) as partition_key,\n    'workerPool' as row_key,\n    entity_buf_encode(\n      entity_buf_encode(\n        entity_buf_encode(\n          jsonb_build_object(\n            'PartitionKey', encode_string_key(worker_pool_id),\n            'RowKey', 'workerPool',\n            'workerPoolId', worker_pool_id,\n            'providerId', provider_id,\n            'owner', owner,\n            'description', description,\n            'emailOnError', email_on_error,\n            'created', created,\n            'lastModified', last_modified),\n          'config', config::text),\n        'providerData', provider_data::text),\n      'previousProviderIds', previous_provider_ids::text) as value,\n    1 as version,\n    etag\n  from worker_pools;\n\n  revoke select, insert, update, delete on worker_pools from $db_user_prefix$_worker_manager;\n  drop table worker_pools;\n  grant select, insert, update, delete on wmworker_pools_entities to $db_user_prefix$_worker_manager;\nend\n",
      "methods": {
        "create_worker_pool": {
          "args": "worker_pool_id_in text, provider_id_in text, previous_provider_ids_in jsonb, description_in text, config_in jsonb, created_in timestamptz, last_modified_in timestamptz, owner_in text, email_on_error_in boolean, provider_data_in jsonb",
          "body": "begin\n  insert\n    into worker_pools (worker_pool_id, provider_id, previous_provider_ids, description, config, created, last_modified, owner, email_on_error, provider_data)\n    values (worker_pool_id_in, provider_id_in, previous_provider_ids_in, description_in, config_in, created_in, last_modified_in, owner_in, email_on_error_in, provider_data_in);\nend",
          "deprecated": false,
          "description": "Create a new worker pool.  Raises UNIQUE_VIOLATION if the pool already exists.",
          "mode": "write",
          "returns": "void",
          "serviceName": "worker_manager"
        },
        "delete_worker_pool": {
          "args": "worker_pool_id_in text",
          "body": "begin\n  delete\n  from worker_pools\n  where worker_pools.worker_pool_id = worker_pool_id_in;\nend",
          "deprecated": false,
          "description": "Delete a worker pool immediately.",
          "mode": "write",
          "returns": "void",
          "serviceName": "worker_manager"
        },
        "expire_worker_pools": {
          "args": "",
          "body": "begin\n  return query delete\n  from worker_pools\n  where worker_pools.provider_id = 'null-provider'\n  and worker_pools.previous_provider_ids = '[]'::jsonb\n  returning worker_pools.worker_pool_id;\nend",
          "deprecated": false,
          "description": "Expire worker pools, deleting those which have provider-id null-provider and\nno previous_provider_ids.  Returns the worker pool ids that it deletes.",
          "mode": "write",
          "returns": "table(worker_pool_id text)",
          "serviceName": "worker_manager"
        },
        "get_worker_pool": {
          "args": "worker_pool_id_in text",
          "body": "begin\n  return query\n  select\n    worker_pools.worker_pool_id,\n    worker_pools.provider_id,\n    worker_pools.previous_provider_ids,\n    worker_pools.description,\n    worker_pools.config,\n    worker_pools.created,\n    worker_pools.last_modified,\n    worker_pools.owner,\n    worker_pools.email_on_error,\n    worker_pools.provider_data\n  from worker_pools\n  where worker_pools.worker_pool_id = worker_pool_id_in;\nend",
          "deprecated": false,
          "description": "Get an existig worker pool.  The returned table will have one or (if no such worker pool is defined) zero rows.",
          "mode": "read",
          "returns": "table(worker_pool_id text, provider_id text, previous_provider_ids jsonb, description text, config jsonb, created timestamptz, last_modified timestamptz, owner text, email_on_error boolean, provider_data jsonb)",
          "serviceName": "worker_manager"
        },
        "get_worker_pools": {
          "args": "page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    worker_pools.worker_pool_id,\n    worker_pools.provider_id,\n    worker_pools.previous_provider_ids,\n    worker_pools.description,\n    worker_pools.config,\n    worker_pools.created,\n    worker_pools.last_modified,\n    worker_pools.owner,\n    worker_pools.email_on_error,\n    worker_pools.provider_data\n  from worker_pools\n  order by worker_pool_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get existing worker pools, ordered by `worker_pool_id`.  If the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(worker_pool_id text, provider_id text, previous_provider_ids jsonb, description text, config jsonb, created timestamptz, last_modified timestamptz, owner text, email_on_error boolean, provider_data jsonb)",
          "serviceName": "worker_manager"
        },
        "remove_worker_pool_previous_provider_id": {
          "args": "worker_pool_id_in text, provider_id_in text",
          "body": "begin\n  update worker_pools\n  set\n    previous_provider_ids = previous_provider_ids - provider_id_in\n  where\n    worker_pool_id = worker_pool_id_in;\nend",
          "deprecated": false,
          "description": "Remove the given provider_id from the worker pool's previous_provider_ids.  It is\nnot an error if the worker pool does not exist, or if the provider_id is not in the\nprevious_provider_ids set.",
          "mode": "write",
          "returns": "void",
          "serviceName": "worker_manager"
        },
        "update_worker_pool": {
          "args": "worker_pool_id_in text, provider_id_in text, description_in text, config_in jsonb, last_modified_in timestamptz, owner_in text, email_on_error_in boolean",
          "body": "declare\n  existing record;\nbegin\n  select\n    worker_pools.provider_id,\n    worker_pools.previous_provider_ids\n  from worker_pools\n  where worker_pools.worker_pool_id = worker_pool_id_in\n  -- lock this row for the duration of this transaction..\n  for update\n  into existing;\n\n  -- update previous_provider_ids, if the provider_id has changed\n  if existing.provider_id <> provider_id_in then\n    -- remove both provider IDs to avoid duplicates, then re-add existing.provider_id\n    existing.previous_provider_ids = (existing.previous_provider_ids - provider_id_in - existing.provider_id) || jsonb_build_array(existing.provider_id);\n  end if;\n\n  return query update worker_pools\n  set\n    provider_id = provider_id_in,\n    description = description_in,\n    config = config_in,\n    last_modified = last_modified_in,\n    owner = owner_in,\n    email_on_error = email_on_error_in,\n    previous_provider_ids = existing.previous_provider_ids\n  where worker_pools.worker_pool_id = worker_pool_id_in\n  returning\n    worker_pools.worker_pool_id,\n    worker_pools.provider_id,\n    worker_pools.description,\n    worker_pools.config,\n    worker_pools.created,\n    worker_pools.last_modified,\n    worker_pools.owner,\n    worker_pools.email_on_error,\n    existing.provider_id as previous_provider_id;\nend",
          "deprecated": false,
          "description": "Update API-accessible columns on an existing worker pool.  All fields are\noverridden, but if the provider_id changes, then the existing provider_id\nis added to previous_provider_ids.  The return value contains values\nrequired for an API response and previous_provider_id (singular) containing\nthe provider_id found before the update.  If no such worker pool exists,\nthe return value is an empty set.",
          "mode": "write",
          "returns": "table(worker_pool_id text, provider_id text, description text, config jsonb, created timestamptz, last_modified timestamptz, owner text, email_on_error boolean, previous_provider_id text)",
          "serviceName": "worker_manager"
        },
        "update_worker_pool_provider_data": {
          "args": "worker_pool_id_in text, provider_id_in text, provider_data_in jsonb",
          "body": "begin\n  update worker_pools\n  set\n    provider_data = provider_data || jsonb_build_object(provider_id_in, provider_data_in)\n  where\n    worker_pool_id = worker_pool_id_in;\nend",
          "deprecated": false,
          "description": "Update the provider_data for the given provider_id in this worker pool.  Note that\nthis sets the provider_data property unconditionally, and it is up to the service\nto ensure that concurrent modifications do not occur.  It is not an error if the\nworker pool does not exist.",
          "mode": "write",
          "returns": "void",
          "serviceName": "worker_manager"
        },
        "wmworker_pools_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_row worker_pools%ROWTYPE;\nbegin\n  select\n    (properties ->> 'workerPoolId')::text as worker_pool_id,\n    (properties ->> 'providerId')::text as provider_id,\n    (properties ->> 'owner')::text as owner,\n    (properties ->> 'description')::text as description,\n    (properties -> 'emailOnError')::boolean as email_on_error,\n    (properties ->> 'created')::timestamptz as created,\n    (properties ->> 'lastModified')::timestamptz as last_modified,\n    entity_buf_decode(properties, 'config')::jsonb as config,\n    entity_buf_decode(properties, 'providerData')::jsonb as provider_data,\n    entity_buf_decode(properties, 'previousProviderIds')::jsonb as previous_provider_ids,\n    public.gen_random_uuid() as etag\n  into new_row;\n  if overwrite then\n    raise exception 'overwrite not implemented';\n  else\n    execute 'insert into worker_pools select $1.*' using new_row;\n  end if;\n  return new_row.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "worker_manager"
        },
        "wmworker_pools_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select\n    wmworker_pools_entities_load.partition_key,\n    'workerPool' as row_key,\n    entity_buf_encode(\n      entity_buf_encode(\n        entity_buf_encode(\n          jsonb_build_object(\n            'PartitionKey', wmworker_pools_entities_load.partition_key,\n            'RowKey', 'workerPool',\n            'workerPoolId', worker_pool_id,\n            'providerId', provider_id,\n            'owner', owner,\n            'description', description,\n            'emailOnError', email_on_error,\n            'created', created,\n            'lastModified', last_modified),\n          'config', config::text),\n        'providerData', provider_data::text),\n      'previousProviderIds', previous_provider_ids::text) as value,\n    1 as version,\n    worker_pools.etag as etag\n  from worker_pools\n  where\n    worker_pools.worker_pool_id = decode_string_key(wmworker_pools_entities_load.partition_key);\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworker_pools_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_row worker_pools%ROWTYPE;\nbegin\n  select\n    (properties ->> 'workerPoolId')::text as worker_pool_id,\n    (properties ->> 'providerId')::text as provider_id,\n    (properties ->> 'owner')::text as owner,\n    (properties ->> 'description')::text as description,\n    (properties -> 'emailOnError')::boolean as email_on_error,\n    (properties ->> 'created')::timestamptz as created,\n    (properties ->> 'lastModified')::timestamptz as last_modified,\n    entity_buf_decode(properties, 'config')::jsonb as config,\n    entity_buf_decode(properties, 'providerData')::jsonb as provider_data,\n    entity_buf_decode(properties, 'previousProviderIds')::jsonb as previous_provider_ids,\n    public.gen_random_uuid() as etag\n  into new_row;\n  update worker_pools\n  set (\n    provider_id,\n    owner,\n    description,\n    email_on_error,\n    created,\n    last_modified,\n    config,\n    provider_data,\n    previous_provider_ids,\n    etag\n  ) = (\n    new_row.provider_id,\n    new_row.owner,\n    new_row.description,\n    new_row.email_on_error,\n    new_row.created,\n    new_row.last_modified,\n    new_row.config,\n    new_row.provider_data,\n    new_row.previous_provider_ids,\n    new_row.etag\n  )\n  where\n    worker_pools.worker_pool_id = decode_string_key(wmworker_pools_entities_modify.partition_key) and\n    worker_pools.etag = wmworker_pools_entities_modify.old_etag;\n\n  if found then\n    return query select new_row.etag;\n    return;\n  end if;\n\n  perform worker_pools.etag from worker_pools\n  where worker_pools.worker_pool_id = decode_string_key(wmworker_pools_entities_modify.partition_key);\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworker_pools_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  delete\n  from worker_pools\n  where\n    worker_pools.worker_pool_id = decode_string_key(partition_key);\n  -- worker-manager does not care if the row existed\n  return query select gen_random_uuid() as etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworker_pools_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\nbegin\n  if not condition is null then\n    raise exception 'condition not supported';\n  end if;\n  return query\n    select\n      wmworker_pools_entities_scan.partition_key,\n      'workerPool' as row_key,\n      entity_buf_encode(\n        entity_buf_encode(\n          entity_buf_encode(\n            jsonb_build_object(\n              'PartitionKey', encode_string_key(worker_pool_id),\n              'RowKey', 'workerPool',\n              'workerPoolId', worker_pool_id,\n              'providerId', provider_id,\n              'owner', owner,\n              'description', description,\n              'emailOnError', email_on_error,\n              'created', created,\n              'lastModified', last_modified),\n            'config', config::text),\n          'providerData', provider_data::text),\n        'previousProviderIds', previous_provider_ids::text) as value,\n      1 as version,\n    worker_pools.etag as etag\n    from worker_pools\n    where\n      partition_key is null or\n      worker_pool_id = decode_string_key(partition_key)\n    order by worker_pool_id\n    limit size + 1\n    offset page;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "worker_manager"
        }
      },
      "migrationScript": "begin\n  -- decode the __buf encoding defined in tc-lib-entities entitytypes.js.\n  -- This is a re-definition of the function from DB version 8, where it incorrectly\n  -- handled rows with more than one buffer.\n  create or replace function entity_buf_decode(value JSONB, name text) RETURNS text\n  as $$\n      declare\n          buffer text = '';\n          chunks integer;\n          chunk integer = 0;\n      begin\n          chunks = (value ->> ('__bufchunks_' || name))::integer;\n          loop\n              exit when chunks is null or chunk >= chunks;\n              buffer = buffer || convert_from(decode((value ->> ('__buf' || chunk || '_' || name))::text, 'base64'), 'utf8');\n              chunk = chunk + 1;\n          end loop;\n          return buffer;\n      end;\n  $$\n  language plpgSQL\n  strict immutable;\n\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.  Note that this may lead to concurrent\n  -- updates failing; the important thing is that they not succeed without\n  -- taking effect.  Failed updates will be retried.\n  lock table wmworker_pools_entities;\n\n  create table worker_pools\n  as\n    select\n      (value ->> 'workerPoolId')::text as worker_pool_id,\n      (value ->> 'providerId')::text as provider_id,\n      (value ->> 'owner')::text as owner,\n      (value ->> 'description')::text as description,\n      (value -> 'emailOnError')::boolean as email_on_error,\n      (value ->> 'created')::timestamptz as created,\n      (value ->> 'lastModified')::timestamptz as last_modified,\n      entity_buf_decode(value, 'config')::jsonb as config,\n      entity_buf_decode(value, 'providerData')::jsonb as provider_data,\n      entity_buf_decode(value, 'previousProviderIds')::jsonb as previous_provider_ids,\n      etag\n    from wmworker_pools_entities;\n  alter table worker_pools add primary key (worker_pool_id);\n  alter table worker_pools\n    alter column provider_id set not null,\n    alter column owner set not null,\n    alter column description set not null,\n    alter column email_on_error set not null,\n    alter column created set not null,\n    alter column last_modified set not null,\n    alter column config set not null,\n    alter column provider_data set not null,\n    alter column previous_provider_ids set not null,\n    alter column etag set not null,\n    alter column etag set default public.gen_random_uuid();\n\n  revoke select, insert, update, delete on wmworker_pools_entities from $db_user_prefix$_worker_manager;\n  drop table wmworker_pools_entities;\n  grant select, insert, update, delete on worker_pools to $db_user_prefix$_worker_manager;\nend\n",
      "version": 10
    },
    {
      "description": "drop widgets table from version 1",
      "downgradeScript": "begin\n  create table widgets (\n    name text\n  );\n  grant select, insert, update, delete on widgets to $db_user_prefix$_notify;\nend",
      "methods": {
        "update_widgets": {
          "args": "name_in text",
          "body": "begin\n  return;\nend",
          "deprecated": true,
          "description": "Temporary method to test infrastructure support fo database access",
          "mode": "write",
          "returns": "table (name text)",
          "serviceName": "notify"
        }
      },
      "migrationScript": "begin\n  revoke select, insert, update, delete on widgets from $db_user_prefix$_notify;\n  drop table widgets;\nend",
      "version": 11
    },
    {
      "description": "worker-manager workers phase 2",
      "downgradeScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.\n  lock table workers;\n\n  create table wmworkers_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n  alter table wmworkers_entities add primary key (partition_key, row_key);\n\n  insert into wmworkers_entities\n  select\n    encode_string_key(worker_pool_id) as partition_key,\n    encode_composite_key(worker_group, worker_id) as row_key,\n    entity_buf_encode(\n        jsonb_build_object(\n          'PartitionKey', encode_string_key(worker_pool_id),\n          'RowKey', 'workerPool',\n          'workerPoolId', worker_pool_id,\n          'workerGroup', worker_group,\n          'workerId', worker_id,\n          'providerId', provider_id,\n          'created', created,\n          'expires', expires,\n          'state', state,\n          'capacity', capacity,\n          'lastModified', last_modified,\n          'lastChecked', last_checked),\n        'providerData', provider_data::text) as value,\n    1 as version,\n    etag\n  from workers;\n\n  revoke select, insert, update, delete on workers from $db_user_prefix$_worker_manager;\n  drop table workers;\n  grant select, insert, update, delete on wmworkers_entities to $db_user_prefix$_worker_manager;\nend\n",
      "methods": {
        "create_worker": {
          "args": "worker_pool_id_in text, worker_group_in text, worker_id_in text, provider_id_in text, created_in timestamptz, expires_in timestamptz, state_in text, provider_data_in jsonb, capacity_in integer, last_modified_in timestamptz, last_checked_in timestamptz",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  insert\n    into workers (worker_pool_id, worker_group, worker_id, provider_id, created, expires, state, provider_data, capacity, last_modified, last_checked, etag)\n    values (worker_pool_id_in, worker_group_in, worker_id_in, provider_id_in, created_in, expires_in, state_in, provider_data_in, capacity_in, last_modified_in, last_checked_in, new_etag);\n\n  return new_etag;\nend",
          "deprecated": false,
          "description": "Create a new worker. Raises UNIQUE_VIOLATION if the worker already exists.\nReturns the etag of the newly created worker.",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "worker_manager"
        },
        "delete_worker": {
          "args": "worker_pool_id_in text, worker_group_in text, worker_id_in text",
          "body": "begin\n  delete\n  from workers\n  where\n    workers.worker_pool_id = worker_pool_id_in and\n    workers.worker_group = worker_group_in and\n    workers.worker_id = worker_id_in;\nend",
          "deprecated": false,
          "description": "Delete a worker.",
          "mode": "write",
          "returns": "void",
          "serviceName": "worker_manager"
        },
        "expire_workers": {
          "args": "expires_in timestamptz",
          "body": "declare\n  count integer;\nbegin\n  delete from workers where workers.expires < expires_in;\n  if found then\n    get diagnostics count = row_count;\n    return count;\n  end if;\n  return 0;\nend",
          "deprecated": false,
          "description": "Expire workers that come before `expires_in`.\nReturns a count of rows that have been deleted.",
          "mode": "write",
          "returns": "integer",
          "serviceName": "worker_manager"
        },
        "get_worker": {
          "args": "worker_pool_id_in text, worker_group_in text, worker_id_in text",
          "body": "begin\n  return query\n  select\n    workers.worker_pool_id,\n    workers.worker_group,\n    workers.worker_id,\n    workers.provider_id,\n    workers.created,\n    workers.expires,\n    workers.state,\n    workers.provider_data,\n    workers.capacity,\n    workers.last_modified,\n    workers.last_checked,\n    workers.etag\n  from workers\n  where\n    workers.worker_pool_id = worker_pool_id_in and\n    workers.worker_group = worker_group_in and\n    workers.worker_id = worker_id_in;\nend",
          "deprecated": false,
          "description": "Get an existing worker. The returned table will have one or (if no such worker is defined) zero rows.",
          "mode": "read",
          "returns": "table(worker_pool_id text, worker_group text, worker_id text, provider_id text, created timestamptz, expires timestamptz, state text, provider_data jsonb, capacity integer, last_modified timestamptz, last_checked timestamptz, etag uuid)",
          "serviceName": "worker_manager"
        },
        "get_workers": {
          "args": "worker_pool_id_in text, worker_group_in text, worker_id_in text, state_in text, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    workers.worker_pool_id,\n    workers.worker_group,\n    workers.worker_id,\n    workers.provider_id,\n    workers.created,\n    workers.expires,\n    workers.state,\n    workers.provider_data,\n    workers.capacity,\n    workers.last_modified,\n    workers.last_checked\n  from workers\n  where\n    (workers.worker_pool_id = worker_pool_id_in or worker_pool_id_in is null) and\n    (workers.worker_group = worker_group_in or worker_group_in is null) and\n    (workers.worker_id = worker_id_in or worker_id_in is null) and\n    (workers.state = state_in or state_in is null)\n  order by worker_pool_id, worker_group, worker_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get existing workers filtered by the optional arguments,\nordered by `worker_pool_id`, `worker_group`, and  `worker_id`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(worker_pool_id text, worker_group text, worker_id text, provider_id text, created timestamptz, expires timestamptz, state text, provider_data jsonb, capacity integer, last_modified timestamptz, last_checked timestamptz)",
          "serviceName": "worker_manager"
        },
        "update_worker": {
          "args": "worker_pool_id_in text, worker_group_in text, worker_id_in text, provider_id_in text, created_in timestamptz, expires_in timestamptz, state_in text, provider_data_in jsonb, capacity_in integer, last_modified_in timestamptz, last_checked_in timestamptz, etag_in uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\n  updated_row workers%ROWTYPE;\nbegin\n  update workers\n  set (provider_id, created, expires, state, provider_data, capacity, last_modified, last_checked, etag) = (\n    coalesce(provider_id_in, workers.provider_id),\n    coalesce(created_in, workers.created),\n    coalesce(expires_in, workers.expires),\n    coalesce(state_in, workers.state),\n    coalesce(provider_data_in, workers.provider_data),\n    coalesce(capacity_in, workers.capacity),\n    coalesce(last_modified_in, workers.last_modified),\n    coalesce(last_checked_in, workers.last_checked),\n    new_etag\n  )\n  where\n    workers.worker_pool_id = worker_pool_id_in and\n    workers.worker_group = worker_group_in and\n    workers.worker_id = worker_id_in and\n    workers.etag = coalesce(etag_in, workers.etag)\n  returning\n    workers.worker_pool_id,\n    workers.worker_group,\n    workers.worker_id,\n    workers.provider_id,\n    workers.created,\n    workers.expires,\n    workers.state,\n    workers.provider_data,\n    workers.capacity,\n    workers.last_modified,\n    workers.last_checked,\n    workers.etag\n  into updated_row;\n\n  if found then\n    return query select\n      updated_row.worker_pool_id,\n      updated_row.worker_group,\n      updated_row.worker_id,\n      updated_row.provider_id,\n      updated_row.created,\n      updated_row.expires,\n      updated_row.state,\n      updated_row.provider_data,\n      updated_row.capacity,\n      updated_row.last_modified,\n      updated_row.last_checked,\n      updated_row.etag;\n    return;\n  end if;\n\n  perform workers.etag from workers\n    where\n      workers.worker_pool_id = worker_pool_id_in and\n      workers.worker_group = worker_group_in and\n      workers.worker_id = worker_id_in;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "Update a worker.\nReturns the up-to-date worker row that have the same worker_pool_id, worker_group, and worker_id.\nIf the etag argument is empty then the update will overwrite the matched row.\nElse, the function will fail if the etag is out of date. This is useful for concurency handling.",
          "mode": "write",
          "returns": "table(worker_pool_id text, worker_group text, worker_id text, provider_id text, created timestamptz, expires timestamptz, state text, provider_data jsonb, capacity integer, last_modified timestamptz, last_checked timestamptz, etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworkers_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_row workers%ROWTYPE;\nbegin\n  select\n    (properties ->> 'workerPoolId')::text,\n    (properties ->> 'workerGroup')::text,\n    (properties ->> 'workerId')::text,\n    (properties ->> 'providerId')::text,\n    (properties ->> 'created')::timestamptz,\n    (properties ->> 'expires')::timestamptz,\n    (properties ->> 'state')::text,\n    entity_buf_decode(properties, 'providerData')::jsonb,\n    (properties ->> 'capacity')::integer as capacity,\n    (properties ->> 'lastModified')::timestamptz,\n    (properties ->> 'lastChecked')::timestamptz,\n    public.gen_random_uuid()\n  into new_row;\n  if overwrite then\n    raise exception 'overwrite not implemented';\n  else\n    execute 'insert into workers select $1.*' using new_row;\n  end if;\n  return new_row.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "worker_manager"
        },
        "wmworkers_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "declare\n  decoded_composite_key text[];\nbegin\n  decoded_composite_key := decode_composite_key(wmworkers_entities_load.row_key);\n  return query\n  select\n    wmworkers_entities_load.partition_key,\n    wmworkers_entities_load.row_key,\n    entity_buf_encode(\n      jsonb_build_object(\n        'PartitionKey', encode_string_key(worker_pool_id),\n        'RowKey', encode_composite_key(worker_group, worker_id),\n        'workerPoolId', worker_pool_id,\n        'workerGroup', worker_group,\n        'workerId', worker_id,\n        'providerId', provider_id,\n        'created', created,\n        'expires', expires,\n        'state', state,\n        'capacity', capacity,\n        'lastModified', last_modified,\n        'lastChecked', last_checked),\n      'providerData', provider_data::text) as value,\n    1 as version,\n    workers.etag as etag\n  from workers\n  where\n    workers.worker_pool_id = decode_string_key(wmworkers_entities_load.partition_key) and workers.worker_group = decoded_composite_key[1] and workers.worker_id = decoded_composite_key[2];\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworkers_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_row workers%ROWTYPE;\n  decoded_composite_key text[];\nbegin\n  decoded_composite_key := decode_composite_key(wmworkers_entities_modify.row_key);\n  select\n    (properties ->> 'workerPoolId')::text,\n    (properties ->> 'workerGroup')::text,\n    (properties ->> 'workerId')::text,\n    (properties ->> 'providerId')::text,\n    (properties ->> 'created')::timestamptz,\n    (properties ->> 'expires')::timestamptz,\n    (properties ->> 'state')::text,\n    entity_buf_decode(properties, 'providerData')::jsonb,\n    (properties ->> 'capacity')::integer,\n    (properties ->> 'lastModified')::timestamptz,\n    (properties ->> 'lastChecked')::timestamptz,\n    public.gen_random_uuid() as etag\n  into new_row;\n  update workers\n  set (\n    provider_id,\n    created,\n    expires,\n    state,\n    capacity,\n    last_modified,\n    last_checked,\n    provider_data,\n    etag\n  ) = (\n    new_row.provider_id,\n    new_row.created,\n    new_row.expires,\n    new_row.state,\n    new_row.capacity,\n    new_row.last_modified,\n    new_row.last_checked,\n    new_row.provider_data,\n    new_row.etag\n  )\n  where\n    workers.worker_pool_id = decode_string_key(wmworkers_entities_modify.partition_key) and\n    workers.worker_group = decoded_composite_key[1] and\n    workers.worker_id = decoded_composite_key[2] and\n    workers.etag = wmworkers_entities_modify.old_etag;\n  if found then\n    return query select new_row.etag;\n    return;\n  end if;\n  perform workers.etag from workers\n  where\n    workers.worker_pool_id = decode_string_key(wmworkers_entities_modify.partition_key) and\n    workers.worker_group = decoded_composite_key[1] and\n    workers.worker_id = decoded_composite_key[2];\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworkers_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "declare\n  decoded_composite_key text[];\nbegin\n  decoded_composite_key := decode_composite_key(wmworkers_entities_remove.row_key);\n  return query delete from workers\n  where\n    workers.worker_pool_id = decode_string_key(wmworkers_entities_remove.partition_key) and\n    workers.worker_group = decoded_composite_key[1] and\n    workers.worker_id = decoded_composite_key[2]\n  returning workers.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworkers_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  cond text[];\n  exp_cond_operator text;\n  exp_cond_operand timestamptz;\n  partition_key_var text;\n  row_key_var text;\n  decoded_composite_key text[];\nbegin\n  decoded_composite_key := decode_composite_key(wmworkers_entities_scan.rk);\n  if not condition is null then\n    cond := regexp_split_to_array(condition, '\\s+');\n    exp_cond_operator := cond[4];\n    exp_cond_operand := cond[5] :: timestamptz;\n\n    return query select\n      encode_string_key(worker_pool_id) as partition_key,\n      encode_composite_key(worker_group, worker_id) as row_key,\n      entity_buf_encode(\n        jsonb_build_object(\n          'PartitionKey', encode_string_key(worker_pool_id),\n          'RowKey', encode_composite_key(worker_group, worker_id),\n          'workerPoolId', worker_pool_id,\n          'workerGroup', worker_group,\n          'workerId', worker_id,\n          'providerId', provider_id,\n          'created', created,\n          'expires', expires,\n          'state', state,\n          'capacity', capacity,\n          'lastModified', last_modified,\n          'lastChecked', last_checked),\n        'providerData', provider_data::text) as value,\n      1 as version,\n      workers.etag as etag from workers\n    where\n      (wmworkers_entities_scan.pk is null or decode_string_key(wmworkers_entities_scan.pk) = worker_pool_id) and\n      (wmworkers_entities_scan.rk is null or wmworkers_entities_scan.rk = decoded_composite_key[1] || '~' || decoded_composite_key[2]) and\n      case\n        when exp_cond_operator = '=' then expires = exp_cond_operand\n        when exp_cond_operator = '<' then expires < exp_cond_operand\n        when exp_cond_operator = '<=' then expires <= exp_cond_operand\n        when exp_cond_operator = '>' then expires > exp_cond_operand\n        when exp_cond_operator = '>=' then expires >= exp_cond_operand\n        else expires <> exp_cond_operand\n      end\n    order by workers.worker_pool_id, workers.worker_group, workers.worker_id\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (page is not null and page > 0) then page\n      else 0\n    end;\n  else\n    return query select\n      encode_string_key(worker_pool_id) as partition_key,\n      encode_composite_key(worker_group, worker_id) as row_key,\n      entity_buf_encode(\n        jsonb_build_object(\n          'PartitionKey', encode_string_key(worker_pool_id),\n          'RowKey', encode_composite_key(worker_group, worker_id),\n          'workerPoolId', worker_pool_id,\n          'workerGroup', worker_group,\n          'workerId', worker_id,\n          'providerId', provider_id,\n          'created', created,\n          'expires', expires,\n          'state', state,\n          'capacity', capacity,\n          'lastModified', last_modified,\n          'lastChecked', last_checked),\n        'providerData', provider_data::text) as value,\n      1 as version,\n      workers.etag as etag from workers\n    where\n      (wmworkers_entities_scan.pk is null or decode_string_key(wmworkers_entities_scan.pk) = worker_pool_id) and\n      (wmworkers_entities_scan.rk is null or (worker_group = decoded_composite_key[1] and worker_id = decoded_composite_key[2]))\n    order by workers.worker_pool_id, workers.worker_group, workers.worker_id\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (size is not null and size > 0 and page is not null and page > 0) then page\n      else 0\n    end;\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "worker_manager"
        }
      },
      "migrationScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.  Note that this may lead to concurrent\n  -- updates failing; the important thing is that they not succeed without\n  -- taking effect.  Failed updates will be retried.\n  lock table wmworkers_entities;\n\n  create table workers\n  as\n    select\n      (value ->> 'workerPoolId')::text as worker_pool_id,\n      (value ->> 'workerGroup')::text as worker_group,\n      (value ->> 'workerId')::text as worker_id,\n      (value ->> 'providerId')::text as provider_id,\n      (value ->> 'created')::timestamptz as created,\n      (value ->> 'expires')::timestamptz as expires,\n      (value ->> 'state')::text as state,\n      entity_buf_decode(value, 'providerData')::jsonb as provider_data,\n      (value ->> 'capacity')::integer as capacity,\n      (value ->> 'lastModified')::timestamptz as last_modified,\n      (value ->> 'lastChecked')::timestamptz as last_checked,\n      etag\n    from wmworkers_entities;\n  alter table workers add primary key (worker_pool_id, worker_group, worker_id);\n  alter table workers\n    alter column worker_pool_id set not null,\n    alter column worker_group set not null,\n    alter column worker_id set not null,\n    alter column provider_id set not null,\n    alter column created set not null,\n    alter column expires set not null,\n    alter column state set not null,\n    alter column provider_data set not null,\n    alter column capacity set not null,\n    alter column last_modified set not null,\n    alter column last_checked set not null,\n    alter column etag set not null,\n    alter column etag set default public.gen_random_uuid();\n\n  revoke select, insert, update, delete on wmworkers_entities from $db_user_prefix$_worker_manager;\n  drop table wmworkers_entities;\n  grant select, insert, update, delete on workers to $db_user_prefix$_worker_manager;\nend\n",
      "version": 12
    },
    {
      "description": "add capacity to worker pools",
      "methods": {
        "get_worker_pool": {
          "deprecated": true
        },
        "get_worker_pool_with_capacity": {
          "args": "worker_pool_id_in text",
          "body": "begin\n  return query\n  select\n    worker_pools.worker_pool_id,\n    worker_pools.provider_id,\n    worker_pools.previous_provider_ids,\n    worker_pools.description,\n    worker_pools.config,\n    worker_pools.created,\n    worker_pools.last_modified,\n    worker_pools.owner,\n    worker_pools.email_on_error,\n    worker_pools.provider_data,\n    coalesce((\n      select sum(workers.capacity) from workers where\n        workers.worker_pool_id = worker_pool_id_in and\n        workers.state != 'stopped'),\n      0)::integer -- sum always wants to return a bigint but we're always going to be safely within integer range\n  from worker_pools\n  where worker_pools.worker_pool_id = worker_pool_id_in;\nend",
          "deprecated": false,
          "description": "Get an existing worker pool.  The returned table will have one or (if no such worker pool is defined) zero rows.",
          "mode": "read",
          "returns": "table(worker_pool_id text, provider_id text, previous_provider_ids jsonb, description text, config jsonb, created timestamptz, last_modified timestamptz, owner text, email_on_error boolean, provider_data jsonb, current_capacity integer)",
          "serviceName": "worker_manager"
        },
        "get_worker_pools": {
          "deprecated": true
        },
        "get_worker_pools_with_capacity": {
          "args": "page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    worker_pools.worker_pool_id,\n    worker_pools.provider_id,\n    worker_pools.previous_provider_ids,\n    worker_pools.description,\n    worker_pools.config,\n    worker_pools.created,\n    worker_pools.last_modified,\n    worker_pools.owner,\n    worker_pools.email_on_error,\n    worker_pools.provider_data,\n    coalesce(sum(worker_capacities.capacity), 0)::integer -- sum always wants to return a bigint but we're always going to be safely within integer range\n  from worker_pools\n  left join (\n    select workers.worker_pool_id, workers.capacity from workers\n      where workers.state != 'stopped'\n  ) as worker_capacities\n  on worker_pools.worker_pool_id = worker_capacities.worker_pool_id\n  group by worker_pools.worker_pool_id\n  order by worker_pools.worker_pool_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get existing worker pools, ordered by `worker_pool_id`.  If the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(worker_pool_id text, provider_id text, previous_provider_ids jsonb, description text, config jsonb, created timestamptz, last_modified timestamptz, owner text, email_on_error boolean, provider_data jsonb, current_capacity integer)",
          "serviceName": "worker_manager"
        },
        "update_worker_pool": {
          "deprecated": true
        },
        "update_worker_pool_with_capacity": {
          "args": "worker_pool_id_in text, provider_id_in text, description_in text, config_in jsonb, last_modified_in timestamptz, owner_in text, email_on_error_in boolean",
          "body": "declare\n  existing record;\nbegin\n  select\n    worker_pools.provider_id,\n    worker_pools.previous_provider_ids\n  from worker_pools\n  where worker_pools.worker_pool_id = worker_pool_id_in\n  -- lock this row for the duration of this transaction..\n  for update\n  into existing;\n\n  -- update previous_provider_ids, if the provider_id has changed\n  if existing.provider_id <> provider_id_in then\n    -- remove both provider IDs to avoid duplicates, then re-add existing.provider_id\n    existing.previous_provider_ids = (existing.previous_provider_ids - provider_id_in - existing.provider_id) || jsonb_build_array(existing.provider_id);\n  end if;\n\n  return query update worker_pools\n  set\n    provider_id = provider_id_in,\n    description = description_in,\n    config = config_in,\n    last_modified = last_modified_in,\n    owner = owner_in,\n    email_on_error = email_on_error_in,\n    previous_provider_ids = existing.previous_provider_ids\n  where worker_pools.worker_pool_id = worker_pool_id_in\n  returning\n    worker_pools.worker_pool_id,\n    worker_pools.provider_id,\n    worker_pools.description,\n    worker_pools.config,\n    worker_pools.created,\n    worker_pools.last_modified,\n    worker_pools.owner,\n    worker_pools.email_on_error,\n    existing.provider_id as previous_provider_id,\n    coalesce((\n      select sum(workers.capacity) from workers where\n        workers.worker_pool_id = worker_pool_id_in and\n        workers.state != 'stopped'),\n      0)::integer; -- sum always wants to return a bigint but we're always going to be safely within integer range\nend",
          "deprecated": false,
          "description": "Update API-accessible columns on an existig worker pool.  All fields are\noverridden, but if the provider_id changes, then the existing provider_id\nis added to previous_provider_ids.  The return value contains values\nrequired for an API response and previous_provider_id (singular) containing\nthe provider_id found before the update.  If no such worker pool exists,\nthe return value is an empty set.",
          "mode": "write",
          "returns": "table(worker_pool_id text, provider_id text, description text, config jsonb, created timestamptz, last_modified timestamptz, owner text, email_on_error boolean, previous_provider_id text, current_capacity integer)",
          "serviceName": "worker_manager"
        }
      },
      "version": 13
    },
    {
      "description": "add secret to workers",
      "downgradeScript": "begin\n  alter table workers\n    drop column secret;\nend\n",
      "methods": {
        "get_worker": {
          "deprecated": true
        },
        "get_worker_2": {
          "args": "worker_pool_id_in text, worker_group_in text, worker_id_in text",
          "body": "begin\n  return query\n  select\n    workers.worker_pool_id,\n    workers.worker_group,\n    workers.worker_id,\n    workers.provider_id,\n    workers.created,\n    workers.expires,\n    workers.state,\n    workers.provider_data,\n    workers.capacity,\n    workers.last_modified,\n    workers.last_checked,\n    workers.secret,\n    workers.etag\n  from workers\n  where\n    workers.worker_pool_id = worker_pool_id_in and\n    workers.worker_group = worker_group_in and\n    workers.worker_id = worker_id_in;\nend",
          "deprecated": false,
          "description": "Get an existing worker. The returned table will have one or (if no such worker is defined) zero rows.",
          "mode": "read",
          "returns": "table(worker_pool_id text, worker_group text, worker_id text, provider_id text, created timestamptz, expires timestamptz, state text, provider_data jsonb, capacity integer, last_modified timestamptz, last_checked timestamptz, secret jsonb, etag uuid)",
          "serviceName": "worker_manager"
        },
        "update_worker": {
          "deprecated": true
        },
        "update_worker_2": {
          "args": "worker_pool_id_in text, worker_group_in text, worker_id_in text, provider_id_in text, created_in timestamptz, expires_in timestamptz, state_in text, provider_data_in jsonb, capacity_in integer, last_modified_in timestamptz, last_checked_in timestamptz, etag_in uuid, secret_in jsonb",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\n  updated_row workers%ROWTYPE;\nbegin\n  update workers\n  set (provider_id, created, expires, state, provider_data, capacity, last_modified, last_checked, etag, secret) = (\n    coalesce(provider_id_in, workers.provider_id),\n    coalesce(created_in, workers.created),\n    coalesce(expires_in, workers.expires),\n    coalesce(state_in, workers.state),\n    coalesce(provider_data_in, workers.provider_data),\n    coalesce(capacity_in, workers.capacity),\n    coalesce(last_modified_in, workers.last_modified),\n    coalesce(last_checked_in, workers.last_checked),\n    new_etag,\n    coalesce(secret_in, workers.secret)\n  )\n  where\n    workers.worker_pool_id = worker_pool_id_in and\n    workers.worker_group = worker_group_in and\n    workers.worker_id = worker_id_in and\n    workers.etag = coalesce(etag_in, workers.etag)\n  returning\n    workers.worker_pool_id,\n    workers.worker_group,\n    workers.worker_id,\n    workers.provider_id,\n    workers.created,\n    workers.expires,\n    workers.state,\n    workers.provider_data,\n    workers.capacity,\n    workers.last_modified,\n    workers.last_checked,\n    workers.etag,\n    workers.secret\n  into updated_row;\n\n  if found then\n    return query select\n      updated_row.worker_pool_id,\n      updated_row.worker_group,\n      updated_row.worker_id,\n      updated_row.provider_id,\n      updated_row.created,\n      updated_row.expires,\n      updated_row.state,\n      updated_row.provider_data,\n      updated_row.capacity,\n      updated_row.last_modified,\n      updated_row.last_checked,\n      updated_row.etag,\n      updated_row.secret;\n    return;\n  end if;\n\n  perform workers.etag from workers\n    where\n      workers.worker_pool_id = worker_pool_id_in and\n      workers.worker_group = worker_group_in and\n      workers.worker_id = worker_id_in;\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "Update a worker.\nReturns the up-to-date worker row that have the same worker_pool_id, worker_group, and worker_id.\nIf the etag argument is empty then the update will overwrite the matched row.\nElse, the function will fail if the etag is out of date. This is useful for concurency handling.",
          "mode": "write",
          "returns": "table(worker_pool_id text, worker_group text, worker_id text, provider_id text, created timestamptz, expires timestamptz, state text, provider_data jsonb, capacity integer, last_modified timestamptz, last_checked timestamptz, etag uuid, secret jsonb)",
          "serviceName": "worker_manager"
        }
      },
      "migrationScript": "begin\n  -- In an effort to support worker reregistration, an encrypted secret column is required.\n  -- https://github.com/taskcluster/taskcluster/issues/3011\n  alter table workers\n    add column secret jsonb default null;\nend\n",
      "version": 14
    },
    {
      "description": "add `sha512` function",
      "downgradeScript": "begin\n  drop function sha512(t text);\nend\n",
      "methods": {
      },
      "migrationScript": "begin\n  -- Compute the sha512 of the given text data.\n  -- sha512 is the algorithm that will be used to generate the hash.\n  -- This replaces Entity.keys.HashKey from taskcluster-lib-entities.\n  create or replace function sha512(t text) returns text\n  as $$\n      begin\n        return encode(digest(t, 'sha512'), 'hex');\n      end;\n  $$\n  language plpgSQL\n  strict immutable;\nend\n",
      "version": 15
    },
    {
      "description": "add `uuid_to_slugid` and `slugid_to_uuid` functions",
      "downgradeScript": "begin\n  drop function uuid_to_slugid(uuid text);\n  drop function slugid_to_uuid(slugid text);\nend\n",
      "methods": {
      },
      "migrationScript": "begin\n  -- SlugIDs are stored as UUIDs in Azure, because Azure stores that efficiently, but this makes it\n  -- difficult to query the DB since everywhere else we deal with SlugIDs in their 22-character\n  -- representation.  So these functions convert uuids to slugids and back.  Refer to\n  -- https://github.com/taskcluster/slugid/blob/53ec9a2de7140afff5b986c7c60a8028512e87d0/slugid.js\n  create or replace function uuid_to_slugid(uuid text) RETURNS text\n  as $$\n    begin\n      return left(replace(replace(encode(decode(replace(uuid, '-', ''), 'hex'), 'base64'), '+', '-'), '/', '_'), 22);\n    end;\n  $$\n  language plpgSQL\n  strict immutable;\n\n  create or replace function slugid_to_uuid(slugid text) RETURNS text\n  as $$\n    begin\n      return (encode(decode(replace(replace(slugid, '_', '/'), '-', '+') || '==', 'base64') , 'hex')::uuid)::text;\n    end;\n  $$\n  language plpgSQL\n  strict immutable;\nend\n",
      "version": 16
    },
    {
      "description": "notify phase 2 step 1+2",
      "downgradeScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.\n  lock table denylisted_notifications;\n\n  create table denylisted_notification_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n  alter table denylisted_notification_entities add primary key (partition_key, row_key);\n\n  insert into denylisted_notification_entities\n  select\n    encode_string_key(notification_type) as partition_key,\n    encode_string_key(notification_address) as row_key,\n    jsonb_build_object(\n      'PartitionKey', encode_string_key(notification_type),\n      'RowKey', encode_string_key(notification_address),\n      'notificationType', notification_type,\n      'notificationAddress', notification_address) as value,\n    1 as version,\n    etag\n  from denylisted_notifications;\n\n  revoke select, insert, update, delete on denylisted_notifications from $db_user_prefix$_notify;\n  drop table denylisted_notifications;\n  grant select, insert, update, delete on denylisted_notification_entities to $db_user_prefix$_notify;\nend\n",
      "methods": {
        "add_denylist_address": {
          "args": "notification_type_in text, notification_address_in text",
          "body": "begin\n  insert into denylisted_notifications(notification_type, notification_address, etag)\n  values (\n    notification_type_in,\n    notification_address_in,\n    public.gen_random_uuid()\n  ) on conflict do nothing;\nend",
          "deprecated": false,
          "description": "If the denylist address already exists, this is a no-op. Otherwise, add the denylist\naddress for the taskcluster-notify service, with a new random etag.",
          "mode": "write",
          "returns": "void",
          "serviceName": "notify"
        },
        "all_denylist_addresses": {
          "args": "page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query select denylisted_notifications.notification_type, denylisted_notifications.notification_address\n  from denylisted_notifications\n  order by 1, 2\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "List all denylist addresses for the taskcluster-notify service.",
          "mode": "read",
          "returns": "table (notification_type text, notification_address text)",
          "serviceName": "notify"
        },
        "delete_denylist_address": {
          "args": "notification_type_in text, notification_address_in text",
          "body": "begin\n  delete from denylisted_notifications where\n  denylisted_notifications.notification_type = notification_type_in and\n  denylisted_notifications.notification_address = notification_address_in;\n  if found then\n    return 1;\n  end if;\n  return 0;\nend",
          "deprecated": false,
          "description": "Delete a denylist address for the taskcluster-notify service.\nReturns number of rows deleted (0 or 1).",
          "mode": "write",
          "returns": "integer",
          "serviceName": "notify"
        },
        "denylisted_notification_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_row denylisted_notifications%ROWTYPE;\nbegin\n  select\n    (properties ->> 'notificationType')::text as notification_type,\n    (properties ->> 'notificationAddress')::text as notification_address,\n    public.gen_random_uuid() as etag\n  into new_row;\n  if overwrite then\n    raise exception 'overwrite not implemented';\n  else\n    execute 'insert into denylisted_notifications select $1.*' using new_row;\n  end if;\n  return new_row.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "notify"
        },
        "denylisted_notification_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select\n    denylisted_notification_entities_load.partition_key,\n    denylisted_notification_entities_load.row_key,\n    jsonb_build_object(\n      'PartitionKey', denylisted_notification_entities_load.partition_key,\n      'RowKey', denylisted_notification_entities_load.row_key,\n      'notificationType', notification_type,\n      'notificationAddress', notification_address) as value,\n    1 as version,\n    denylisted_notifications.etag as etag\n  from denylisted_notifications\n  where\n    denylisted_notifications.notification_type = decode_string_key(denylisted_notification_entities_load.partition_key) and denylisted_notifications.notification_address = decode_string_key(denylisted_notification_entities_load.row_key);\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "notify"
        },
        "denylisted_notification_entities_modify": {
          "deprecated": true
        },
        "denylisted_notification_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from denylisted_notifications\n  where\n    denylisted_notifications.notification_type = decode_string_key(denylisted_notification_entities_remove.partition_key) and denylisted_notifications.notification_address = decode_string_key(denylisted_notification_entities_remove.row_key)\n  returning denylisted_notifications.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "notify"
        },
        "denylisted_notification_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  cond text[];\n  exp_cond_operator text;\n  exp_cond_operand timestamptz;\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if not condition is null then\n    raise exception 'no condition support in denylisted_notification_entities_scan';\n  else\n    return query select\n      encode_string_key(denylisted_notifications.notification_type) as partition_key,\n      encode_string_key(denylisted_notifications.notification_address) as row_key,\n      jsonb_build_object(\n        'PartitionKey', encode_string_key(denylisted_notifications.notification_type),\n        'RowKey', encode_string_key(denylisted_notifications.notification_address),\n        'notificationType', notification_type,\n        'notificationAddress', notification_address\n      ) as value,\n      1 as version,\n      denylisted_notifications.etag as etag from denylisted_notifications\n    where\n      (denylisted_notification_entities_scan.pk is null or decode_string_key(denylisted_notification_entities_scan.pk) = notification_type) and\n      (denylisted_notification_entities_scan.rk is null or decode_string_key(denylisted_notification_entities_scan.rk) = notification_address)\n    order by denylisted_notifications.notification_type, denylisted_notifications.notification_address\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (size is not null and size > 0 and page is not null and page > 0) then page\n      else 0\n    end;\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "notify"
        },
        "exists_denylist_address": {
          "args": "notification_type_in text, notification_address_in text",
          "body": "begin\n  perform 1 from denylisted_notifications where\n  denylisted_notifications.notification_type = notification_type_in and\n  denylisted_notifications.notification_address = notification_address_in;\n  return found;\nend",
          "deprecated": false,
          "description": "Returns a boolean indicating whether the denylist type/address exists.",
          "mode": "read",
          "returns": "boolean",
          "serviceName": "notify"
        }
      },
      "migrationScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.  Note that this may lead to concurrent\n  -- updates failing; the important thing is that they not succeed without\n  -- taking effect.  Failed updates will be retried.\n  lock table denylisted_notification_entities;\n\n  create table denylisted_notifications\n  as\n    select\n      (value ->> 'notificationType')::text as notification_type,\n      (value ->> 'notificationAddress')::text as notification_address,\n      etag\n    from denylisted_notification_entities;\n  alter table denylisted_notifications add primary key (notification_type, notification_address);\n  alter table denylisted_notifications\n    alter column notification_type set not null,\n    alter column notification_address set not null,\n    alter column etag set default public.gen_random_uuid();\n\n  revoke select, insert, update, delete on denylisted_notification_entities from $db_user_prefix$_notify;\n  drop table denylisted_notification_entities;\n  grant select, insert, update, delete on denylisted_notifications to $db_user_prefix$_notify;\n\nend\n",
      "version": 17
    },
    {
      "description": "index phase 2 step 1",
      "downgradeScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.\n  lock table indexed_tasks;\n\n  raise log 'TIMING start indexed_tasks_entities create table';\n  create table indexed_tasks_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n\n  raise log 'TIMING start indexed_tasks_entities add primary key';\n  alter table indexed_tasks_entities add primary key (partition_key, row_key);\n\n  raise log 'TIMING start indexed_tasks_entities insert into';\n  insert into indexed_tasks_entities\n  select\n    sha512(namespace) as partition_key,\n    encode_string_key(name) as row_key,\n    entity_buf_encode(\n        jsonb_build_object(\n          'PartitionKey', sha512(namespace),\n          'RowKey', encode_string_key(name),\n          'namespace', namespace,\n          'name', name,\n          'rank', rank,\n          'taskId', task_id,\n          'expires', expires),\n        'data', data::text) as value,\n    1 as version,\n    etag\n  from indexed_tasks;\n\n  raise log 'TIMING start indexed_tasks_entities set permissions';\n  revoke select, insert, update, delete on indexed_tasks from $db_user_prefix$_index;\n  drop table indexed_tasks;\n  grant select, insert, update, delete on indexed_tasks_entities to $db_user_prefix$_index;\n\n\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.\n  lock table index_namespaces;\n\n  raise log 'TIMING start namespaces_entities create table';\n  create table namespaces_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n\n  raise log 'TIMING start namespaces_entities add primary key';\n  alter table namespaces_entities add primary key (partition_key, row_key);\n\n  raise log 'TIMING start namespaces_entities insert into';\n  insert into namespaces_entities\n  select\n    sha512(parent) as partition_key,\n    encode_string_key(name) as row_key,\n    jsonb_build_object(\n      'PartitionKey', sha512(parent),\n      'RowKey', encode_string_key(name),\n      'parent', parent,\n      'name', name,\n      'expires', expires),\n    1 as version,\n    etag\n  from index_namespaces;\n\n  raise log 'TIMING start namespaces_entities set permissions';\n  revoke select, insert, update, delete on index_namespaces from $db_user_prefix$_index;\n  drop table index_namespaces;\n  grant select, insert, update, delete on namespaces_entities to $db_user_prefix$_index;\nend\n",
      "methods": {
        "indexed_tasks_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_row indexed_tasks%ROWTYPE;\nbegin\n  select\n    (properties ->> 'namespace')::text,\n    (properties ->> 'name')::text,\n    (properties ->> 'rank')::text,\n    (properties ->> 'taskId')::text,\n    entity_buf_decode(properties, 'data')::jsonb,\n    (properties ->> 'expires')::timestamptz,\n    public.gen_random_uuid()\n  into new_row;\n  if overwrite then\n    raise exception 'overwrite not implemented';\n  else\n    execute 'insert into indexed_tasks select $1.*' using new_row;\n  end if;\n  return new_row.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "index"
        },
        "indexed_tasks_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select\n    indexed_tasks_entities_load.partition_key,\n    indexed_tasks_entities_load.row_key,\n    entity_buf_encode(\n      jsonb_build_object(\n        'PartitionKey', sha512(namespace),\n        'RowKey', encode_string_key(name),\n        'namespace', namespace,\n        'name', name,\n        'rank', rank,\n        'taskId', task_id,\n        'expires', expires),\n      'data', data::text) as value,\n    1 as version,\n    indexed_tasks.etag as etag\n  from indexed_tasks\n  where\n    sha512(indexed_tasks.namespace) = partition_key and\n    indexed_tasks.name = decode_string_key(row_key);\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "index"
        },
        "indexed_tasks_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_row indexed_tasks%ROWTYPE;\nbegin\n  select\n    (properties ->> 'namespace')::text,\n    (properties ->> 'name')::text,\n    (properties ->> 'rank')::integer,\n    (properties ->> 'taskId')::text,\n    entity_buf_decode(properties, 'data')::jsonb,\n    (properties ->> 'expires')::timestamptz,\n    public.gen_random_uuid() as etag\n  into new_row;\n  update indexed_tasks\n  set (\n    namespace,\n    name,\n    rank,\n    task_id,\n    data,\n    expires,\n    etag\n  ) = (\n    new_row.namespace,\n    new_row.name,\n    new_row.rank,\n    new_row.task_id,\n    new_row.data,\n    new_row.expires,\n    new_row.etag\n  )\n  where\n    sha512(indexed_tasks.namespace) = indexed_tasks_entities_modify.partition_key and\n    indexed_tasks.name = decode_string_key(indexed_tasks_entities_modify.row_key) and\n    indexed_tasks.etag = indexed_tasks_entities_modify.old_etag;\n  if found then\n    return query select new_row.etag;\n    return;\n  end if;\n  perform indexed_tasks.etag from indexed_tasks\n  where\n    sha512(indexed_tasks.namespace) = indexed_tasks_entities_modify.partition_key and\n    indexed_tasks.name = decode_string_key(indexed_tasks_entities_modify.row_key);\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise warning 'no such row... % pk: %, row_key %, new_row.etag %, old_etag %', properties, partition_key, row_key, new_row.etag, old_etag;\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "index"
        },
        "indexed_tasks_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from indexed_tasks\n  where\n    sha512(indexed_tasks.namespace) = indexed_tasks_entities_remove.partition_key and\n    indexed_tasks.name = decode_string_key(indexed_tasks_entities_remove.row_key)\n  returning indexed_tasks.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "index"
        },
        "indexed_tasks_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  cond text[];\n  exp_cond_operator text;\n  exp_cond_operand timestamptz;\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if not condition is null then\n    cond := regexp_split_to_array(condition, '\\s+');\n    exp_cond_operator := cond[4];\n    exp_cond_operand := cond[5] :: timestamptz;\n    return query select\n      sha512(namespace) as partition_key,\n      encode_string_key(name) as row_key,\n      entity_buf_encode(\n        jsonb_build_object(\n          'PartitionKey', sha512(namespace),\n          'RowKey', encode_string_key(name),\n          'namespace', namespace,\n          'name', name,\n          'rank', rank,\n          'taskId', task_id,\n          'expires', expires),\n        'data', data::text) as value,\n      1 as version,\n      indexed_tasks.etag as etag from indexed_tasks\n    where\n      (indexed_tasks_entities_scan.pk is null or sha512(namespace) = indexed_tasks_entities_scan.pk) and\n      (indexed_tasks_entities_scan.rk is null or decode_string_key(indexed_tasks_entities_scan.rk) = name) and\n      case\n        when exp_cond_operator = '=' then expires = exp_cond_operand\n        when exp_cond_operator = '<' then expires < exp_cond_operand\n        when exp_cond_operator = '<=' then expires <= exp_cond_operand\n        when exp_cond_operator = '>' then expires > exp_cond_operand\n        when exp_cond_operator = '>=' then expires >= exp_cond_operand\n        else expires <> exp_cond_operand\n      end\n    order by partition_key, indexed_tasks.name\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (page is not null and page > 0) then page\n      else 0\n    end;\n  else\n    return query select\n      sha512(namespace) as partition_key,\n      encode_string_key(name) as row_key,\n      entity_buf_encode(\n        jsonb_build_object(\n          'PartitionKey', sha512(namespace),\n          'RowKey', encode_string_key(name),\n          'namespace', namespace,\n          'name', name,\n          'rank', rank,\n          'taskId', task_id,\n          'expires', expires),\n        'data', data::text) as value,\n      1 as version,\n      indexed_tasks.etag as etag from indexed_tasks\n    where\n      (indexed_tasks_entities_scan.pk is null or sha512(namespace) = indexed_tasks_entities_scan.pk) and\n      (indexed_tasks_entities_scan.rk is null or decode_string_key(indexed_tasks_entities_scan.rk) = name)\n    order by partition_key, indexed_tasks.name\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (size is not null and size > 0 and page is not null and page > 0) then page\n      else 0\n    end;\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "index"
        },
        "namespaces_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_row index_namespaces%ROWTYPE;\nbegin\n  select\n    (properties ->> 'parent')::text,\n    (properties ->> 'name')::text,\n    (properties ->> 'expires')::timestamptz,\n    public.gen_random_uuid()\n  into new_row;\n  if overwrite then\n    raise exception 'overwrite not implemented';\n  else\n    execute 'insert into index_namespaces select $1.*' using new_row;\n  end if;\n  return new_row.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "index"
        },
        "namespaces_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select\n    namespaces_entities_load.partition_key,\n    namespaces_entities_load.row_key,\n    jsonb_build_object(\n      'PartitionKey', sha512(parent),\n      'RowKey', encode_string_key(name),\n      'parent', parent,\n      'name', name,\n      'expires', expires) as value,\n    1 as version,\n    index_namespaces.etag as etag\n  from index_namespaces\n  where\n    sha512(index_namespaces.parent) = partition_key and\n    index_namespaces.name = decode_string_key(row_key);\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "index"
        },
        "namespaces_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_row index_namespaces%ROWTYPE;\nbegin\n  select\n    (properties ->> 'parent')::text,\n    (properties ->> 'name')::text,\n    (properties ->> 'expires')::timestamptz,\n    public.gen_random_uuid() as etag\n  into new_row;\n  update index_namespaces\n  set (\n    parent,\n    name,\n    expires,\n    etag\n  ) = (\n    new_row.parent,\n    new_row.name,\n    new_row.expires,\n    new_row.etag\n  )\n  where\n    sha512(index_namespaces.parent) = namespaces_entities_modify.partition_key and\n    index_namespaces.name = decode_string_key(namespaces_entities_modify.row_key) and\n    index_namespaces.etag = namespaces_entities_modify.old_etag;\n  if found then\n    return query select new_row.etag;\n    return;\n  end if;\n  perform index_namespaces.etag from index_namespaces\n  where\n    sha512(index_namespaces.parent) = namespaces_entities_modify.partition_key and\n    index_namespaces.name = decode_string_key(namespaces_entities_modify.row_key);\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise warning 'no such row... % pk: %, row_key %, new_row.etag %, old_etag %', properties, partition_key, row_key, new_row.etag, old_etag;\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "index"
        },
        "namespaces_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from index_namespaces\n  where\n    sha512(index_namespaces.parent) = namespaces_entities_remove.partition_key and\n    index_namespaces.name = decode_string_key(namespaces_entities_remove.row_key)\n  returning index_namespaces.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "index"
        },
        "namespaces_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  cond text[];\n  exp_cond_operator text;\n  exp_cond_operand timestamptz;\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if not condition is null then\n    cond := regexp_split_to_array(condition, '\\s+');\n    exp_cond_operator := cond[4];\n    exp_cond_operand := cond[5] :: timestamptz;\n    return query select\n      sha512(parent) as partition_key,\n      encode_string_key(name) as row_key,\n      jsonb_build_object(\n        'PartitionKey', sha512(parent),\n        'RowKey', encode_string_key(name),\n        'parent', parent,\n        'name', name,\n        'expires', expires) as value,\n      1 as version,\n      index_namespaces.etag as etag from index_namespaces\n    where\n      (namespaces_entities_scan.pk is null or sha512(parent) = namespaces_entities_scan.pk) and\n      (namespaces_entities_scan.rk is null or decode_string_key(namespaces_entities_scan.rk) = name) and\n      case\n        when exp_cond_operator = '=' then expires = exp_cond_operand\n        when exp_cond_operator = '<' then expires < exp_cond_operand\n        when exp_cond_operator = '<=' then expires <= exp_cond_operand\n        when exp_cond_operator = '>' then expires > exp_cond_operand\n        when exp_cond_operator = '>=' then expires >= exp_cond_operand\n        else expires <> exp_cond_operand\n      end\n    order by partition_key, index_namespaces.name\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (page is not null and page > 0) then page\n      else 0\n    end;\n  else\n    return query select\n      sha512(parent) as partition_key,\n      encode_string_key(name) as row_key,\n      jsonb_build_object(\n        'PartitionKey', sha512(parent),\n        'RowKey', encode_string_key(name),\n        'parent', parent,\n        'name', name,\n        'expires', expires) as value,\n      1 as version,\n      index_namespaces.etag as etag from index_namespaces\n    where\n      (namespaces_entities_scan.pk is null or sha512(parent) = namespaces_entities_scan.pk) and\n      (namespaces_entities_scan.rk is null or decode_string_key(namespaces_entities_scan.rk) = name)\n    order by partition_key, index_namespaces.name\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (size is not null and size > 0 and page is not null and page > 0) then page\n      else 0\n    end;\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "index"
        }
      },
      "migrationScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.  Note that this may lead to concurrent\n  -- updates failing; the important thing is that they not succeed without\n  -- taking effect.  Failed updates will be retried.\n  lock table indexed_tasks_entities;\n\n  raise log 'TIMING start indexed_tasks create table .. as select';\n  create table indexed_tasks\n  as\n    select\n      (value ->> 'namespace')::text as namespace,\n      (value ->> 'name')::text as name,\n      (value ->> 'rank')::integer as rank,\n      (value ->> 'taskId')::text as task_id,\n      entity_buf_decode(value, 'data')::jsonb as data,\n      (value ->> 'expires')::timestamptz as expires,\n      etag\n    from indexed_tasks_entities;\n  raise log 'TIMING start indexed_tasks add primary key';\n  alter table indexed_tasks add primary key (namespace, name);\n  raise log 'TIMING start indexed_tasks set not null';\n  alter table indexed_tasks\n    alter column namespace set not null,\n    alter column name set not null,\n    alter column rank set not null,\n    alter column task_id set not null,\n    alter column expires set not null,\n    alter column data set not null,\n    alter column etag set not null,\n    alter column etag set default public.gen_random_uuid();\n\n  raise log 'TIMING start indexed_tasks set permissions';\n  revoke select, insert, update, delete on indexed_tasks_entities from $db_user_prefix$_index;\n  drop table indexed_tasks_entities;\n  grant select, insert, update, delete on indexed_tasks to $db_user_prefix$_index;\n\n\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.  Note that this may lead to concurrent\n  -- updates failing; the important thing is that they not succeed without\n  -- taking effect.  Failed updates will be retried.\n  lock table namespaces_entities;\n\n  raise log 'TIMING start index_namespaces create table .. as select';\n  create table index_namespaces\n  as\n    select\n      (value ->> 'parent')::text as parent,\n      (value ->> 'name')::text as name,\n      (value ->> 'expires')::timestamptz as expires,\n      etag\n    from namespaces_entities;\n  raise log 'TIMING start index_namespaces add primary key';\n  alter table index_namespaces add primary key (parent, name);\n  raise log 'TIMING start index_namespaces set not null';\n  alter table index_namespaces\n    alter column parent set not null,\n    alter column name set not null,\n    alter column expires set not null,\n    alter column etag set not null,\n    alter column etag set default public.gen_random_uuid();\n\n  -- drop that index later when we drop all of the entities support\n  raise log 'TIMING start index_namespaces add sha512_index_namespaces_idx';\n  create index sha512_index_namespaces_idx on index_namespaces (sha512(parent), name);\n  raise log 'TIMING start index_namespaces add sha512_indexed_tasks_idx';\n  create index sha512_indexed_tasks_idx on indexed_tasks (sha512(namespace), name);\n\n  raise log 'TIMING start index_namespaces set permissions';\n  revoke select, insert, update, delete on namespaces_entities from $db_user_prefix$_index;\n  drop table namespaces_entities;\n  grant select, insert, update, delete on index_namespaces to $db_user_prefix$_index;\nend\n",
      "version": 18
    },
    {
      "description": "queue artifacts phase 2 step 1 (slow migration: about 20μs per row in table `queue_artifacts_entities`)",
      "downgradeScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.\n  lock table queue_artifacts;\n\n  raise log 'TIMING start queue_artifacts_entities create table';\n  create table queue_artifacts_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n\n  raise log 'TIMING start queue_artifacts_entities primary key';\n  alter table queue_artifacts_entities add primary key (partition_key, row_key);\n\n  raise log 'TIMING start queue_artifacts_entities insert into';\n  insert into queue_artifacts_entities\n  select\n    encode_composite_key(task_id, run_id::text) as partition_key,\n    encode_string_key(name) as row_key,\n    entity_buf_encode(\n      jsonb_build_object(\n        'PartitionKey', encode_composite_key(task_id, run_id::text),\n        'RowKey', encode_string_key(name),\n        'taskId', slugid_to_uuid(task_id),\n        'runId', run_id,\n        'name', name,\n        'storageType', storage_type,\n        'contentType', content_type,\n        'present', present,\n        'expires', expires),\n      'details', details::text) as value,\n    1 as version,\n    etag\n  from queue_artifacts;\n\n  raise log 'TIMING start queue_artifacts_entities permissions';\n  revoke select, insert, update, delete on queue_artifacts from $db_user_prefix$_queue;\n  drop table queue_artifacts;\n  grant select, insert, update, delete on queue_artifacts_entities to $db_user_prefix$_queue;\nend\n",
      "methods": {
        "queue_artifacts_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_row queue_artifacts%ROWTYPE;\nbegin\n  select\n    uuid_to_slugid(properties ->> 'taskId'),\n    (properties ->> 'runId')::integer,\n    (properties ->> 'name')::text,\n    (properties ->> 'storageType')::text,\n    (properties ->> 'contentType')::text,\n    entity_buf_decode(properties, 'details')::jsonb,\n    (properties ->> 'present')::boolean,\n    (properties ->> 'expires')::timestamptz,\n    public.gen_random_uuid()\n  into new_row;\n  if overwrite then\n    raise exception 'overwrite not implemented';\n  else\n    execute 'insert into queue_artifacts select $1.*' using new_row;\n  end if;\n  return new_row.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "queue_artifacts_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "declare\n  decoded_composite_key text[];\nbegin\n  decoded_composite_key := decode_composite_key(queue_artifacts_entities_load.partition_key);\n  return query\n  select\n    queue_artifacts_entities_load.partition_key,\n    queue_artifacts_entities_load.row_key,\n    entity_buf_encode(\n      jsonb_build_object(\n        'PartitionKey', queue_artifacts_entities_load.partition_key,\n        'RowKey', queue_artifacts_entities_load.row_key,\n        'taskId', slugid_to_uuid(task_id),\n        'runId', run_id,\n        'name', name,\n        'storageType', storage_type,\n        'contentType', content_type,\n        'present', present,\n        'expires', expires),\n      'details', details::text) as value,\n    1 as version,\n    queue_artifacts.etag as etag\n  from queue_artifacts\n  where\n    queue_artifacts.task_id = decoded_composite_key[1] and queue_artifacts.run_id = decoded_composite_key[2]::int and queue_artifacts.name = decode_string_key(queue_artifacts_entities_load.row_key);\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_artifacts_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_row queue_artifacts%ROWTYPE;\n  decoded_composite_key text[];\nbegin\n  decoded_composite_key := decode_composite_key(queue_artifacts_entities_modify.partition_key);\n  select\n    (properties ->> 'taskId')::text,\n    (properties ->> 'runId')::integer,\n    (properties ->> 'name')::text,\n    (properties ->> 'storageType')::text,\n    (properties ->> 'contentType')::text,\n    entity_buf_decode(properties, 'details')::jsonb,\n    (properties ->> 'present')::boolean,\n    (properties ->> 'expires')::timestamptz,\n    public.gen_random_uuid()\n  into new_row;\n  update queue_artifacts\n  set (\n    storage_type,\n    content_type,\n    details,\n    present,\n    expires,\n    etag\n  ) = (\n    new_row.storage_type,\n    new_row.content_type,\n    new_row.details,\n    new_row.present,\n    new_row.expires,\n    new_row.etag\n  )\n  where\n    queue_artifacts.task_id = decoded_composite_key[1] and\n    queue_artifacts.run_id = decoded_composite_key[2]::int and\n    queue_artifacts.name = decode_string_key(queue_artifacts_entities_modify.row_key) and\n    queue_artifacts.etag = queue_artifacts_entities_modify.old_etag;\n  if found then\n    return query select new_row.etag;\n    return;\n  end if;\n  perform queue_artifacts.etag from queue_artifacts\n  where\n    queue_artifacts.task_id = decoded_composite_key[1] and\n    queue_artifacts.run_id = decoded_composite_key[2]::int and\n    queue_artifacts.name = decode_string_key(queue_artifacts_entities_modify.row_key);\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_artifacts_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "declare\n  decoded_composite_key text[];\nbegin\n  decoded_composite_key := decode_composite_key(queue_artifacts_entities_remove.partition_key);\n  return query delete from queue_artifacts\n  where\n    queue_artifacts.task_id = decoded_composite_key[1] and queue_artifacts.run_id = decoded_composite_key[2]::int and queue_artifacts.name = decode_string_key(queue_artifacts_entities_remove.row_key)\n  returning queue_artifacts.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_artifacts_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  cond text[];\n  exp_cond_operator text;\n  exp_cond_operand timestamptz;\n  partition_key_var text;\n  row_key_var text;\n  decoded_composite_key text[];\nbegin\n  decoded_composite_key := decode_composite_key(queue_artifacts_entities_scan.pk);\n  if not condition is null then\n    cond := regexp_split_to_array(condition, '\\s+');\n    exp_cond_operator := cond[4];\n    exp_cond_operand := cond[5] :: timestamptz;\n\n    return query select\n      encode_composite_key(queue_artifacts.task_id, queue_artifacts.run_id::text) as partition_key,\n      encode_string_key(queue_artifacts.name) as row_key,\n      entity_buf_encode(\n        jsonb_build_object(\n          'PartitionKey', encode_composite_key(queue_artifacts.task_id, queue_artifacts.run_id::text),\n          'RowKey', name,\n          'taskId', slugid_to_uuid(task_id),\n          'runId', run_id,\n          'name', name,\n          'storageType', storage_type,\n          'contentType', content_type,\n          'present', present,\n          'expires', expires\n        ),\n      'details', details::text) as value,\n      1 as version,\n      queue_artifacts.etag as etag from queue_artifacts\n    where\n      (queue_artifacts_entities_scan.pk is null or queue_artifacts_entities_scan.pk = decoded_composite_key[1] || '~' || decoded_composite_key[2]) and\n      (queue_artifacts_entities_scan.rk is null or queue_artifacts_entities_scan.rk = name) and\n      case\n        when exp_cond_operator = '=' then expires = exp_cond_operand\n        when exp_cond_operator = '<' then expires < exp_cond_operand\n        when exp_cond_operator = '<=' then expires <= exp_cond_operand\n        when exp_cond_operator = '>' then expires > exp_cond_operand\n        when exp_cond_operator = '>=' then expires >= exp_cond_operand\n        else expires <> exp_cond_operand\n      end\n    order by queue_artifacts.task_id, queue_artifacts.run_id, queue_artifacts.name\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (page is not null and page > 0) then page\n      else 0\n    end;\n  else\n    return query select\n      encode_composite_key(queue_artifacts.task_id, queue_artifacts.run_id::text) as partition_key,\n      encode_string_key(queue_artifacts.name) as row_key,\n      entity_buf_encode(\n        jsonb_build_object(\n          'PartitionKey', encode_composite_key(queue_artifacts.task_id, queue_artifacts.run_id::text),\n          'RowKey', name,\n          'taskId', slugid_to_uuid(task_id),\n          'runId', run_id,\n          'name', name,\n          'storageType', storage_type,\n          'contentType', content_type,\n          'present', present,\n          'expires', expires),\n        'details', details::text) as value,\n      1 as version,\n      queue_artifacts.etag as etag from queue_artifacts\n    where\n      (queue_artifacts_entities_scan.pk is null or (queue_artifacts.task_id = decoded_composite_key[1] and queue_artifacts.run_id = decoded_composite_key[2]::int)) and\n      (queue_artifacts_entities_scan.rk is null or queue_artifacts_entities_scan.rk = name)\n    order by queue_artifacts.task_id, queue_artifacts.run_id, queue_artifacts.name\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (size is not null and size > 0 and page is not null and page > 0) then page\n      else 0\n    end;\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        }
      },
      "migrationScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.  Note that this may lead to concurrent\n  -- updates failing; the important thing is that they not succeed without\n  -- taking effect.  Failed updates will be retried.\n  lock table queue_artifacts_entities;\n\n  raise log 'TIMING start queue_artifacts create table .. as select';\n  create table queue_artifacts\n  as\n    select\n      artifacts_entities.task_id as task_id,\n      (value ->> 'runId')::integer as run_id,\n      (value ->> 'name')::text as name,\n      (value ->> 'storageType')::text as storage_type,\n      (value ->> 'contentType')::text as content_type,\n      entity_buf_decode(value, 'details')::jsonb as details,\n      (value ->> 'present')::boolean as present,\n      (value ->> 'expires')::timestamptz as expires,\n      etag\n    from (\n      select\n        *,\n        uuid_to_slugid(value ->> 'taskId')::text as task_id\n      from queue_artifacts_entities\n    ) as artifacts_entities;\n  raise log 'TIMING start queue_artifacts add primary key';\n  alter table queue_artifacts add primary key (task_id, run_id, name);\n  raise log 'TIMING start queue_artifacts set not null';\n  alter table queue_artifacts\n    alter column task_id set not null,\n    alter column run_id set not null,\n    alter column name set not null,\n    alter column storage_type set not null,\n    alter column content_type set not null,\n    alter column details set not null,\n    alter column present set not null,\n    alter column expires set not null,\n    alter column etag set not null,\n    alter column etag set default public.gen_random_uuid();\n\n  raise log 'TIMING start queue_artifacts permissions';\n  revoke select, insert, update, delete on queue_artifacts_entities from $db_user_prefix$_queue;\n  drop table queue_artifacts_entities;\n  grant select, insert, update, delete on queue_artifacts to $db_user_prefix$_queue;\nend\n",
      "version": 19
    },
    {
      "description": "queue tasks phase 2 step 1 (slow migration: 700-2000μs per row in table `queue_tasks_entities`)",
      "downgradeScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.\n  lock table tasks;\n\n  raise log 'TIMING start queue_tasks_entities create table';\n  create table queue_tasks_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n\n  raise log 'TIMING start queue_tasks_entities primary key';\n  alter table queue_tasks_entities add primary key (partition_key, row_key);\n\n  raise log 'TIMING start queue_task_group_members_entities create table';\n  create table queue_task_group_members_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n\n  raise log 'TIMING start queue_task_group_members_entities primary key';\n  alter table queue_task_group_members_entities add primary key (partition_key, row_key);\n\n  raise log 'TIMING start queue_task_group_active_sets_entities create table';\n  create table queue_task_group_active_sets_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n\n  raise log 'TIMING start queue_task_group_active_sets_entities primary key';\n  alter table queue_task_group_active_sets_entities add primary key (partition_key, row_key);\n\n  raise log 'TIMING start queue_tasks_entities insert';\n  insert into queue_tasks_entities\n  select\n    -- note, the partition_key is the slugid form, not uuid, and contains no\n    -- characters that must be escaped in an Azure key\n    task_id as partition_key,\n    'task' as row_key,\n    entity_buf_encode(\n      entity_buf_encode(\n        entity_buf_encode(\n          entity_buf_encode(\n            entity_buf_encode(\n              entity_buf_encode(\n                entity_buf_encode(\n                  entity_buf_encode(\n                    jsonb_build_object(\n                      'PartitionKey', task_id,\n                      'RowKey', 'task',\n                      'taskId', slugid_to_uuid(task_id),\n                      'provisionerId', provisioner_id,\n                      'workerType', worker_type,\n                      'schedulerId', scheduler_id,\n                      'taskGroupId', slugid_to_uuid(task_group_id),\n                      'requires', requires::text,\n                      'priority', priority::text,\n                      'retries', retries,\n                      'retriesLeft', retries_left,\n                      'created', created,\n                      'deadline', deadline,\n                      'expires', expires,\n                      'takenUntil', coalesce(taken_until, '1970-01-01 00:00:00+00'::timestamptz)),\n                    'dependencies', dependencies::text),\n                  'routes', routes::text),\n                'scopes', scopes::text),\n              'payload', payload::text),\n            'metadata', metadata::text),\n          'tags', tags::text),\n        'extra', extra::text),\n      'runs', runs::text) as value,\n    1 as version,\n    etag\n  from tasks;\n\n  -- queue_task_group_members_entities is just a different index on the tasks\n  -- table, so we reconstruct it from the tasks\n  raise log 'TIMING start queue_task_group_members_entities insert';\n  insert into queue_task_group_members_entities\n  select\n    task_group_id as partition_key,\n    task_id as row_key,\n    jsonb_build_object(\n      'PartitionKey', task_group_id,\n      'RowKey', task_id,\n      'taskId', slugid_to_uuid(task_id),\n      'taskGroupId', slugid_to_uuid(task_group_id)) as value,\n    1 as version,\n    etag\n  from tasks;\n\n  -- similarly for queue_task_group_active_sets_entities\n  raise log 'TIMING start queue_task_group_active_sets_entities insert';\n  insert into queue_task_group_active_sets_entities\n  select\n    task_group_id as partition_key,\n    task_id as row_key,\n    jsonb_build_object(\n      'PartitionKey', task_group_id,\n      'RowKey', task_id,\n      'taskId', slugid_to_uuid(task_id),\n      'taskGroupId', slugid_to_uuid(task_group_id)) as value,\n    1 as version,\n    etag\n  from tasks\n  where not ever_resolved;\n\n  raise log 'TIMING start queue_{tasks,task_group_{active_sets.members}_entities} permissions';\n  revoke select, insert, update, delete on tasks from $db_user_prefix$_queue;\n  drop table tasks;\n\n  grant select, insert, update, delete on queue_tasks_entities to $db_user_prefix$_queue;\n  grant select, insert, update, delete on queue_task_group_members_entities to $db_user_prefix$_queue;\n  grant select, insert, update, delete on queue_task_group_active_sets_entities to $db_user_prefix$_queue;\n\n  drop type task_requires;\n  drop type task_priority;\nend\n",
      "methods": {
        "queue_task_group_active_sets_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "begin\n  -- this method is only called before the task is created, and tasks are\n  -- created in an unresolved state, so there's nothing to do here.  Note\n  -- that the service does not mark a task as unresolved when rerunning it\n  return public.gen_random_uuid();\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "queue_task_group_active_sets_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select\n    task_group_id,\n    task_id,\n    jsonb_build_object(\n      'PartitionKey', task_group_id,\n      'RowKey', task_id,\n      'taskId', slugid_to_uuid(task_id),\n      'taskGroupId', slugid_to_uuid(task_group_id),\n      'expires', expires) as value,\n    1 as version,\n    tasks.etag as etag\n  from tasks\n  where\n    not ever_resolved and\n    tasks.task_group_id = partition_key and\n    tasks.task_id = row_key;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_group_active_sets_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "begin\n  raise exception 'modify not supported';\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_group_active_sets_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  -- DependencyTracker uses this to mark a task as ever_resolved\n  update tasks\n  set ever_resolved = true\n  where\n    tasks.task_group_id = partition_key and\n    tasks.task_id = row_key;\n  return query select public.gen_random_uuid() as etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_group_active_sets_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  cond text[];\n  exp_cond_field text;\n  exp_cond_operator text;\n  exp_cond_operand timestamptz;\nbegin\n  if not condition is null then\n    cond := regexp_split_to_array(condition, '\\s+');\n    exp_cond_field := trim(cond[3], '''');\n\n    if exp_cond_field != 'expires' then\n      raise exception 'queue_task_group_active_sets_entities_scan only supports filtering for expired rows';\n    end if;\n\n    -- there's no distinct table to expire, so just return an empty set to make the\n    -- expiration crontask finish immediately\n    return;\n  end if;\n\n  -- the idea here is to use the tasks_task_group_id_unresolved_idx index for a quick response\n  return query select\n    task_group_id as partition_key,\n    task_id as row_key,\n    jsonb_build_object(\n      'PartitionKey', task_group_id,\n      'RowKey', task_id,\n      'taskGroupId', slugid_to_uuid(task_group_id),\n      'taskId', slugid_to_uuid(task_id),\n      'expires', expires) as value,\n    1 as version,\n    public.gen_random_uuid() as etag\n  from tasks\n  where\n    (pk is NULL or pk = task_group_id) and\n    not ever_resolved\n  order by tasks.task_group_id, tasks.task_id\n  limit case\n    when (size is not null and size > 0) then size + 1\n    else null\n  end\n  offset case\n    when (page is not null and page > 0) then page\n    else 0\n  end;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_group_members_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "begin\n  return public.gen_random_uuid();\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "queue_task_group_members_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select\n    task_group_id,\n    task_id,\n    jsonb_build_object(\n      'PartitionKey', task_group_id,\n      'RowKey', task_id,\n      'taskId', slugid_to_uuid(task_id),\n      'taskGroupId', slugid_to_uuid(task_group_id),\n      'expires', expires) as value,\n    1 as version,\n    tasks.etag as etag\n  from tasks\n  where\n    tasks.task_group_id = partition_key and\n    tasks.task_id = row_key;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_group_members_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "begin\n  raise exception 'modify not implemented for queue_task_group_members_entities';\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_group_members_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select tasks.etag\n  from tasks\n  where\n    tasks.task_group_id = partition_key and\n    tasks.task_id = row_key;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_group_members_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  cond text[];\n  exp_cond_field text;\n  exp_cond_operator text;\n  exp_cond_operand timestamptz;\nbegin\n  if pk is null then\n    -- if pk is null, then this is the expiration scan for the members table; since\n    -- there is no members table, we stub this out by simply returning an empty set,\n    -- ignoring the condition\n    return;\n  end if;\n\n  if not condition is null then\n    cond := regexp_split_to_array(condition, '\\s+');\n    exp_cond_field := trim(cond[3], '''');\n    exp_cond_operator := cond[4];\n    exp_cond_operand := cond[5] :: timestamptz;\n\n    if exp_cond_operator != '>=' or exp_cond_field != 'expires' then\n      raise exception 'queue_task_group_memberss_entities_scan only supports `expires >= <timestamp>` conditions';\n    end if;\n  end if;\n\n  return query\n  select\n    task_group_id,\n    task_id,\n    jsonb_build_object(\n      'PartitionKey', task_group_id,\n      'RowKey', task_id,\n      'taskId', slugid_to_uuid(task_id),\n      'taskGroupId', slugid_to_uuid(task_group_id),\n      'expires', expires) as value,\n    1 as version,\n    tasks.etag as etag\n  from tasks\n  where\n    tasks.task_group_id = pk and\n    (exp_cond_operand is NULL or expires >= exp_cond_operand)\n  order by tasks.task_id\n  limit case\n    when (size is not null and size > 0) then size + 1\n    else null\n  end\n  offset case\n    when (page is not null and page > 0) then page\n    else 0\n  end;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_tasks_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_row tasks%ROWTYPE;\nbegin\n  select\n    uuid_to_slugid(properties ->> 'taskId'),\n    (properties ->> 'provisionerId')::text,\n    (properties ->> 'workerType')::text,\n    (properties ->> 'schedulerId')::text,\n    uuid_to_slugid(properties ->> 'taskGroupId')::text,\n    entity_buf_decode(properties, 'dependencies')::jsonb,\n    (properties ->> 'requires')::task_requires,\n    entity_buf_decode(properties, 'routes')::jsonb,\n    (properties ->> 'priority')::task_priority,\n    (properties ->> 'retries')::int,\n    (properties ->> 'retriesLeft')::int,\n    (properties ->> 'created')::timestamptz,\n    (properties ->> 'deadline')::timestamptz,\n    (properties ->> 'expires')::timestamptz,\n    entity_buf_decode(properties, 'scopes')::jsonb,\n    entity_buf_decode(properties, 'payload')::jsonb,\n    entity_buf_decode(properties, 'metadata')::jsonb,\n    entity_buf_decode(properties, 'tags')::jsonb,\n    entity_buf_decode(properties, 'extra')::jsonb,\n    entity_buf_decode(properties, 'runs')::jsonb,\n    case (properties ->> 'takenUntil')::timestamptz\n      when '1970-01-01 00:00:00+00'::timestamptz then null\n      else (properties ->> 'takenUntil')::timestamptz\n    end,\n    false,\n    public.gen_random_uuid()\n  into new_row;\n  if overwrite then\n    raise exception 'overwrite not implemented';\n  else\n    execute 'insert into tasks select $1.*' using new_row;\n  end if;\n  return new_row.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "queue_tasks_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select\n    queue_tasks_entities_load.partition_key,\n    queue_tasks_entities_load.row_key,\n    entity_buf_encode(\n      entity_buf_encode(\n        entity_buf_encode(\n          entity_buf_encode(\n            entity_buf_encode(\n              entity_buf_encode(\n                entity_buf_encode(\n                  entity_buf_encode(\n                    jsonb_build_object(\n                      'PartitionKey', task_id,\n                      'RowKey', 'task',\n                      'taskId', slugid_to_uuid(task_id),\n                      'provisionerId', provisioner_id,\n                      'workerType', worker_type,\n                      'schedulerId', scheduler_id,\n                      'taskGroupId', slugid_to_uuid(task_group_id),\n                      'requires', requires::text,\n                      'priority', priority::text,\n                      'retries', retries,\n                      'retriesLeft', retries_left,\n                      'created', created,\n                      'deadline', deadline,\n                      'expires', expires,\n                      'takenUntil', coalesce(taken_until, '1970-01-01 00:00:00+00'::timestamptz)),\n                    'dependencies', dependencies::text),\n                  'routes', routes::text),\n                'scopes', scopes::text),\n              'payload', payload::text),\n            'metadata', metadata::text),\n          'tags', tags::text),\n        'extra', extra::text),\n      'runs', runs::text) as value,\n    1 as version,\n    tasks.etag as etag\n  from tasks\n  where\n    tasks.task_id = partition_key;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_tasks_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid;\nbegin\n  -- NOTE: queue only updates runs, retriesLeft, and takenUntil, so only those fields are\n  -- supported here.\n  new_etag = public.gen_random_uuid();\n  update tasks\n  set (\n    runs,\n    retries_left,\n    taken_until,\n    etag\n  ) = (\n    entity_buf_decode(properties, 'runs')::jsonb,\n    (properties ->> 'retriesLeft')::int,\n    case (properties ->> 'takenUntil')::timestamptz\n      when '1970-01-01 00:00:00+00'::timestamptz then null\n      else (properties ->> 'takenUntil')::timestamptz\n    end,\n    new_etag\n  )\n  where\n    tasks.task_id = partition_key and\n    tasks.etag = queue_tasks_entities_modify.old_etag;\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n  perform tasks.etag from tasks\n  where\n    tasks.task_id = partition_key;\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_tasks_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from tasks\n  where\n    tasks.task_id = partition_key\n  returning tasks.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_tasks_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  cond text[];\n  exp_cond_field text;\n  exp_cond_operator text;\n  exp_cond_operand timestamptz;\nbegin\n  if not condition is null then\n    cond := regexp_split_to_array(condition, '\\s+');\n    exp_cond_field := trim(cond[3], '''');\n    exp_cond_operator := cond[4];\n    exp_cond_operand := cond[5] :: timestamptz;\n\n    if not (exp_cond_field || exp_cond_operator) in ('takenUntil=', 'deadline=', 'expires<') then\n      raise exception 'queue_tasks_entities_scan only supports certain takenUntil, deadline, and expires conditions';\n    end if;\n  end if;\n\n  return query select\n    task_id as partition_key,\n    'task' as row_key,\n    entity_buf_encode(\n      entity_buf_encode(\n        entity_buf_encode(\n          entity_buf_encode(\n            entity_buf_encode(\n              entity_buf_encode(\n                entity_buf_encode(\n                  entity_buf_encode(\n                    jsonb_build_object(\n                      'PartitionKey', task_id,\n                      'RowKey', 'task',\n                      'taskId', slugid_to_uuid(task_id),\n                      'provisionerId', provisioner_id,\n                      'workerType', worker_type,\n                      'schedulerId', scheduler_id,\n                      'taskGroupId', slugid_to_uuid(task_group_id),\n                      'requires', requires::text,\n                      'priority', priority::text,\n                      'retries', retries,\n                      'retriesLeft', retries_left,\n                      'created', created,\n                      'deadline', deadline,\n                      'expires', expires,\n                      'takenUntil', coalesce(taken_until, '1970-01-01 00:00:00+00'::timestamptz)),\n                    'dependencies', dependencies::text),\n                  'routes', routes::text),\n                'scopes', scopes::text),\n              'payload', payload::text),\n            'metadata', metadata::text),\n          'tags', tags::text),\n        'extra', extra::text),\n      'runs', runs::text) as value,\n    1 as version,\n    tasks.etag as etag from tasks\n  where\n    (pk is NULL or pk = task_id) and\n    case\n      when exp_cond_field = 'deadline' then deadline = exp_cond_operand\n      -- note that queue never queries for takenUntil = new Date(0)\n      when exp_cond_field = 'takenUntil' then taken_until = exp_cond_operand\n      when exp_cond_field = 'expires' then expires < exp_cond_operand\n      else true\n    end\n  order by tasks.task_id\n  limit case\n    when (size is not null and size > 0) then size + 1\n    else null\n  end\n  offset case\n    when (page is not null and page > 0) then page\n    else 0\n  end;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        }
      },
      "migrationScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.  Note that this may lead to concurrent\n  -- updates failing; the important thing is that they not succeed without\n  -- taking effect.  Failed updates will be retried.\n  lock table queue_tasks_entities;\n\n  create type task_requires as enum('all-completed', 'all-resolved');\n  create type task_priority as enum(\n    'highest',\n    'very-high',\n    'high',\n    'medium',\n    'low',\n    'very-low',\n    'lowest',\n    'normal'\n  );\n\n  -- use a temporary fuction to convert taken_until.  The JS code uses `new\n  -- Date(0)` as a null value, so we convert that to NULL here\n  create function pg_temp.parse_taken_until(taken_until timestamptz) returns timestamptz\n  as $$\n      begin\n        return case taken_until\n          when '1970-01-01 00:00:00+00'::timestamptz then null\n          else taken_until\n        end;\n      end;\n  $$\n  language plpgSQL\n  strict immutable;\n\n  -- left join the tasks entities with the task_group_active_sets entities\n  -- which tracks whether a task has ever been resolved or not, and reflect\n  -- that into an `ever_resolved` boolean\n  raise log 'TIMING start tasks create table .. as select';\n  create table tasks\n  as\n    select\n      tasks_entities.task_id as task_id,\n      (value ->> 'provisionerId')::text as provisioner_id,\n      (value ->> 'workerType')::text as worker_type,\n      (value ->> 'schedulerId')::text as scheduler_id,\n      tasks_entities.task_group_id as task_group_id,\n      entity_buf_decode(value, 'dependencies')::jsonb as dependencies,\n      (value ->> 'requires')::task_requires as requires,\n      entity_buf_decode(value, 'routes')::jsonb as routes,\n      (value ->> 'priority')::task_priority as priority,\n      (value ->> 'retries')::int as retries,\n      (value ->> 'retriesLeft')::int as retries_left,\n      (value ->> 'created')::timestamptz as created,\n      (value ->> 'deadline')::timestamptz as deadline,\n      (value ->> 'expires')::timestamptz as expires,\n      entity_buf_decode(value, 'scopes')::jsonb as scopes,\n      entity_buf_decode(value, 'payload')::jsonb as payload,\n      entity_buf_decode(value, 'metadata')::jsonb as metadata,\n      entity_buf_decode(value, 'tags')::jsonb as tags,\n      entity_buf_decode(value, 'extra')::jsonb as extra,\n      entity_buf_decode(value, 'runs')::jsonb as runs,\n      case (value ->> 'takenUntil')::timestamptz\n        when '1970-01-01 00:00:00+00'::timestamptz then null\n        else (value ->> 'takenUntil')::timestamptz\n      end as taken_until,\n      (active_tasks.task_id is null)::boolean as ever_resolved,\n      etag\n    from (\n      select\n        *,\n        partition_key as task_id,\n        uuid_to_slugid(value ->> 'taskGroupId')::text as task_group_id\n      from queue_tasks_entities\n    ) as tasks_entities\n    left join (\n      select row_key as task_id\n      from queue_task_group_active_sets_entities\n    ) as active_tasks\n    on\n      tasks_entities.task_id = active_tasks.task_id;\n\n  raise log 'TIMING start tasks add primary key';\n  alter table tasks add primary key (task_id);\n  -- this index servces the purpose of queue_task_group_members_entities\n  raise log 'TIMING start tasks add tasks_task_group_id_idx';\n  create index tasks_task_group_id_idx on tasks (task_group_id);\n  -- this index servces the purpose of queue_task_group_active_sets_entities\n  raise log 'TIMING start tasks add tasks_task_group_id_unresolved_idx';\n  create index tasks_task_group_id_unresolved_idx on tasks (task_group_id)\n    where not ever_resolved;\n\n  raise log 'TIMING start tasks set not null';\n  alter table tasks\n    alter column task_id set not null,\n    alter column provisioner_id set not null,\n    alter column worker_type set not null,\n    alter column scheduler_id set not null,\n    alter column task_group_id set not null,\n    alter column dependencies set not null,\n    alter column requires set not null,\n    alter column routes set not null,\n    alter column priority set not null,\n    alter column retries set not null,\n    alter column retries_left set not null,\n    alter column created set not null,\n    alter column deadline set not null,\n    alter column expires set not null,\n    alter column scopes set not null,\n    alter column payload set not null,\n    alter column metadata set not null,\n    alter column tags set not null,\n    alter column extra set not null,\n    alter column runs set not null,\n    -- note that taken_until is omitted here as it is intended to be nullable\n    alter column ever_resolved set not null,\n    alter column etag set not null,\n    alter column etag set default public.gen_random_uuid();\n\n  raise log 'TIMING start tasks set perms';\n  revoke select, insert, update, delete on queue_tasks_entities from $db_user_prefix$_queue;\n  drop table queue_tasks_entities;\n\n  -- these tables are replaced by indexes, and no longer needed\n  revoke select, insert, update, delete on queue_task_group_members_entities from $db_user_prefix$_queue;\n  drop table queue_task_group_members_entities;\n  revoke select, insert, update, delete on queue_task_group_active_sets_entities from $db_user_prefix$_queue;\n  drop table queue_task_group_active_sets_entities;\n\n  grant select, insert, update, delete on tasks to $db_user_prefix$_queue;\nend\n",
      "version": 20
    },
    {
      "description": "queue task groups phase 2 step 1",
      "downgradeScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.\n  lock table task_groups;\n\n  raise log 'TIMING start queue_task_groups_entities create table';\n  create table queue_task_groups_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n\n  raise log 'TIMING start queue_task_groups_entities primary key';\n  alter table queue_task_groups_entities add primary key (partition_key, row_key);\n\n  raise log 'TIMING start queue_task_groups_entities insert';\n  insert into queue_task_groups_entities\n  select\n    task_group_id as partition_key,\n    'task-group' as row_key,\n    jsonb_build_object(\n      'PartitionKey', task_group_id,\n      'RowKey', 'task-group',\n      'taskGroupId', slugid_to_uuid(task_group_id),\n      'schedulerId', scheduler_id,\n      'expires', expires) as value,\n    1 as version,\n    task_groups.etag as etag\n  from task_groups;\n\n  raise log 'TIMING start queue_task_groups_entities permissions';\n  revoke select, insert, update, delete on task_groups from $db_user_prefix$_queue;\n  drop table task_groups;\n\n  grant select, insert, update, delete on queue_task_groups_entities to $db_user_prefix$_queue;\nend\n",
      "methods": {
        "queue_task_groups_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid;\nbegin\n  if overwrite then\n    raise exception 'overwrite not implemented';\n  end if;\n  new_etag = public.gen_random_uuid();\n  insert into task_groups select\n    uuid_to_slugid(properties ->> 'taskGroupId')::text as task_group_id,\n    (properties ->> 'schedulerId')::text as scheduler_id,\n    (properties ->> 'expires')::timestamptz as expires,\n    new_etag as etag;\n  return new_etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "queue_task_groups_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select\n    task_group_id,\n    'task-group',\n    jsonb_build_object(\n      'PartitionKey', task_group_id,\n      'RowKey', 'task-group',\n      'taskGroupId', slugid_to_uuid(task_group_id),\n      'schedulerId', scheduler_id,\n      'expires', expires) as value,\n    1 as version,\n    task_groups.etag as etag\n  from task_groups\n  where\n    task_groups.task_group_id = partition_key;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_groups_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid;\nbegin\n  -- NOTE: queue only updates expires, so that's all that's supported here\n  new_etag = public.gen_random_uuid();\n  update task_groups\n  set (\n    expires,\n    etag\n  ) = (\n    (properties ->> 'expires')::timestamptz,\n    new_etag\n  )\n  where\n    task_groups.task_group_id = partition_key and\n    task_groups.etag = queue_task_groups_entities_modify.old_etag;\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n  perform task_groups.etag from task_groups\n  where\n    task_groups.task_group_id = partition_key;\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_groups_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from task_groups\n  where\n    task_groups.task_group_id = partition_key\n  returning task_groups.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_groups_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  cond text[];\n  exp_cond_field text;\n  exp_cond_operator text;\n  exp_cond_operand timestamptz;\nbegin\n  if pk is not null then\n    raise exception 'scanning by primary key is not supported';\n  end if;\n\n  if not condition is null then\n    cond := regexp_split_to_array(condition, '\\s+');\n    exp_cond_field := trim(cond[3], '''');\n    exp_cond_operator := cond[4];\n    exp_cond_operand := cond[5] :: timestamptz;\n\n    if exp_cond_field || exp_cond_operator != 'expires<' then\n      raise exception 'queue_task_group_active_sets_entities_scan only supports filtering for expired rows';\n    end if;\n  end if;\n\n  return query select\n    task_group_id as partition_key,\n    'task-group' as row_key,\n    jsonb_build_object(\n      'PartitionKey', task_group_id,\n      'RowKey', 'task-group',\n      'taskGroupId', slugid_to_uuid(task_group_id),\n      'schedulerId', scheduler_id,\n      'expires', expires) as value,\n    1 as version,\n    task_groups.etag as etag from task_groups\n  where\n    case\n      when exp_cond_field = 'expires' then expires < exp_cond_operand\n      else true\n    end\n  order by task_groups.task_group_id\n  limit case\n    when (size is not null and size > 0) then size + 1\n    else null\n  end\n  offset case\n    when (page is not null and page > 0) then page\n    else 0\n  end;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        }
      },
      "migrationScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.  Note that this may lead to concurrent\n  -- updates failing; the important thing is that they not succeed without\n  -- taking effect.  Failed updates will be retried.\n  lock table queue_task_groups_entities;\n\n  raise log 'TIMING start task_groups create table .. as select';\n  create table task_groups\n  as\n    select\n      uuid_to_slugid(value ->> 'taskGroupId')::text as task_group_id,\n      (value ->> 'schedulerId')::text as scheduler_id,\n      (value ->> 'expires')::timestamptz as expires,\n      queue_task_groups_entities.etag as etag\n    from queue_task_groups_entities;\n  raise log 'TIMING start task_groups add primary key';\n  alter table task_groups add primary key (task_group_id);\n\n  raise log 'TIMING start task_groups set not null';\n  alter table task_groups\n    alter column task_group_id set not null,\n    alter column scheduler_id set not null,\n    alter column expires set not null,\n    alter column etag set not null;\n\n  raise log 'TIMING start task_groups set permissions';\n  revoke select, insert, update, delete on queue_task_groups_entities from $db_user_prefix$_queue;\n  drop table queue_task_groups_entities;\n\n  grant select, insert, update, delete on task_groups to $db_user_prefix$_queue;\nend\n",
      "version": 21
    },
    {
      "description": "queue task dependencies phase 2 step 1 (slow migration: 10μs per row in table `queue_task_dependency_entities`)",
      "downgradeScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.\n  lock table task_dependencies;\n\n  raise log 'TIMING start queue_task_dependency_entities create table';\n  create table queue_task_dependency_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n\n  raise log 'TIMING start queue_task_dependency_entities primary key';\n  alter table queue_task_dependency_entities add primary key (partition_key, row_key);\n\n  raise log 'TIMING start queue_task_requirement_entities create table';\n  create table queue_task_requirement_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n\n  raise log 'TIMING start queue_task_requirement_entities primary key';\n  alter table queue_task_requirement_entities add primary key (partition_key, row_key);\n\n  raise log 'TIMING start queue_task_dependency_entities insert';\n  insert into queue_task_dependency_entities\n  select\n    required_task_id,\n    dependent_task_id,\n    jsonb_build_object(\n      'PartitionKey', required_task_id,\n      'RowKey', dependent_task_id,\n      'taskId', slugid_to_uuid(required_task_id),\n      'dependentTaskId', slugid_to_uuid(dependent_task_id),\n      'require', replace(requires::text, 'all-', ''),\n      'expires', expires) as value,\n    1 as version,\n    task_dependencies.etag as etag\n  from task_dependencies;\n\n  raise log 'TIMING start queue_task_requirement_entities insert';\n  insert into queue_task_requirement_entities\n  select\n    dependent_task_id,\n    required_task_id,\n    jsonb_build_object(\n      'PartitionKey', dependent_task_id,\n      'RowKey', required_task_id,\n      'taskId', slugid_to_uuid(dependent_task_id),\n      'requiredTaskId', slugid_to_uuid(required_task_id),\n      'expires', expires) as value,\n    1 as version,\n    task_dependencies.etag as etag\n  from task_dependencies\n  where not satisfied;\n\n  raise log 'TIMING start queue_task{requirement,dependency}_entities permissions';\n  revoke select, insert, update, delete on task_dependencies from $db_user_prefix$_queue;\n  drop table task_dependencies;\n\n  grant select, insert, update, delete on queue_task_dependency_entities to $db_user_prefix$_queue;\n  grant select, insert, update, delete on queue_task_requirement_entities to $db_user_prefix$_queue;\nend\n",
      "methods": {
        "queue_task_dependency_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid;\nbegin\n  -- note that this function always overwrites (queue always calls it that way anyway)\n  new_etag = public.gen_random_uuid();\n  insert into task_dependencies select\n    rk,\n    pk,\n    ('all-' || (properties ->> 'require'))::task_requires,\n    true,\n    (properties ->> 'expires')::timestamptz,\n    new_etag\n  -- if the row already exists, that's because it was created by\n  -- queue_task_requirement_entities_create, so just update the requires value\n  on conflict (required_task_id, dependent_task_id) do\n    update\n    set requires = ('all-' || (properties ->> 'require'))::task_requires\n    where\n      task_dependencies.required_task_id = pk and\n      task_dependencies.dependent_task_id = rk;\n  return new_etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "queue_task_dependency_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select\n    required_task_id,\n    dependent_task_id,\n    jsonb_build_object(\n      'PartitionKey', required_task_id,\n      'RowKey', dependent_task_id,\n      'taskId', slugid_to_uuid(required_task_id),\n      'dependentTaskId', slugid_to_uuid(dependent_task_id),\n      'require', replace(requires::text, 'all-', ''),\n      'expires', expires) as value,\n    1 as version,\n    task_dependencies.etag as etag\n  from task_dependencies\n  where\n    required_task_id = partition_key and dependent_task_id = row_key;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_dependency_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "begin\n  raise exception 'not implemented';\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_dependency_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from task_dependencies\n  where\n    task_dependencies.required_task_id = partition_key and\n    task_dependencies.dependent_task_id = row_key\n  returning task_dependencies.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_dependency_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  cond text[];\n  exp_cond_field text;\n  exp_cond_operator text;\n  exp_cond_operand timestamptz;\nbegin\n  if not condition is null then\n    cond := regexp_split_to_array(condition, '\\s+');\n    exp_cond_field := trim(cond[3], '''');\n    exp_cond_operator := cond[4];\n    exp_cond_operand := cond[5] :: timestamptz;\n\n    if not exp_cond_field || exp_cond_operator in ('expires<', 'expires>=') then\n      raise exception 'scan only supports filtering for expired rows, not on %', exp_cond_field || exp_cond_operator;\n    end if;\n  end if;\n\n  return query select\n    required_task_id as partition_key,\n    dependent_task_id as row_key,\n    jsonb_build_object(\n      'PartitionKey', required_task_id,\n      'RowKey', dependent_task_id,\n      'taskId', slugid_to_uuid(required_task_id),\n      'dependentTaskId', slugid_to_uuid(dependent_task_id),\n      'require', replace(requires::text, 'all-', ''),\n      'expires', expires) as value,\n    1 as version,\n    task_dependencies.etag as etag from task_dependencies\n  where\n    (pk is null or required_task_id = pk) and\n    case\n      when exp_cond_field = 'expires' and exp_cond_operator = '<' then expires < exp_cond_operand\n      when exp_cond_field = 'expires' and exp_cond_operator = '>=' then expires >= exp_cond_operand\n      else true\n    end\n  order by task_dependencies.required_task_id, task_dependencies.dependent_task_id\n  limit case\n    when (size is not null and size > 0) then size + 1\n    else null\n  end\n  offset case\n    when (page is not null and page > 0) then page\n    else 0\n  end;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_requirement_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid;\nbegin\n  -- note that this function always overwrites (queue always calls it that way anyway)\n  new_etag = public.gen_random_uuid();\n  insert into task_dependencies select\n    pk,\n    rk,\n    -- arbitrary value; in practice TaskDependency.create is called shortly\n    -- after this and will set the value correctly\n    'all-completed',\n    -- if a TaskRequirement exists, then the dependency is not satisified\n    false,\n    (properties ->> 'expires')::timestamptz,\n    new_etag\n  -- if the dependency was created already, just update satisfied\n  on conflict (required_task_id, dependent_task_id) do\n    update\n    set satisfied = false\n    where\n      task_dependencies.required_task_id = rk and\n      task_dependencies.dependent_task_id = pk;\n  return new_etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "queue_task_requirement_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select\n    dependent_task_id,\n    required_task_id,\n    jsonb_build_object(\n      'PartitionKey', dependent_task_id,\n      'RowKey', required_task_id,\n      'taskId', slugid_to_uuid(dependent_task_id),\n      'requiredTaskId', slugid_to_uuid(required_task_id),\n      'expires', expires) as value,\n    1 as version,\n    task_dependencies.etag as etag\n  from task_dependencies\n  where\n    dependent_task_id = partition_key and required_task_id = row_key and not satisfied;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_requirement_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "begin\n  raise exception 'not implemented';\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_requirement_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  -- removing the requirement means that this dep is satisfied\n  return query\n  update task_dependencies\n  set satisfied = true\n  where\n    task_dependencies.dependent_task_id = partition_key and\n    task_dependencies.required_task_id = row_key and\n    not task_dependencies.satisfied\n  returning task_dependencies.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_task_requirement_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  cond text[];\n  exp_cond_field text;\n  exp_cond_operator text;\n  exp_cond_operand timestamptz;\nbegin\n  if not condition is null then\n    cond := regexp_split_to_array(condition, '\\s+');\n    exp_cond_field := trim(cond[3], '''');\n    exp_cond_operator := cond[4];\n    exp_cond_operand := cond[5] :: timestamptz;\n\n    if exp_cond_field || exp_cond_operator != 'expires<' then\n      raise exception 'scan only supports filtering for expired rows';\n    end if;\n\n    -- if this is the expiration crontask, return an empty set -- expiration of TaskDependency\n    -- is sufficient\n    return;\n  end if;\n\n  return query select\n    dependent_task_id as partition_key,\n    required_task_id as row_key,\n    jsonb_build_object(\n      'PartitionKey', dependent_task_id,\n      'RowKey', required_task_id,\n      'taskId', slugid_to_uuid(dependent_task_id),\n      'requiredTaskId', slugid_to_uuid(required_task_id),\n      'expires', expires) as value,\n    1 as version,\n    task_dependencies.etag as etag from task_dependencies\n  where\n    (pk is null or dependent_task_id = pk) and\n    not satisfied\n  order by task_dependencies.dependent_task_id, task_dependencies.required_task_id\n  limit case\n    when (size is not null and size > 0) then size + 1\n    else null\n  end\n  offset case\n    when (page is not null and page > 0) then page\n    else 0\n  end;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        }
      },
      "migrationScript": "begin\n  -- this migration combines queue_task_dependency_entries (which contains all task\n  -- dependencies) and queue_task_requirement_entities (which contains\n  -- as-yet-unsatisfied dependencies) into a single table with a `satisfied`\n  -- column.  If we consider a dependency to be from \"dependent\" to \"required\", then\n  -- task_dependency_entities has\n  --   partition_key = required_task_id\n  --   row_key = dependent_task_id\n  -- task_requirement_entities has\n  --   partition key = dependent_task_id\n  --   row_key = required_task_id\n  lock table queue_task_dependency_entities;\n  lock table queue_task_requirement_entities;\n\n  raise log 'TIMING start task_dependencies create table .. as select';\n  create table task_dependencies\n  as\n    select\n      deps.dependent_task_id,\n      deps.required_task_id,\n      deps.requires,\n      reqs.dependent_task_id is null as satisfied,\n      deps.expires,\n      deps.etag as etag\n    from (\n      select\n        partition_key as required_task_id,\n        row_key as dependent_task_id,\n        ('all-' || (value ->> 'require'))::task_requires as requires,\n        (value ->> 'expires')::timestamptz as expires,\n        etag\n      from queue_task_dependency_entities\n    ) as deps\n    left join (\n      select\n        partition_key as dependent_task_id,\n        row_key as required_task_id,\n        etag\n      from queue_task_requirement_entities\n    ) as reqs\n    on deps.dependent_task_id = reqs.dependent_task_id and\n      deps.required_task_id = reqs.required_task_id;\n\n  raise log 'TIMING start task_dependencies set not null';\n  alter table task_dependencies\n    alter column dependent_task_id set not null,\n    alter column required_task_id set not null,\n    alter column requires set not null,\n    alter column satisfied set not null,\n    alter column expires set not null,\n    alter column etag set not null,\n    alter column etag set default public.gen_random_uuid();\n\n  -- the task itself contains an array of dependent tasks, so querying by\n  -- dependent_task_id is unusual, so primary key begins with required_task_id.\n  raise log 'TIMING start task_dependencies add primary key';\n  alter table task_dependencies add primary key (required_task_id, dependent_task_id);\n\n  -- queue's isBlocked indexes by dependent_task_id, looking only for unsatisfied\n  -- entries.  This index then takes the place of the queue_task_requirement_entities\n  -- table and that these queries can be handled by accessing only the index.\n  raise log 'TIMING start task_dependencies add task_dependencies_dependent_task_id_idx';\n  create index task_dependencies_dependent_task_id_idx on task_dependencies (dependent_task_id)\n    where not satisfied;\n\n  raise log 'TIMING start task_dependencies set permissions';\n  revoke select, insert, update, delete on queue_task_dependency_entities from $db_user_prefix$_queue;\n  drop table queue_task_dependency_entities;\n  revoke select, insert, update, delete on queue_task_requirement_entities from $db_user_prefix$_queue;\n  drop table queue_task_requirement_entities;\n\n  grant select, insert, update, delete on task_dependencies to $db_user_prefix$_queue;\nend\n",
      "version": 22
    },
    {
      "description": "github builds phase 2",
      "downgradeScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.\n  lock table github_builds;\n\n  create table taskcluster_github_builds_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n  alter table taskcluster_github_builds_entities add primary key (partition_key, row_key);\n\n  insert into taskcluster_github_builds_entities\n  select\n    encode_string_key(task_group_id) as partition_key,\n    'taskGroupId' as row_key,\n    jsonb_build_object(\n      'PartitionKey', encode_string_key(task_group_id),\n      'RowKey', 'taskGroupId',\n      'organization', organization,\n      'repository', repository,\n      'sha', sha,\n      'taskGroupId', task_group_id,\n      'state', state,\n      'created', created,\n      'updated', updated,\n      'installationId', installation_id,\n      'eventType', event_type,\n      'eventId', event_id) as value,\n    1 as version,\n    etag\n  from github_builds;\n\n  revoke select, insert, update, delete on github_builds from $db_user_prefix$_github;\n  drop table github_builds;\n  grant select, insert, update, delete on taskcluster_github_builds_entities to $db_user_prefix$_github;\nend\n",
      "methods": {
        "create_github_build": {
          "args": "organization_in text, repository_in text, sha_in text, task_group_id_in text, state_in text, created_in timestamptz, updated_in timestamptz, installation_id_in integer, event_type_in text, event_id_in text",
          "body": "begin\n  insert\n    into github_builds (organization, repository, sha, task_group_id, state, created, updated, installation_id, event_type, event_id)\n    values (organization_in, repository_in, sha_in, task_group_id_in, state_in, created_in, updated_in, installation_id_in, event_type_in, event_id_in);\nend",
          "deprecated": false,
          "description": "Create a new github build.  Raises UNIQUE_VIOLATION if the pool already exists.",
          "mode": "write",
          "returns": "void",
          "serviceName": "github"
        },
        "delete_github_build": {
          "args": "task_group_id_in text",
          "body": "begin\n  delete\n  from github_builds\n  where github_builds.task_group_id = task_group_id_in;\nend",
          "deprecated": false,
          "description": "Delete a github build.",
          "mode": "write",
          "returns": "void",
          "serviceName": "github"
        },
        "get_github_build": {
          "args": "task_group_id_in text",
          "body": "begin\n  return query\n  select\n    github_builds.organization,\n    github_builds.repository,\n    github_builds.sha,\n    github_builds.task_group_id,\n    github_builds.state,\n    github_builds.created,\n    github_builds.updated,\n    github_builds.installation_id,\n    github_builds.event_type,\n    github_builds.event_id,\n    github_builds.etag\n  from github_builds\n  where github_builds.task_group_id = task_group_id_in;\nend",
          "deprecated": false,
          "description": "Get a github build. The returned table will have one or zero rows.",
          "mode": "read",
          "returns": "table (organization text, repository text, sha text, task_group_id text, state text, created timestamptz, updated timestamptz, installation_id integer, event_type text, event_id text, etag uuid)",
          "serviceName": "github"
        },
        "get_github_builds": {
          "args": "page_size_in integer, page_offset_in integer, organization_in text, repository_in text, sha_in text",
          "body": "begin\n  return query\n  select\n    github_builds.organization,\n    github_builds.repository,\n    github_builds.sha,\n    github_builds.task_group_id,\n    github_builds.state,\n    github_builds.created,\n    github_builds.updated,\n    github_builds.installation_id,\n    github_builds.event_type,\n    github_builds.event_id,\n    github_builds.etag\n  from github_builds\n  where\n    (organization_in is null or github_builds.organization = organization_in) and\n    (repository_in is null or github_builds.repository = repository_in) and\n    (sha_in is null or github_builds.sha = sha_in)\n  order by github_builds.updated asc\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get github builds.",
          "mode": "read",
          "returns": "table (organization text, repository text, sha text, task_group_id text, state text, created timestamptz, updated timestamptz, installation_id integer, event_type text, event_id text, etag uuid)",
          "serviceName": "github"
        },
        "set_github_build_state": {
          "args": "task_group_id_in text, state_in text",
          "body": "begin\n  update github_builds\n  set (state, updated, etag) = (\n    state_in,\n    now(),\n    public.gen_random_uuid()\n  ) where github_builds.task_group_id = task_group_id_in;\n  if not found then\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "Only update the state of a build and update the `updated` timestamp",
          "mode": "write",
          "returns": "void",
          "serviceName": "github"
        },
        "taskcluster_github_builds_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_row github_builds%ROWTYPE;\nbegin\n  select\n    (properties ->> 'organization')::text as organization,\n    (properties ->> 'repository')::text as repository,\n    (properties ->> 'sha')::text as sha,\n    (properties ->> 'taskGroupId')::text as task_group_id,\n    (properties ->> 'state')::text as state,\n    (properties ->> 'created')::timestamptz as created,\n    (properties ->> 'updated')::timestamptz as updated,\n    (properties ->> 'installationId')::integer as installation_id,\n    (properties ->> 'eventType')::text as event_type,\n    (properties ->> 'eventId')::text as event_id,\n    public.gen_random_uuid() as etag\n  into new_row;\n  if overwrite then\n    raise exception 'overwrite not implemented';\n  else\n    execute 'insert into github_builds select $1.*' using new_row;\n  end if;\n  return new_row.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "github"
        },
        "taskcluster_github_builds_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n return query\n select\n    taskcluster_github_builds_entities_load.partition_key,\n    'taskGroupId' as row_key,\n    jsonb_build_object(\n      'PartitionKey', taskcluster_github_builds_entities_load.partition_key,\n      'RowKey', 'taskGroupId',\n      'organization', organization,\n      'repository', repository,\n      'sha', sha,\n      'taskGroupId', task_group_id,\n      'state', state,\n      'created', created,\n      'updated', updated,\n      'installationId', installation_id,\n      'eventType', event_type,\n      'eventId', event_id) as value,\n    1 as version,\n    github_builds.etag as etag\nfrom github_builds\nwhere\n  github_builds.task_group_id = decode_string_key(taskcluster_github_builds_entities_load.partition_key);\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_github_builds_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_row github_builds%ROWTYPE;\nbegin\n  select\n    (properties ->> 'organization')::text as organization,\n    (properties ->> 'repository')::text as repository,\n    (properties ->> 'sha')::text as sha,\n    (properties ->> 'taskGroupId')::text as task_group_id,\n    (properties ->> 'state')::text as state,\n    (properties ->> 'created')::timestamptz as created,\n    (properties ->> 'updated')::timestamptz as updated,\n    (properties ->> 'installationId')::integer as installation_id,\n    (properties ->> 'eventType')::text as event_type,\n    (properties ->> 'eventId')::text as event_id,\n    public.gen_random_uuid() as etag\n  into new_row;\n  update github_builds\n  set (\n    organization,\n    repository,\n    sha,\n    task_group_id,\n    state,\n    created,\n    updated,\n    installation_id,\n    event_type,\n    event_id,\n    etag\n  ) = (\n    new_row.organization,\n    new_row.repository,\n    new_row.sha,\n    new_row.task_group_id,\n    new_row.state,\n    new_row.created,\n    new_row.updated,\n    new_row.installation_id,\n    new_row.event_type,\n    new_row.event_id,\n    new_row.etag\n  )\n  where\n    github_builds.task_group_id = decode_string_key(taskcluster_github_builds_entities_modify.partition_key) and\n    github_builds.etag = taskcluster_github_builds_entities_modify.old_etag;\n\n  if found then\n    return query select new_row.etag;\n    return;\n  end if;\n\n  perform github_builds.etag from github_builds\n  where github_builds.task_group_id = decode_string_key(taskcluster_github_builds_entities_modify.partition_key);\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_github_builds_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  delete\n  from github_builds\n  where\n    github_builds.task_group_id = decode_string_key(partition_key);\n  -- tc-gh does not care if the row existed\n  return query select gen_random_uuid() as etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_github_builds_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\nbegin\n  if not condition is null then\n    raise exception 'condition not supported';\n  end if;\n  return query\n    select\n      encode_string_key(task_group_id),\n      'taskGroupId' as row_key,\n      jsonb_build_object(\n        'PartitionKey', encode_string_key(task_group_id),\n        'RowKey', 'taskGroupId',\n        'organization', organization,\n        'repository', repository,\n        'sha', sha,\n        'taskGroupId', task_group_id,\n        'state', state,\n        'created', created,\n        'updated', updated,\n        'installationId', installation_id,\n        'eventType', event_type,\n        'eventId', event_id) as value,\n        1 as version,\n    github_builds.etag as etag\n    from github_builds\n    where\n      partition_key is null or\n      task_group_id = decode_string_key(partition_key)\n    order by task_group_id\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (page is not null and page > 0) then page\n      else 0\n    end;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "github"
        }
      },
      "migrationScript": "begin\n\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.  Note that this may lead to concurrent\n  -- updates failing; the important thing is that they not succeed without\n  -- taking effect.  Failed updates will be retried.\n  lock table taskcluster_github_builds_entities;\n\n  create table github_builds\n  as\n    select\n      (value ->> 'organization')::text as organization,\n      (value ->> 'repository')::text as repository,\n      (value ->> 'sha')::text as sha,\n      (value ->> 'taskGroupId')::text as task_group_id,\n      (value ->> 'state')::text as state,\n      (value ->> 'created')::timestamptz as created,\n      (value ->> 'updated')::timestamptz as updated,\n      (value ->> 'installationId')::integer as installation_id,\n      (value ->> 'eventType')::text as event_type,\n      (value ->> 'eventId')::text as event_id,\n      etag\n    from taskcluster_github_builds_entities;\n  alter table github_builds add primary key (task_group_id);\n  alter table github_builds\n    alter column organization set not null,\n    alter column repository set not null,\n    alter column sha set not null,\n    alter column task_group_id set not null,\n    alter column state set not null,\n    alter column created set not null,\n    alter column updated set not null,\n    alter column installation_id set not null,\n    alter column event_type set not null,\n    alter column event_id set not null,\n    alter column etag set not null,\n    alter column etag set default public.gen_random_uuid();\n  create index on github_builds (organization, repository, sha);\n\n  revoke select, insert, update, delete on taskcluster_github_builds_entities from $db_user_prefix$_github;\n  drop table taskcluster_github_builds_entities;\n  grant select, insert, update, delete on github_builds to $db_user_prefix$_github;\nend\n",
      "version": 23
    },
    {
      "description": "queue artifacts phase 2 step 2",
      "methods": {
        "create_queue_artifact": {
          "args": "task_id_in text, run_id_in integer, name_in text, storage_type_in text, content_type_in text, details_in jsonb, present_in boolean, expires_in timestamptz",
          "body": "begin\n  return query insert\n    into queue_artifacts (task_id, run_id, name, storage_type, content_type, details, present, expires)\n    values (task_id_in, run_id_in, name_in, storage_type_in, content_type_in, details_in, present_in, expires_in)\n  returning queue_artifacts.task_id, queue_artifacts.run_id, queue_artifacts.name, queue_artifacts.storage_type, queue_artifacts.content_type, queue_artifacts.details, queue_artifacts.present, queue_artifacts.expires;\nend",
          "deprecated": false,
          "description": "Create a new artifact. Raises UNIQUE_VIOLATION if the artifact already exists.\nReturns the newly created artifact.",
          "mode": "write",
          "returns": "table(task_id text, run_id integer, name text, storage_type text, content_type text, details jsonb, present boolean, expires timestamptz)",
          "serviceName": "queue"
        },
        "delete_queue_artifact": {
          "args": "task_id_in text, run_id_in integer, name_in text",
          "body": "begin\n  delete from queue_artifacts\n  where\n    queue_artifacts.task_id = task_id_in and\n    queue_artifacts.run_id = run_id_in and\n    queue_artifacts.name = name_in;\nend",
          "deprecated": false,
          "description": "Delete a queue artifact.",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "get_queue_artifact": {
          "args": "task_id_in text, run_id_in integer, name_in text",
          "body": "begin\n  return query select\n    queue_artifacts.task_id,\n    queue_artifacts.run_id,\n    queue_artifacts.name,\n    queue_artifacts.storage_type,\n    queue_artifacts.content_type,\n    queue_artifacts.details,\n    queue_artifacts.present,\n    queue_artifacts.expires\n  from queue_artifacts\n  where\n    queue_artifacts.task_id = task_id_in and\n    queue_artifacts.run_id = run_id_in and\n    queue_artifacts.name = name_in;\nend",
          "deprecated": false,
          "description": "Get a queue artifact. The returned table will have one or zero row.",
          "mode": "read",
          "returns": "table(task_id text, run_id integer, name text, storage_type text, content_type text, details jsonb, present boolean, expires timestamptz)",
          "serviceName": "queue"
        },
        "get_queue_artifacts": {
          "args": "task_id_in text, run_id_in integer, expires_in timestamptz, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    queue_artifacts.task_id,\n    queue_artifacts.run_id,\n    queue_artifacts.name,\n    queue_artifacts.storage_type,\n    queue_artifacts.content_type,\n    queue_artifacts.details,\n    queue_artifacts.present,\n    queue_artifacts.expires\n  from queue_artifacts\n  where\n    (queue_artifacts.task_id = task_id_in or task_id_in is null) and\n    (queue_artifacts.run_id = run_id_in or run_id_in is null) and\n    (queue_artifacts.expires < expires_in or expires_in is null)\n  order by queue_artifacts.task_id, queue_artifacts.run_id, queue_artifacts.name\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get existing queue artifacts filtered by the optional arguments,\nordered by the `task_id`, `run_id`, and `name`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(task_id text, run_id integer, name text, storage_type text, content_type text, details jsonb, present boolean, expires timestamptz)",
          "serviceName": "queue"
        },
        "update_queue_artifact": {
          "args": "task_id_in text, run_id_in integer, name_in text, details_in jsonb, expires_in timestamptz",
          "body": "declare\n  updated_row queue_artifacts%ROWTYPE;\nbegin\n  update queue_artifacts\n  set (details, expires) = (\n    coalesce(details_in, queue_artifacts.details),\n    coalesce(expires_in, queue_artifacts.expires)\n  )\n  where\n    queue_artifacts.task_id = task_id_in and\n    queue_artifacts.run_id = run_id_in and\n    queue_artifacts.name = name_in\n  returning\n    queue_artifacts.task_id,\n    queue_artifacts.run_id,\n    queue_artifacts.name,\n    queue_artifacts.storage_type,\n    queue_artifacts.content_type,\n    queue_artifacts.details,\n    queue_artifacts.present,\n    queue_artifacts.expires\n  into updated_row;\n  if found then\n    return query select\n      updated_row.task_id,\n      updated_row.run_id,\n      updated_row.name,\n      updated_row.storage_type,\n      updated_row.content_type,\n      updated_row.details,\n      updated_row.present,\n      updated_row.expires\n    return;\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "Update a queue artifact.\nReturns the up-to-date artifact row that have the same task id, run id, and name.",
          "mode": "write",
          "returns": "table(task_id text, run_id integer, name text, storage_type text, content_type text, details jsonb, present boolean, expires timestamptz)",
          "serviceName": "queue"
        }
      },
      "version": 24
    },
    {
      "description": "auth roles phase 2",
      "downgradeScript": "begin\n  lock table roles;\n\n  raise log 'TIMING start roles_entities create table';\n  create table roles_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n\n  raise log 'TIMING start roles_entities primary key';\n  alter table roles_entities add primary key (partition_key, row_key);\n\n  raise log 'TIMING start roles_entities insert';\n  perform 1 from roles;\n  if found then\n    insert into roles_entities\n    select\n      'role' as partition_key,\n      'role' as row_key,\n      entity_buf_encode(\n        jsonb_build_object(\n          'PartitionKey', 'roles',\n          'RowKey', 'roles'),\n        'blob', jsonb_agg(\n          jsonb_build_object(\n            'roleId', role_id,\n            'scopes', scopes,\n            'created', to_js_iso8601(created::text),\n            'description', description,\n            'lastModified', to_js_iso8601(last_modified::text))\n        )::text) as value,\n      1 as version,\n      -- use an aggregate function to select the etag (all rows have the same etag)\n      min(etag::text)::uuid as etag\n    from roles;\n  end if;\n\n  raise log 'TIMING start roles_entities permissions';\n  revoke select, insert, update, delete on roles from $db_user_prefix$_auth;\n  drop table roles;\n\n  grant select, insert, update, delete on roles_entities to $db_user_prefix$_auth;\n\n  drop function to_js_iso8601(ts_in text);\nend\n",
      "methods": {
        "get_roles": {
          "args": "",
          "body": "begin\n  return query\n  select\n    roles.role_id,\n    roles.scopes,\n    roles.created,\n    roles.description,\n    roles.last_modified,\n    roles.etag\n  from roles\n  order by role_id;\nend",
          "deprecated": false,
          "description": "Get the full set of roles.  Each result row has an etag, but all such\netags will be the same, representing the etag for the most recent\nmodification of the table.  Results are sorted by role_id.",
          "mode": "read",
          "returns": "table (role_id text, scopes jsonb, created timestamptz, description text, last_modified timestamptz, etag uuid)",
          "serviceName": "auth"
        },
        "modify_roles": {
          "args": "roles_in jsonb, old_etag_in uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  -- lock the table, avoiding risk of conflicts when inserting after\n  -- finding no rows\n  lock table roles;\n\n  delete from roles where etag = old_etag_in;\n  if not found then\n    -- delete may have done nothing because the table is empty (which is\n    -- ok) or because the etag did not match (which is an unsuccessful\n    -- update)\n    perform role_id from roles limit 1;\n    if found then\n      raise exception 'unsuccessful update' using errcode = 'P0004';\n    end if;\n  end if;\n\n  insert into roles\n  select\n    (role ->> 'role_id') as role_id,\n    (role ->> 'scopes')::jsonb as scopes,\n    (role ->> 'created')::timestamptz as created,\n    (role ->> 'description') as description,\n    (role ->> 'last_modified')::timestamptz as last_modified,\n    new_etag as etag\n  from jsonb_array_elements(roles_in) as role;\nend",
          "deprecated": false,
          "description": "Replace the current set of roles entirely with the given set of roles, if the current etag matches the existing etag.\nThe role objects are specified with underscore spelling (`role_id`).\nIf the etag has changed, this returns P0004 signalling that the caller should fetch a fresh set of roles and try again.\nIf there are no existing roles, then the old etag is not used.",
          "mode": "write",
          "returns": "void",
          "serviceName": "auth"
        },
        "roles_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid = gen_random_uuid();\nbegin\n  -- lock the table, avoiding risk of conflicts when inserting after\n  -- finding no rows\n  lock table roles;\n\n  perform 1 from roles limit 1;\n  if found then\n    raise exception 'roles already exist' using errcode = '23505'; -- unique violation\n  end if;\n\n  insert into roles\n  select\n    (role ->> 'roleId') as role_id,\n    (role ->> 'scopes')::jsonb as scopes,\n    (role ->> 'created')::timestamptz as created,\n    (role ->> 'description') as description,\n    (role ->> 'lastModified')::timestamptz as last_modified,\n    new_etag as etag\n  from jsonb_array_elements(entity_buf_decode(properties, 'blob')::jsonb) as role;\n\n  return new_etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "auth"
        },
        "roles_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  -- if no roles, return canned \"empty\" value, as the below expression will return NULL\n  perform 1 from roles limit 1;\n  if not found then\n    return query\n    select\n      'role',\n      'role',\n      entity_buf_encode(\n        jsonb_build_object(\n          'PartitionKey', 'role',\n          'RowKey', 'role'),\n        'blob', '[]'),\n      1,\n      gen_random_uuid();\n  end if;\n\n  return query\n  select\n    'role',\n    'role',\n    entity_buf_encode(\n      jsonb_build_object(\n        'PartitionKey', 'role',\n        'RowKey', 'role'),\n      'blob', jsonb_agg(\n        jsonb_build_object(\n          'roleId', role_id,\n          'scopes', scopes,\n          'created', to_js_iso8601(created::text),\n          'description', description,\n          'lastModified', to_js_iso8601(last_modified::text))\n      )::text),\n    1,\n    -- use an aggregate function to select the etag (all rows have the same etag)\n    min(roles.etag::text)::uuid\n  from roles;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "auth"
        },
        "roles_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  -- lock the table, avoiding risk of conflicts when inserting after\n  -- finding no rows\n  lock table roles;\n\n  delete from roles where roles.etag = old_etag;\n  if not found then\n    -- delete may have done nothing because the table is empty (which is\n    -- ok) or because the etag did not match (which is an unsuccessful\n    -- update)\n    perform role_id from roles limit 1;\n    if found then\n      raise exception 'unsuccessful update' using errcode = 'P0004';\n    end if;\n    -- ..otherwise continue to make the modification\n  end if;\n\n  insert into roles\n  select\n    (role ->> 'roleId') as role_id,\n    (role ->> 'scopes')::jsonb as scopes,\n    (role ->> 'created')::timestamptz as created,\n    (role ->> 'description') as description,\n    (role ->> 'lastModified')::timestamptz as last_modified,\n    new_etag as etag\n  from jsonb_array_elements(entity_buf_decode(properties, 'blob')::jsonb) as role;\n\n  return query select new_etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "auth"
        },
        "roles_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  raise exception 'not implemented';\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "auth"
        },
        "roles_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "begin\n  raise exception 'not implemented';\nend;",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "auth"
        }
      },
      "migrationScript": "begin\n  -- convert the postgres string form of a timestamp (which is valid iso8601) into the\n  -- precise format returned by JS's Date.toJSON\n  create or replace function to_js_iso8601(ts_in text) RETURNS text\n  as $$\n    begin\n      return regexp_replace(ts_in, '(.*) (.*)\\+00(:00)?', '\\1T\\2Z');\n    end;\n  $$\n  language plpgSQL\n  strict immutable;\n\n  lock table roles_entities;\n\n  -- Note that roles must be updated as a block, as there are some inter-role\n  -- consistency checks that must be followed and are too complex to describe in\n  -- SQL, but are expressed in JS in the Auth service.\n\n  raise log 'TIMING start roles create table .. as select';\n  create table roles\n  as\n    select\n      (expanded.role ->> 'roleId') as role_id,\n      (expanded.role ->> 'scopes')::jsonb as scopes,\n      (expanded.role ->> 'created')::timestamptz as created,\n      (expanded.role ->> 'description') as description,\n      (expanded.role ->> 'lastModified')::timestamptz as last_modified,\n      expanded.etag as etag\n    from (\n      select\n        jsonb_array_elements(\n          entity_buf_decode(value, 'blob')::jsonb\n        ) as role,\n        etag\n      from roles_entities\n    ) as expanded;\n\n  raise log 'TIMING start roles add primary key';\n  alter table roles add primary key (role_id);\n\n  raise log 'TIMING start roles set not null';\n  alter table roles\n    alter column role_id set not null,\n    alter column scopes set not null,\n    alter column created set not null,\n    alter column description set not null,\n    alter column last_modified set not null,\n    alter column etag set not null;\n\n\n  raise log 'TIMING start roles set permissions';\n  revoke select, insert, update, delete on roles_entities from $db_user_prefix$_auth;\n  drop table roles_entities;\n\n  grant select, insert, update, delete on roles to $db_user_prefix$_auth;\nend\n",
      "version": 25
    },
    {
      "description": "index phase 2 step 2",
      "methods": {
        "create_index_namespace": {
          "args": "parent_in text, name_in text, expires_in timestamptz",
          "body": "begin\n  return query insert\n    into index_namespaces (parent, name, expires)\n    values (parent_in, name_in, expires_in)\n    returning index_namespaces.parent, index_namespaces.name, index_namespaces.expires;\nend",
          "deprecated": false,
          "description": "Create a new namespace. Raises UNIQUE_VIOLATION if the namespace already exists.\nReturns the newly created namespace.",
          "mode": "write",
          "returns": "table(parent text, name text, expires timestamptz)",
          "serviceName": "index"
        },
        "create_indexed_task": {
          "args": "namespace_in text, name_in text, rank_in integer, task_id_in text, data_in jsonb, expires_in timestamptz",
          "body": "begin\n  return query insert\n    into indexed_tasks (namespace, name, rank, task_id, data, expires)\n    values (namespace_in, name_in, rank_in, slugid_to_uuid(task_id_in), data_in, expires_in)\n    returning indexed_tasks.namespace, indexed_tasks.name, indexed_tasks.rank, indexed_tasks.task_id, indexed_tasks.data, indexed_tasks.expires;\nend",
          "deprecated": false,
          "description": "Create a new indexed task. Raises UNIQUE_VIOLATION if the indexed task already exists.\nReturns the newly created indexed task.",
          "mode": "write",
          "returns": "table(namespace text, name text, rank integer, task_id text, data jsonb, expires timestamptz)",
          "serviceName": "index"
        },
        "expire_index_namespaces": {
          "args": "",
          "body": "declare\n  count integer;\nbegin\n  delete from index_namespaces where index_namespaces.expires < now();\n  if found then\n    get diagnostics count = row_count;\n    return count;\n  end if;\n  return 0;\nend",
          "deprecated": false,
          "description": "Expire index_namespaces that come before the current time.\nReturns a count of rows that have been deleted.",
          "mode": "write",
          "returns": "integer",
          "serviceName": "index"
        },
        "expire_indexed_tasks": {
          "args": "",
          "body": "declare\n  count integer;\nbegin\n  delete from indexed_tasks where indexed_tasks.expires < now();\n  if found then\n    get diagnostics count = row_count;\n    return count;\n  end if;\n  return 0;\nend",
          "deprecated": false,
          "description": "Expire indexed tasks that come before the current time.\nReturns a count of rows that have been deleted.",
          "mode": "write",
          "returns": "integer",
          "serviceName": "index"
        },
        "get_index_namespace": {
          "args": "parent_in text, name_in text",
          "body": "begin\n  return query select\n    index_namespaces.parent,\n    index_namespaces.name,\n    index_namespaces.expires\n  from index_namespaces\n  where\n    index_namespaces.parent = parent_in and\n    index_namespaces.name = name_in;\nend",
          "deprecated": false,
          "description": "Get a namespace. The returned table will have one or zero rows.",
          "mode": "read",
          "returns": "table(parent text, name text, expires timestamptz)",
          "serviceName": "index"
        },
        "get_index_namespaces": {
          "args": "parent_in text, name_in text, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    index_namespaces.parent,\n    index_namespaces.name,\n    index_namespaces.expires\n  from index_namespaces\n  where\n    (index_namespaces.parent = parent_in or parent_in is null) and\n    (index_namespaces.name = name_in or name_in is null) and\n    (index_namespaces.expires > now())\n  -- we previously used to order by the hashed parent but there's probably no need to add this complication.\n  order by index_namespaces.parent, name\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get existing index_namespaces filtered by the optional arguments,\nordered by the `parent` and `name`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(parent text, name text, expires timestamptz)",
          "serviceName": "index"
        },
        "get_indexed_task": {
          "args": "namespace_in text, name_in text",
          "body": "begin\n  return query select\n    indexed_tasks.namespace,\n    indexed_tasks.name,\n    indexed_tasks.rank,\n    uuid_to_slugid(indexed_tasks.task_id) as task_id,\n    indexed_tasks.data,\n    indexed_tasks.expires\n  from indexed_tasks\n  where\n    indexed_tasks.namespace = namespace_in and\n    indexed_tasks.name = name_in;\nend",
          "deprecated": false,
          "description": "Get an indexed task. The returned table will have one or zero rows.",
          "mode": "read",
          "returns": "table(namespace text, name text, rank integer, task_id text, data jsonb, expires timestamptz)",
          "serviceName": "index"
        },
        "get_indexed_tasks": {
          "args": "namespace_in text, name_in text, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    indexed_tasks.namespace,\n    indexed_tasks.name,\n    indexed_tasks.rank,\n    uuid_to_slugid(indexed_tasks.task_id) as task_id,\n    indexed_tasks.data,\n    indexed_tasks.expires\n  from indexed_tasks\n  where\n    (indexed_tasks.namespace = namespace_in or namespace_in is null) and\n    (indexed_tasks.name = name_in or name_in is null) and\n    (indexed_tasks.expires > now())\n  -- we previously used to order by the hashed namespace but there's probably no need to add this complication.\n  order by indexed_tasks.namespace, name\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get existing indexed tasks filtered by the optional arguments,\nordered by the `namespace` and `name`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(namespace text, name text, rank integer, task_id text, data jsonb, expires timestamptz)",
          "serviceName": "index"
        },
        "update_index_namespace": {
          "args": "parent_in text, name_in text, expires_in timestamptz",
          "body": "declare\n  updated_row index_namespaces%ROWTYPE;\nbegin\n  update index_namespaces\n  set expires = coalesce(expires_in, index_namespaces.expires)\n  where\n    index_namespaces.parent = parent_in and\n    index_namespaces.name = name_in\n  returning\n    index_namespaces.parent,\n    index_namespaces.name,\n    index_namespaces.expires\n  into updated_row;\n  if found then\n    return query select\n      updated_row.parent,\n      updated_row.name,\n      updated_row.expires;\n    return;\n  end if;\n  raise exception 'no such row' using errcode = 'P0002';\nend",
          "deprecated": false,
          "description": "Update a namespace.\nReturns the up-to-date namespace row that have the same parent and name.\nIf the row is not found then an exception with code 'P0002' is thrown.",
          "mode": "write",
          "returns": "table(parent text, name text, expires timestamptz)",
          "serviceName": "index"
        },
        "update_indexed_task": {
          "args": "namespace_in text, name_in text, rank_in integer, task_id_in text, data_in jsonb, expires_in timestamptz",
          "body": "declare\n  updated_row indexed_tasks%ROWTYPE;\nbegin\n  update indexed_tasks\n  set (rank, task_id, data, expires) = (\n    coalesce(rank_in, indexed_tasks.rank),\n    coalesce(slugid_to_uuid(task_id_in), indexed_tasks.task_id),\n    coalesce(data_in, indexed_tasks.data),\n    coalesce(expires_in, indexed_tasks.expires)\n  )\n  where\n    indexed_tasks.namespace = namespace_in and\n    indexed_tasks.name = name_in\n  returning\n    indexed_tasks.namespace,\n    indexed_tasks.name,\n    indexed_tasks.rank,\n    uuid_to_slugid(indexed_tasks.task_id),\n    indexed_tasks.data,\n    indexed_tasks.expires\n  into updated_row;\n  if found then\n    return query select\n      updated_row.namespace,\n      updated_row.name,\n      updated_row.rank,\n      updated_row.task_id,\n      updated_row.data,\n      updated_row.expires;\n    return;\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "Update an indexed task.\nReturns the up-to-date indexed task row that have the same namespace and name.",
          "mode": "write",
          "returns": "table(namespace text, name text, rank integer, task_id text, data jsonb, expires timestamptz)",
          "serviceName": "index"
        }
      },
      "version": 26
    },
    {
      "description": "web-server github access tokens phase 2 step 1+2",
      "downgradeScript": "begin\n\n  create table github_access_token_table_entities(\n    partition_key text,\n    row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n  alter table github_access_token_table_entities add primary key (partition_key, row_key);\n  grant select, insert, update, delete on github_access_token_table_entities to $db_user_prefix$_web_server;\n\n  revoke select, insert, update, delete on github_access_tokens from $db_user_prefix$_web_server;\n  drop table github_access_tokens;\n\nend\n",
      "methods": {
        "add_github_access_token": {
          "args": "user_id_in text, encrypted_access_token_in jsonb",
          "body": "begin\n  insert into github_access_tokens(user_id, encrypted_access_token)\n  values (\n    user_id_in,\n    encrypted_access_token_in\n  ) on conflict (user_id) do\n  update\n  set encrypted_access_token = encrypted_access_token_in\n  where github_access_tokens.user_id = add_github_access_token.user_id_in;\nend",
          "deprecated": false,
          "description": "Sets the encrypted access token for `user_id_in` to\n`encrypted_access_token_in`.\n\nIf no access token is currently set for `user_id_in`, a new row is\ninserted, otherwise the existing row's encrypted access token is updated\nto `encrypted_access_token_in`.",
          "mode": "write",
          "returns": "void",
          "serviceName": "web_server"
        },
        "github_access_token_table_entities_create": {
          "deprecated": true
        },
        "github_access_token_table_entities_load": {
          "deprecated": true
        },
        "github_access_token_table_entities_modify": {
          "deprecated": true
        },
        "github_access_token_table_entities_remove": {
          "deprecated": true
        },
        "github_access_token_table_entities_scan": {
          "deprecated": true
        },
        "load_github_access_token": {
          "args": "user_id_in text",
          "body": "begin\n  return query\n  select github_access_tokens.encrypted_access_token from github_access_tokens\n  where github_access_tokens.user_id = user_id_in;\nend",
          "deprecated": false,
          "description": "Returns the encrypted github access token for a given user.",
          "mode": "read",
          "returns": "table(encrypted_access_token jsonb)",
          "serviceName": "web_server"
        }
      },
      "migrationScript": "begin\n\n  create table github_access_tokens(\n    user_id text not null,\n    encrypted_access_token jsonb not null);\n  alter table github_access_tokens add primary key (user_id);\n  grant select, insert, update, delete on github_access_tokens to $db_user_prefix$_web_server;\n\n  revoke select, insert, update, delete on github_access_token_table_entities from $db_user_prefix$_web_server;\n  drop table github_access_token_table_entities;\n\nend\n",
      "version": 27
    },
    {
      "description": "queue tasks, task groups, dependencies phase 2 step 2",
      "methods": {
        "add_task_dependency": {
          "args": "dependent_task_id_in text, required_task_id_in text, requires_in task_requires, expires_in timestamptz",
          "body": "begin\n  insert\n  into task_dependencies (dependent_task_id, required_task_id, requires, satisfied, expires, etag)\n  values (\n    dependent_task_id_in,\n    required_task_id_in,\n    requires_in,\n    false,\n    expires_in,\n    public.gen_random_uuid()\n  )\n  on conflict do nothing;\nend",
          "deprecated": false,
          "description": "Create an un-satisfied task dependency between the two tasks, with the given\nrequirement style and expiration.  If the dependency already exists, nothing\nhappens.",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "cancel_task": {
          "args": "task_id text, reason text",
          "body": "declare\n  runs jsonb;\n  run jsonb;\n  new_runs jsonb;\n  last_run_id int;\nbegin\n  -- lock the task row to prevent concurrent updates\n  select tasks.runs\n  into runs\n  from tasks\n  where tasks.task_id = cancel_task.task_id\n  for update;\n\n  if runs is null then\n    -- the task row was not found\n    return;\n  end if;\n\n  last_run_id := jsonb_array_length(runs) - 1;\n  if last_run_id >= 0 then\n    -- if the most recent run is not pending or running, then\n    -- there is nothing to cancel\n    run = runs -> last_run_id;\n    if not run ->> 'state' in ('pending', 'running') then\n      return;\n    end if;\n\n    -- reconstruct the runs object with an updated run\n    new_runs = (runs - last_run_id) || jsonb_build_array(\n      run || jsonb_build_object(\n        'state', 'exception',\n        'reasonResolved', reason,\n        'resolved', now()));\n  else\n    new_runs = jsonb_build_array(\n      jsonb_build_object(\n        'state', 'exception',\n        'reasonCreated', 'exception',\n        'reasonResolved', reason,\n        'scheduled', now(),\n        'resolved', now()));\n  end if;\n\n  update tasks\n  set\n    runs = new_runs,\n    taken_until = null\n  where tasks.task_id = cancel_task.task_id;\n\n  return query\n  select tasks.retries_left, tasks.runs, tasks.taken_until\n  from tasks\n  where tasks.task_id = cancel_task.task_id;\nend",
          "deprecated": false,
          "description": "If the current run is pending or running, mark it as exception with the given\nreason.  If the task is unscheduled, a run with that status is\ncreated to represent the cancellation.  This returns the task's updated\nstatus, or nothing if the current status was not as expected.",
          "mode": "write",
          "returns": "table(retries_left integer, runs jsonb, taken_until timestamptz)",
          "serviceName": "queue"
        },
        "check_task_claim": {
          "args": "task_id text, run_id int, taken_until_in timestamptz",
          "body": "declare\n  task record;\n  runs jsonb;\n  run jsonb;\n  new_runs jsonb;\n  new_taken_until timestamptz;\nbegin\n  -- lock the task row to prevent concurrent updates\n  select tasks.retries_left, tasks.runs, tasks.deadline\n  into task\n  from tasks\n  where\n    tasks.task_id = check_task_claim.task_id and\n    tasks.taken_until = taken_until_in\n  for update;\n\n  if task.runs is null then\n    -- no such task, or taken_until did not match\n    return;\n  end if;\n\n  if jsonb_array_length(task.runs) != run_id + 1 then\n    -- run_id is not the latest run\n    return;\n  end if;\n\n  run = task.runs -> run_id;\n  if run ->> 'state' != 'running' then\n    -- run is not running\n    return;\n  end if;\n\n  if (run ->> 'takenUntil')::timestamptz != taken_until_in then\n    -- run has updated takenUntil\n    return;\n  end if;\n\n  if task.deadline < now() then\n    -- task has passed its deadline, so let check_task_deadline handle it\n    return;\n  end if;\n\n  -- reconstruct the runs object with an updated run\n  new_runs = (task.runs - run_id) || jsonb_build_array(\n    run || jsonb_build_object(\n      'state', 'exception',\n      'reasonResolved', 'claim-expired',\n      'resolved', now()));\n\n  -- add a retry if there are any left\n  if task.retries_left > 0 then\n    new_runs = new_runs || jsonb_build_array(\n      jsonb_build_object(\n        'state', 'pending',\n        'reasonCreated', 'retry',\n        'scheduled', now()));\n    task.retries_left = task.retries_left - 1;\n  end if;\n\n  update tasks\n  set\n    retries_left = task.retries_left,\n    runs = new_runs,\n    taken_until = null\n  where tasks.task_id = check_task_claim.task_id;\n\n  return query\n  select tasks.retries_left, tasks.runs, tasks.taken_until\n  from tasks\n  where tasks.task_id = check_task_claim.task_id;\nend",
          "deprecated": false,
          "description": "Check the given task for a claim on the given run expiring at the given\ntime.  If the run is still running, it is marked as claim-expired and\na retry scheduled (if retries_left).\n\nThis returns the task's updated status, or nothing if the current status\nwas not as expected.",
          "mode": "write",
          "returns": "table(retries_left integer, runs jsonb, taken_until timestamptz)",
          "serviceName": "queue"
        },
        "claim_task": {
          "args": "task_id text, run_id int, worker_group text, worker_id text, hint_id text, taken_until_in timestamptz",
          "body": "declare\n  runs jsonb;\n  run jsonb;\n  new_runs jsonb;\nbegin\n  -- lock the task row to prevent concurrent updates\n  select tasks.runs\n  into runs\n  from tasks\n  where tasks.task_id = claim_task.task_id\n  for update;\n\n  if runs is null then\n    -- the task row was not found\n    return;\n  end if;\n\n  if jsonb_array_length(runs) != run_id + 1 then\n    -- run_id is not the latest run\n    return;\n  end if;\n\n  run = runs -> run_id;\n  if run ->> 'state' != 'pending' then\n    -- run is not pending\n    return;\n  end if;\n\n  -- reconstruct the runs object with an updated run\n  new_runs = (runs - run_id) || jsonb_build_array(\n    run || jsonb_build_object(\n      'state', 'running',\n      'workerGroup', worker_group,\n      'workerId', worker_id,\n      'hintId', hint_id,\n      'takenUntil', taken_until_in,\n      'started', now()));\n\n  update tasks\n  set\n    runs = new_runs,\n    taken_until = taken_until_in\n  where tasks.task_id = claim_task.task_id;\n\n  return query\n  select tasks.retries_left, tasks.runs, tasks.taken_until\n  from tasks\n  where tasks.task_id = claim_task.task_id;\nend",
          "deprecated": false,
          "description": "Claim the given run of the given task for the given worker.  The hint is recorded in the run,\nfor comparison when the claim expires.  This returns the task's updated\nstatus, or nothing if the current status was not as expected.",
          "mode": "write",
          "returns": "table(retries_left integer, runs jsonb, taken_until timestamptz)",
          "serviceName": "queue"
        },
        "create_task": {
          "args": "task_id text,\nprovisioner_id text,\nworker_type text,\nscheduler_id text,\ntask_group_id text,\ndependencies jsonb,\nrequires task_requires,\nroutes jsonb,\npriority task_priority,\nretries integer,\ncreated timestamptz,\ndeadline timestamptz,\nexpires timestamptz,\nscopes jsonb,\npayload jsonb,\nmetadata jsonb,\ntags jsonb,\nextra jsonb",
          "body": "begin\n  insert\n  into tasks (\n    task_id,\n    provisioner_id,\n    worker_type,\n    scheduler_id,\n    task_group_id,\n    dependencies,\n    requires,\n    routes,\n    priority,\n    retries,\n    created,\n    deadline,\n    expires,\n    scopes,\n    payload,\n    metadata,\n    tags,\n    extra,\n    retries_left,\n    runs,\n    taken_until,\n    ever_resolved,\n    etag\n  )\n  values (\n    task_id,\n    provisioner_id,\n    worker_type,\n    scheduler_id,\n    task_group_id,\n    dependencies,\n    requires,\n    routes,\n    priority,\n    retries,\n    created,\n    deadline,\n    expires,\n    scopes,\n    payload,\n    metadata,\n    tags,\n    extra,\n    -- default values for the mutable bits\n    retries,\n    jsonb_build_array(),\n    null, -- not taken\n    false,\n    public.gen_random_uuid()\n  );\nend",
          "deprecated": false,
          "description": "Create a new task, without scheduling it, and with empty values\nfor the status information.",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "ensure_task_group": {
          "args": "task_group_id_in text,\nscheduler_id_in text,\nexpires_in timestamptz",
          "body": "declare\n  task_group task_groups%ROWTYPE;\nbegin\n  select *\n  from task_groups\n  where task_groups.task_group_id = task_group_id_in\n  for update\n  into task_group;\n\n  -- insert with expiration one hour later than given\n  if task_group.task_group_id is NULL then\n    begin\n      insert\n      into task_groups (task_group_id, scheduler_id, expires, etag)\n      values (\n        task_group_id_in,\n        scheduler_id_in,\n        expires_in + interval '1 hour',\n        gen_random_uuid()\n      );\n      return;\n    exception\n      when unique_violation then\n        -- we raced with another call's insert, so get that inserted row\n        select *\n        from task_groups\n        where task_groups.task_group_id = task_group_id_in\n        for update\n        into task_group;\n    end;\n  end if;\n\n  if task_group.scheduler_id != scheduler_id_in then\n    raise exception 'task group exists with different scheduler_id'\n      using errcode = '23505';\n  end if;\n\n  -- if necessary, update the expires value\n  if expires_in > task_group.expires then\n    update task_groups\n    set expires = expires_in + interval '1 hour'\n    where task_groups.task_group_id = task_group_id_in;\n  end if;\nend",
          "deprecated": false,
          "description": "Ensure that the given task group exists, has the matching scheduler_id,\nand has an expiration greater than the given expiration.  Expiration is\nbumped by an hour at a time to avoid unnecessary updates.  This returns\n23505 (UNIQUE_VIOLATION) when the group exists with a different\nscheduler_id.",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "expire_task_dependencies": {
          "args": "expires_in timestamptz",
          "body": "declare\n  count integer;\nbegin\n  delete\n  from task_dependencies\n  where expires < expires_in;\n\n  if found then\n    get diagnostics count = row_count;\n    return count;\n  end if;\n  return 0;\nend",
          "deprecated": false,
          "description": "Delete task dependencies with expiration dates before `expires_in`.\nReturns a count of rows that have been deleted.",
          "mode": "write",
          "returns": "integer",
          "serviceName": "queue"
        },
        "expire_task_groups": {
          "args": "expires_in timestamptz",
          "body": "declare\n  count integer;\nbegin\n  delete\n  from task_groups\n  where expires < expires_in;\n\n  if found then\n    get diagnostics count = row_count;\n    return count;\n  end if;\n  return 0;\nend",
          "deprecated": false,
          "description": "Delete task groups with expiration dates before `expires_in`.\nReturns a count of rows that have been deleted.",
          "mode": "write",
          "returns": "integer",
          "serviceName": "queue"
        },
        "expire_tasks": {
          "args": "expires_in timestamptz",
          "body": "declare\n  count integer;\nbegin\n  delete\n  from tasks\n  where expires < expires_in;\n\n  if found then\n    get diagnostics count = row_count;\n    return count;\n  end if;\n  return 0;\nend",
          "deprecated": false,
          "description": "Delete tasks with expiration dates before `expires_in`.\nReturns a count of rows that have been deleted.",
          "mode": "write",
          "returns": "integer",
          "serviceName": "queue"
        },
        "get_dependent_tasks": {
          "args": "required_task_id_in text, satisfied_in boolean, tasks_after_in text, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    task_dependencies.dependent_task_id,\n    task_dependencies.requires,\n    task_dependencies.satisfied\n  from task_dependencies\n  where\n    required_task_id = required_task_id_in and\n    expires > now() and\n    (satisfied_in is null or task_dependencies.satisfied = satisfied_in) and\n    (tasks_after_in is null or task_dependencies.dependent_task_id > tasks_after_in)\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get the un-expired tasks that depend on this one, limiting to only (un)satisfied\ndependencies if `satisfied_in` is not null.\n\nOnly dependencies with `dependent_task_id > tasks_after_in` are returned.\nThis supports paginated queries that are not susceptible to rows being\nadded or removed.  Typically only one of `page_offset_in` and\n`tasks_after_in` are non-null.",
          "mode": "read",
          "returns": "table(dependent_task_id text, requires task_requires, satisfied boolean)",
          "serviceName": "queue"
        },
        "get_task": {
          "args": "task_id_in text",
          "body": "begin\n  return query\n  select\n    tasks.task_id,\n    tasks.provisioner_id,\n    tasks.worker_type,\n    tasks.scheduler_id,\n    tasks.task_group_id,\n    tasks.dependencies,\n    tasks.requires,\n    tasks.routes,\n    tasks.priority,\n    tasks.retries,\n    tasks.retries_left,\n    tasks.created,\n    tasks.deadline,\n    tasks.expires,\n    tasks.scopes,\n    tasks.payload,\n    tasks.metadata,\n    tasks.tags,\n    tasks.extra,\n    tasks.runs,\n    tasks.taken_until\n  from tasks\n  where\n    tasks.task_id = task_id_in;\nend",
          "deprecated": false,
          "description": "Get all properties of a task.  Note that all properties but `runs`,\n`retries_left`, and `taken_until` are immutable.",
          "mode": "read",
          "returns": "table (\n  task_id text,\n  provisioner_id text,\n  worker_type text,\n  scheduler_id text,\n  task_group_id text,\n  dependencies jsonb,\n  requires task_requires,\n  routes jsonb,\n  priority task_priority,\n  retries integer,\n  retries_left int,\n  created timestamptz,\n  deadline timestamptz,\n  expires timestamptz,\n  scopes jsonb,\n  payload jsonb,\n  metadata jsonb,\n  tags jsonb,\n  extra jsonb,\n  runs jsonb,\n  taken_until timestamptz\n)",
          "serviceName": "queue"
        },
        "get_task_group": {
          "args": "task_group_id_in text",
          "body": "begin\n  return query\n  select\n    task_groups.task_group_id,\n    task_groups.scheduler_id,\n    task_groups.expires\n  from task_groups\n  where task_groups.task_group_id = task_group_id_in;\nend",
          "deprecated": false,
          "description": "Get a task group.",
          "mode": "read",
          "returns": "table(\n  task_group_id text,\n  scheduler_id text,\n  expires timestamptz\n)",
          "serviceName": "queue"
        },
        "get_tasks_by_task_group": {
          "args": "task_group_id_in text, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    tasks.task_id,\n    tasks.provisioner_id,\n    tasks.worker_type,\n    tasks.scheduler_id,\n    tasks.task_group_id,\n    tasks.dependencies,\n    tasks.requires,\n    tasks.routes,\n    tasks.priority,\n    tasks.retries,\n    tasks.retries_left,\n    tasks.created,\n    tasks.deadline,\n    tasks.expires,\n    tasks.scopes,\n    tasks.payload,\n    tasks.metadata,\n    tasks.tags,\n    tasks.extra,\n    tasks.runs,\n    tasks.taken_until\n  from tasks\n  where tasks.task_group_id = task_group_id_in\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get all properties of all tasks in the given task group.",
          "mode": "read",
          "returns": "table (\n  task_id text,\n  provisioner_id text,\n  worker_type text,\n  scheduler_id text,\n  task_group_id text,\n  dependencies jsonb,\n  requires task_requires,\n  routes jsonb,\n  priority task_priority,\n  retries integer,\n  retries_left int,\n  created timestamptz,\n  deadline timestamptz,\n  expires timestamptz,\n  scopes jsonb,\n  payload jsonb,\n  metadata jsonb,\n  tags jsonb,\n  extra jsonb,\n  runs jsonb,\n  taken_until timestamptz\n)",
          "serviceName": "queue"
        },
        "is_task_blocked": {
          "args": "dependent_task_id_in text",
          "body": "begin\n  perform 1\n  from task_dependencies\n  where\n    dependent_task_id = dependent_task_id_in and\n    not task_dependencies.satisfied\n  limit 1;\n  return found;\nend",
          "deprecated": false,
          "description": "Return true if the task has remaining un-satisfied dependencies.",
          "mode": "read",
          "returns": "boolean",
          "serviceName": "queue"
        },
        "is_task_group_active": {
          "args": "task_group_id_in text",
          "body": "begin\n  perform true\n  from tasks\n  where\n    task_group_id = task_group_id_in and\n    not ever_resolved\n  limit 1;\n  return found;\nend",
          "deprecated": false,
          "description": "temp, removed in next commit",
          "mode": "read",
          "returns": "boolean",
          "serviceName": "queue"
        },
        "mark_task_ever_resolved": {
          "args": "task_id_in text",
          "body": "begin\n  update tasks\n  set ever_resolved = true\n  where task_id = task_id_in;\nend",
          "deprecated": false,
          "description": "temp, removed in next commit",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "reclaim_task": {
          "args": "task_id text, run_id int, taken_until_in timestamptz",
          "body": "declare\n  runs jsonb;\n  run jsonb;\n  new_runs jsonb;\n  new_taken_until timestamptz;\nbegin\n  -- lock the task row to prevent concurrent updates\n  select tasks.runs\n  into runs\n  from tasks\n  where tasks.task_id = reclaim_task.task_id\n  for update;\n\n  if runs is null then\n    -- the task row was not found\n    return;\n  end if;\n\n  if jsonb_array_length(runs) != run_id + 1 then\n    -- run_id is not the latest run\n    return;\n  end if;\n\n  run = runs -> run_id;\n  if run ->> 'state' != 'running' then\n    -- run is not running\n    return;\n  end if;\n\n  -- always set the taken_until forward in time\n  new_taken_until = greatest(taken_until_in, (run ->> 'taken_until')::timestamptz);\n\n  -- reconstruct the runs object with an updated run\n  new_runs = (runs - run_id) || jsonb_build_array(\n    run || jsonb_build_object(\n      'takenUntil', new_taken_until));\n\n  update tasks\n  set\n    runs = new_runs,\n    taken_until = new_taken_until\n  where tasks.task_id = reclaim_task.task_id;\n\n  return query\n  select tasks.retries_left, tasks.runs, tasks.taken_until\n  from tasks\n  where tasks.task_id = reclaim_task.task_id;\nend",
          "deprecated": false,
          "description": "Relaim the given run of the given task run, until the new taken_until time.\nThis returns the task's updated status, or nothing if the current status was not as expected.",
          "mode": "write",
          "returns": "table(retries_left integer, runs jsonb, taken_until timestamptz)",
          "serviceName": "queue"
        },
        "remove_task": {
          "args": "task_id text",
          "body": "begin\n  delete\n  from tasks\n  where tasks.task_id = remove_task.task_id;\nend",
          "deprecated": false,
          "description": "Remove the given task, regardless of its expiration status.  This is\ntypically used when task creation has failed.",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "remove_task_dependency": {
          "args": "dependent_task_id_in text, required_task_id_in text",
          "body": "begin\n  delete from task_dependencies\n  where\n    dependent_task_id = dependent_task_id_in and\n    required_task_id = required_task_id_in;\nend",
          "deprecated": false,
          "description": "Mark the given dependency as satisfied.  If the dependency does not exist, nothing\nhappens.",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "rerun_task": {
          "args": "task_id text",
          "body": "declare\n  runs jsonb;\n  run jsonb;\n  last_run_id int;\n  max_runs_allowed constant int = 50;\nbegin\n  -- lock the task row to prevent concurrent updates\n  select tasks.runs\n  into runs\n  from tasks\n  where tasks.task_id = rerun_task.task_id\n  for update;\n\n  if runs is null then\n    -- the task row was not found\n    return;\n  end if;\n\n  last_run_id := jsonb_array_length(runs) - 1;\n  if last_run_id >= 0 then\n    -- verify the most recent run is not pending or running\n    run = runs -> last_run_id;\n    if run ->> 'state' in ('pending', 'running') then\n      return;\n    end if;\n  end if;\n\n  -- apply a sanity check on the number of runs\n  if last_run_id + 1 >= max_runs_allowed then\n    return;\n  end if;\n\n  update tasks\n  set\n    retries_left = least(tasks.retries, max_runs_allowed - last_run_id - 2),\n    runs = tasks.runs || jsonb_build_array(\n      jsonb_build_object(\n        'state', 'pending',\n        'reasonCreated', 'rerun',\n        'scheduled', now())),\n    taken_until = null\n  where tasks.task_id = rerun_task.task_id;\n\n  return query\n  select tasks.retries_left, tasks.runs, tasks.taken_until\n  from tasks\n  where tasks.task_id = rerun_task.task_id;\nend",
          "deprecated": false,
          "description": "Ensure that no run is currently running or pending, and then create a new\npending run with the given reason.  This also resets the retries_left\ncolumn to `retries` (unless the sanity-check maximum runs has been\nreached).  This returns the task's updated status, or nothing if the\ncurrent status was not as expected.",
          "mode": "write",
          "returns": "table(retries_left integer, runs jsonb, taken_until timestamptz)",
          "serviceName": "queue"
        },
        "resolve_task": {
          "args": "task_id text, run_id int, state text, reason text, retry_reason text",
          "body": "declare\n  task record;\n  run jsonb;\n  new_runs jsonb;\n  new_taken_until timestamptz;\nbegin\n  -- lock the task row to prevent concurrent updates\n  select tasks.retries_left, tasks.runs\n  into task\n  from tasks\n  where tasks.task_id = resolve_task.task_id\n  for update;\n\n  if task.runs is null then\n    return;\n  end if;\n\n  if jsonb_array_length(task.runs) != run_id + 1 then\n    -- run_id is not the latest run\n    return;\n  end if;\n\n  run = task.runs -> run_id;\n  if run ->> 'state' != 'running' then\n    -- run is not running\n    return;\n  end if;\n\n\n  -- reconstruct the task.runs object with an updated run\n  new_runs = (task.runs - run_id) || jsonb_build_array(\n    run || jsonb_build_object(\n      'state', state,\n      'reasonResolved', reason,\n      'resolved', now()));\n\n  -- add a retry if there are any left\n  if retry_reason is not null and task.retries_left > 0 then\n    new_runs = new_runs || jsonb_build_array(\n      jsonb_build_object(\n        'state', 'pending',\n        'reasonCreated', retry_reason,\n        'scheduled', now()));\n    task.retries_left = task.retries_left - 1;\n  end if;\n\n  update tasks\n  set\n    retries_left = task.retries_left,\n    runs = new_runs,\n    taken_until = null\n  where tasks.task_id = resolve_task.task_id;\n\n  return query\n  select tasks.retries_left, tasks.runs, tasks.taken_until\n  from tasks\n  where tasks.task_id = resolve_task.task_id;\nend",
          "deprecated": false,
          "description": "Resolve the given run with the given state and reason, setting\nrun.resolved and resetting `taken_until`.  If `retry_reason` is not null\nand there are `retries_left`, a new pending run is added, and\n`retries_left` is decremented.  This returns the task's updated status,\nor nothing if the current status was not as expected.",
          "mode": "write",
          "returns": "table(retries_left integer, runs jsonb, taken_until timestamptz)",
          "serviceName": "queue"
        },
        "resolve_task_at_deadline": {
          "args": "task_id text",
          "body": "declare\n  task record;\n  runs jsonb;\n  run jsonb;\n  new_runs jsonb;\n  new_taken_until timestamptz;\nbegin\n  -- lock the task row to prevent concurrent updates\n  select tasks.retries_left, tasks.runs, tasks.deadline\n  into task\n  from tasks\n  where\n    tasks.task_id = check_task_claim.task_id and\n    tasks.taken_until = taken_until_in\n  for update;\n\n  if task.runs is null then\n    -- no such task, or taken_until did not match\n    return;\n  end if;\n\n  if jsonb_array_length(task.runs) != run_id + 1 then\n    -- run_id is not the latest run\n    return;\n  end if;\n\n  run = task.runs -> run_id;\n  if run ->> 'state' != 'running' then\n    -- run is not running\n    return;\n  end if;\n\n  if (run ->> 'takenUntil')::timestamptz != taken_until_in then\n    -- run has updated takenUntil\n    return;\n  end if;\n\n  if task.deadline < now() then\n    -- task has passed its deadline, so let check_task_deadline handle it\n    return;\n  end if;\n\n  -- reconstruct the runs object with an updated run\n  new_runs = (task.runs - run_id) || jsonb_build_array(\n    run || jsonb_build_object(\n      'state', 'exception',\n      'reasonResolved', 'claim-expired',\n      'resolved', now()));\n\n  -- add a retry if there are any left\n  if task.retries_left > 0 then\n    new_runs = new_runs || jsonb_build_array(\n      jsonb_build_object(\n        'state', 'pending',\n        'reasonCreated', 'retry',\n        'scheduled', now()));\n    task.retries_left = task.retries_left - 1;\n  end if;\n\n  update tasks\n  set\n    retries_left = task.retries_left,\n    runs = new_runs,\n    taken_until = null\n  where tasks.task_id = check_task_claim.task_id;\n\n  return query\n  select tasks.retries_left, tasks.runs, tasks.taken_until\n  from tasks\n  where tasks.task_id = check_task_claim.task_id;\nend",
          "deprecated": false,
          "description": "The given task has reached its deadline, so mark it as resolved, adding a\nrun if necessary.  This returns the task's updated status, or nothing if\nthe current status was not as expected.",
          "mode": "write",
          "returns": "table(retries_left integer, runs jsonb, taken_until timestamptz)",
          "serviceName": "queue"
        },
        "satisfy_task_dependency": {
          "args": "dependent_task_id_in text, required_task_id_in text",
          "body": "begin\n  update task_dependencies\n  set satisfied = true\n  where\n    dependent_task_id = dependent_task_id_in and\n    required_task_id = required_task_id_in;\nend",
          "deprecated": false,
          "description": "Mark the given dependency as satisfied.  If the dependency does not exist, nothing\nhappens.",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "schedule_task": {
          "args": "task_id text, reason_created text",
          "body": "declare\n  runs jsonb;\n  run_id int;\nbegin\n  -- lock the task row to prevent concurrent updates\n  select tasks.runs\n  into runs\n  from tasks\n  where tasks.task_id = schedule_task.task_id\n  for update;\n\n  if runs is null then\n    -- the task row was not found\n    return;\n  end if;\n\n  run_id := jsonb_array_length(runs);\n  if run_id != 0 then\n    return;\n  end if;\n\n  update tasks\n  set\n    runs = jsonb_build_array(\n      jsonb_build_object(\n        'state', 'pending',\n        'reasonCreated', reason_created,\n        'scheduled', now())),\n    taken_until = null\n  where tasks.task_id = schedule_task.task_id;\n\n  return query\n  select tasks.retries_left, tasks.runs, tasks.taken_until\n  from tasks\n  where tasks.task_id = schedule_task.task_id;\nend",
          "deprecated": false,
          "description": "Schedule the initial run for a task, moving the task from \"unscheduled\" to \"pending\".\nThis returns the task's updated status, or nothing if the current status was not\nas expected.",
          "mode": "write",
          "returns": "table(retries_left integer, runs jsonb, taken_until timestamptz)",
          "serviceName": "queue"
        }
      },
      "version": 28
    },
    {
      "description": "worker-manager worker pools phase 2",
      "downgradeScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.\n  lock table worker_pool_errors;\n\n  create table wmworker_pool_errors_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n  alter table wmworker_pool_errors_entities add primary key (partition_key, row_key);\n\n  insert into wmworker_pool_errors_entities\n  select\n    encode_string_key(worker_pool_id) as partition_key,\n    encode_string_key(error_id) as row_key,\n    entity_buf_encode(\n      jsonb_build_object(\n        'PartitionKey', encode_string_key(worker_pool_id),\n        'RowKey', encode_string_key(error_id),\n        'errorId', slugid_to_uuid(error_id),\n        'workerPoolId', worker_pool_id,\n        'reported', reported,\n        'kind', kind,\n        'title', title,\n        'description', description),\n      'extra', extra::text) as value,\n    1 as version,\n    etag\n  from worker_pool_errors;\n\n  revoke select, insert, update, delete on worker_pool_errors from $db_user_prefix$_worker_manager;\n  drop table worker_pool_errors;\n  grant select, insert, update, delete on wmworker_pool_errors_entities to $db_user_prefix$_worker_manager;\nend\n",
      "methods": {
        "create_worker_pool_error": {
          "args": "error_id_in text, worker_pool_id_in text, reported_in timestamptz, kind_in text, title_in text, description_in text, extra_in jsonb",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  insert\n    into worker_pool_errors (error_id, worker_pool_id, reported, kind, title, description, extra)\n    values (error_id_in, worker_pool_id_in, reported_in, kind_in, title_in, description_in, extra_in);\n  return new_etag;\nend",
          "deprecated": false,
          "description": "Create a new worker pool error.  Raises UNIQUE_VIOLATION if the error already exists.",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "worker_manager"
        },
        "delete_worker_pool_error": {
          "args": "error_id_in text, worker_pool_id_in text",
          "body": "begin\n  delete\n  from worker_pool_errors\n  where\n  worker_pool_errors.worker_pool_id = worker_pool_id_in and\n  worker_pool_errors.error_id = error_id_in;\nend",
          "deprecated": false,
          "description": "Delete a worker pool error immediately.",
          "mode": "write",
          "returns": "void",
          "serviceName": "worker_manager"
        },
        "expire_worker_pool_errors": {
          "args": "expires_in timestamptz",
          "body": "declare\n  count integer;\nbegin\n  delete from worker_pool_errors where worker_pool_errors.reported < expires_in;\n  if found then\n    get diagnostics count = row_count;\n    return count;\n  end if;\n  return 0;\nend",
          "deprecated": false,
          "description": "Expire worker pool errors reported before `expires_in`.\nReturns a count of rows that have been deleted.",
          "mode": "write",
          "returns": "integer",
          "serviceName": "worker_manager"
        },
        "get_worker_pool_error": {
          "args": "error_id_in text, worker_pool_id_in text",
          "body": "begin\n  return query\n  select\n    worker_pool_errors.error_id,\n    worker_pool_errors.worker_pool_id,\n    worker_pool_errors.reported,\n    worker_pool_errors.kind,\n    worker_pool_errors.title,\n    worker_pool_errors.description,\n    worker_pool_errors.extra\n  from worker_pool_errors\n  where\n    worker_pool_errors.worker_pool_id = worker_pool_id_in and\n    worker_pool_errors.error_id = error_id_in;\nend",
          "deprecated": false,
          "description": "Get an existing worker pool error.  The returned table will have one or (if no such worker pool error is defined) zero rows.",
          "mode": "read",
          "returns": "table(error_id text, worker_pool_id text, reported timestamptz, kind text, title text, description text, extra jsonb)",
          "serviceName": "worker_manager"
        },
        "get_worker_pool_errors": {
          "args": "page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    worker_pool_errors.error_id,\n    worker_pool_errors.worker_pool_id,\n    worker_pool_errors.reported,\n    worker_pool_errors.kind,\n    worker_pool_errors.title,\n    worker_pool_errors.description,\n    worker_pool_errors.extra\n  from worker_pool_errors\n  order by worker_pool_errors.reported desc\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get existing worker pool errors, ordered by `reported` date.  If the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(error_id text, worker_pool_id text, reported timestamptz, kind text, title text, description text, extra jsonb)",
          "serviceName": "worker_manager"
        },
        "wmworker_pool_errors_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_row worker_pool_errors%ROWTYPE;\nbegin\n  select\n    uuid_to_slugid(properties ->> 'errorId')::text as error_id,\n    (properties ->> 'workerPoolId')::text as worker_pool_id,\n    (properties ->> 'reported')::timestamptz as reported,\n    (properties ->> 'kind')::text as kind,\n    (properties ->> 'title')::text as title,\n    (properties ->> 'description')::text as description,\n    entity_buf_decode(properties, 'extra')::jsonb as extra,\n    public.gen_random_uuid() as etag\n  into new_row;\n  if overwrite then\n    raise exception 'overwrite not implemented';\n  else\n    execute 'insert into worker_pool_errors select $1.*' using new_row;\n  end if;\n  return new_row.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "worker_manager"
        },
        "wmworker_pool_errors_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select\n    wmworker_pool_errors_entities_load.partition_key,\n    wmworker_pool_errors_entities_load.row_key,\n    entity_buf_encode(\n      jsonb_build_object(\n        'PartitionKey', wmworker_pool_errors_entities_load.partition_key,\n        'RowKey', wmworker_pool_errors_entities_load.row_key,\n        'errorId', slugid_to_uuid(error_id),\n        'workerPoolId', worker_pool_id,\n        'reported', reported,\n        'kind', kind,\n        'title', title,\n        'description', description),\n      'extra', extra::text) as value,\n    1 as version,\n    worker_pool_errors.etag as etag\n  from worker_pool_errors\n  where\n    worker_pool_errors.worker_pool_id = decode_string_key(wmworker_pool_errors_entities_load.partition_key) and\n    worker_pool_errors.error_id = decode_string_key(wmworker_pool_errors_entities_load.row_key);\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworker_pool_errors_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_row worker_pool_errors%ROWTYPE;\nbegin\n  select\n    uuid_to_slugid(properties ->> 'errorId')::text as error_id,\n    (properties ->> 'workerPoolId')::text as worker_pool_id,\n    (properties ->> 'reported')::timestamptz as reported,\n    (properties ->> 'kind')::text as kind,\n    (properties ->> 'title')::text as title,\n    (properties ->> 'description')::text as description,\n    entity_buf_decode(properties, 'extra')::jsonb as extra,\n    public.gen_random_uuid() as etag\n  into new_row;\n  update worker_pool_errors\n  set (\n    error_id,\n    worker_pool_id,\n    reported,\n    kind,\n    title,\n    description,\n    extra,\n    etag\n  ) = (\n    new_row.error_id,\n    new_row.worker_pool_id,\n    new_row.reported,\n    new_row.kind,\n    new_row.title,\n    new_row.description,\n    new_row.extra,\n    new_row.etag\n  )\n  where\n    worker_pool_errors.worker_pool_id = decode_string_key(wmworker_pool_errors_entities_modify.partition_key) and\n    worker_pool_errors.error_id = decode_string_key(wmworker_pool_errors_entities_modify.row_key) and\n    worker_pool_errors.etag = wmworker_pool_errors_entities_modify.old_etag;\n\n  if found then\n    return query select new_row.etag;\n    return;\n  end if;\n\n  perform worker_pool_errors.etag from worker_pool_errors\n  where\n    worker_pool_errors.worker_pool_id = decode_string_key(wmworker_pool_errors_entities_modify.partition_key) and\n    worker_pool_errors.error_id = decode_string_key(wmworker_pool_errors_entities_modify.row_key);\n\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworker_pool_errors_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from worker_pool_errors\n  where\n    worker_pool_errors.worker_pool_id = decode_string_key(wmworker_pool_errors_entities_remove.partition_key) and\n    worker_pool_errors.error_id = decode_string_key(wmworker_pool_errors_entities_remove.row_key)\n  returning worker_pool_errors.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "worker_manager"
        },
        "wmworker_pool_errors_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\nbegin\n  if not condition is null then\n    raise exception 'condition not supported';\n  end if;\n  return query\n    select\n      encode_string_key(wmworker_pool_errors_entities_scan.partition_key),\n      encode_string_key(wmworker_pool_errors_entities_scan.row_key),\n      entity_buf_encode(\n        jsonb_build_object(\n          'PartitionKey', encode_string_key(worker_pool_id),\n          'RowKey', encode_string_key(error_id),\n          'errorId', slugid_to_uuid(error_id),\n          'workerPoolId', worker_pool_id,\n          'reported', reported,\n          'kind', kind,\n          'title', title,\n          'description', description),\n        'extra', extra::text) as value,\n      1 as version,\n    worker_pool_errors.etag as etag\n    from worker_pool_errors\n    where\n      partition_key is null or\n      worker_pool_id = decode_string_key(partition_key) and\n      row_key is null or\n      error_id = decode_string_key(row_key)\n    order by error_id\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (page is not null and page > 0) then page\n      else 0\n    end;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "worker_manager"
        }
      },
      "migrationScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.  Note that this may lead to concurrent\n  -- updates failing; the important thing is that they not succeed without\n  -- taking effect.  Failed updates will be retried.\n  lock table wmworker_pool_errors_entities;\n\n  create table worker_pool_errors\n  as\n    select\n      uuid_to_slugid(value ->> 'errorId')::text as error_id,\n      (value ->> 'workerPoolId')::text as worker_pool_id,\n      (value ->> 'reported')::timestamptz as reported,\n      (value ->> 'kind')::text as kind,\n      (value ->> 'title')::text as title,\n      (value ->> 'description')::text as description,\n      entity_buf_decode(value, 'extra')::jsonb as extra,\n      etag\n    from wmworker_pool_errors_entities;\n  alter table worker_pool_errors add primary key (error_id);\n  alter table worker_pool_errors\n    alter column error_id set not null,\n    alter column worker_pool_id set not null,\n    alter column reported set not null,\n    alter column kind set not null,\n    alter column title set not null,\n    alter column description set not null,\n    alter column etag set not null,\n    alter column etag set default public.gen_random_uuid();\n  create index on worker_pool_errors (reported);\n\n  revoke select, insert, update, delete on wmworker_pool_errors_entities from $db_user_prefix$_worker_manager;\n  drop table wmworker_pool_errors_entities;\n  grant select, insert, update, delete on worker_pool_errors to $db_user_prefix$_worker_manager;\nend\n",
      "version": 29
    },
    {
      "description": "bugfix for sha512()",
      "downgradeScript": "begin\n  -- Compute the sha512 of the given text data.\n  -- sha512 is the algorithm that will be used to generate the hash.\n  -- This replaces Entity.keys.HashKey from taskcluster-lib-entities.\n  create or replace function sha512(t text) returns text\n  as $$\n      begin\n        return encode(digest(t, 'sha512'), 'hex');\n      end;\n  $$\n  language plpgSQL\n  strict immutable;\nend\n",
      "methods": {
      },
      "migrationScript": "begin\n  -- Compute the sha512 of the given text data.\n  -- sha512 is the algorithm that will be used to generate the hash.\n  -- This replaces Entity.keys.HashKey from taskcluster-lib-entities.\n  create or replace function sha512(t text) returns text\n  as $$\n      begin\n        -- note that because this function is called in a restricted context during\n        -- index analysis, it must explicitly reference the schema for the digest\n        -- function; see\n        -- https://www.postgresql.org/message-id/8572.1531922146%40sss.pgh.pa.us\n        return encode(public.digest(t, 'sha512'), 'hex');\n      end;\n  $$\n  language plpgSQL\n  strict immutable;\nend\n",
      "version": 30
    },
    {
      "description": "bugfix for get_worker_pool_errors_for_worker_pool",
      "methods": {
        "get_worker_pool_errors": {
          "deprecated": true
        },
        "get_worker_pool_errors_for_worker_pool": {
          "args": "error_id_in text, worker_pool_id_in text, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    worker_pool_errors.error_id,\n    worker_pool_errors.worker_pool_id,\n    worker_pool_errors.reported,\n    worker_pool_errors.kind,\n    worker_pool_errors.title,\n    worker_pool_errors.description,\n    worker_pool_errors.extra\n  from worker_pool_errors\n  where\n    (worker_pool_errors.worker_pool_id = worker_pool_id_in or worker_pool_id_in is null) and\n    (worker_pool_errors.error_id = error_id_in or error_id_in is null)\n  order by worker_pool_errors.reported desc\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get existing worker pool errors filtered by `worker_pool_id` and `error_id`,\nordered by `reported`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(error_id text, worker_pool_id text, reported timestamptz, kind text, title text, description text, extra jsonb)",
          "serviceName": "worker_manager"
        }
      },
      "version": 31
    },
    {
      "description": "hooks last-fire phase 2",
      "downgradeScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.\n  lock table hooks_last_fires;\n\n  create table last_fire_3_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n  alter table last_fire_3_entities add primary key (partition_key, row_key);\n\n  insert into last_fire_3_entities\n  select\n    encode_composite_key(hook_group_id, hook_id) as partition_key,\n    encode_string_key(task_id) as row_key,\n    jsonb_build_object(\n      'PartitionKey', encode_composite_key(hook_group_id, hook_id),\n      'RowKey', encode_string_key(task_id),\n      'hookGroupId', hook_group_id,\n      'hookId', hook_id,\n      'firedBy', fired_by,\n      'taskId', task_id,\n      'taskCreateTime', task_create_time,\n      'result', result,\n      'error', error) as value,\n    1 as version,\n    etag\n  from hooks_last_fires;\n\n  revoke select, insert, update, delete on hooks_last_fires from $db_user_prefix$_hooks;\n  drop table hooks_last_fires;\n  grant select, insert, update, delete on last_fire_3_entities to $db_user_prefix$_hooks;\nend\n",
      "methods": {
        "create_last_fire": {
          "args": "hook_group_id_in text, hook_id_in text, fired_by_in text, task_id_in text, task_create_time_in timestamptz, result_in text, error_in text",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  insert\n    into hooks_last_fires (hook_group_id, hook_id, fired_by, task_id, task_create_time, result, error, etag)\n    values (hook_group_id_in, hook_id_in, fired_by_in, task_id_in, task_create_time_in, result_in, error_in, new_etag);\n\n    return new_etag;\nend",
          "deprecated": false,
          "description": "Create a new hook last fire.  Raises UNIQUE_VIOLATION if the hook already exists.",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "hooks"
        },
        "delete_last_fires": {
          "args": "hook_group_id_in text, hook_id_in text",
          "body": "begin\n  delete from hooks_last_fires\n  where\n    hooks_last_fires.hook_group_id = hook_group_id_in and\n    hooks_last_fires.hook_id = hook_id_in;\nend",
          "deprecated": false,
          "description": "Delete last fires that match a given `hook_group_id` and `hook_id`.",
          "mode": "write",
          "returns": "void",
          "serviceName": "hooks"
        },
        "expire_last_fires": {
          "args": "",
          "body": "declare\n  count integer;\nbegin\n  delete from hooks_last_fires where hooks_last_fires.task_create_time < now() - interval '1 year';\n  if found then\n    get diagnostics count = row_count;\n    return count;\n  end if;\n  return 0;\nend",
          "deprecated": false,
          "description": "Expire last fires that are older than a year.\nReturns a count of rows that have been deleted.",
          "mode": "write",
          "returns": "integer",
          "serviceName": "hooks"
        },
        "get_last_fire": {
          "args": "hook_group_id_in text, hook_id_in text, task_id_in text",
          "body": "begin\n  return query\n  select\n    hooks_last_fires.hook_group_id,\n    hooks_last_fires.hook_id,\n    hooks_last_fires.fired_by,\n    hooks_last_fires.task_id,\n    hooks_last_fires.task_create_time,\n    hooks_last_fires.result,\n    hooks_last_fires.error,\n    hooks_last_fires.etag\n  from hooks_last_fires\n  where\n    hooks_last_fires.hook_group_id = hook_group_id_in and\n    hooks_last_fires.hook_id = hook_id_in and\n    hooks_last_fires.task_id = task_id_in;\nend",
          "deprecated": false,
          "description": "Get a hook last fire.",
          "mode": "read",
          "returns": "table(hook_group_id text, hook_id text, fired_by text, task_id text, task_create_time timestamptz, result text, error text, etag uuid)",
          "serviceName": "hooks"
        },
        "get_last_fires": {
          "args": "hook_group_id_in text, hook_id_in text, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    hooks_last_fires.hook_group_id,\n    hooks_last_fires.hook_id,\n    hooks_last_fires.fired_by,\n    hooks_last_fires.task_id,\n    hooks_last_fires.task_create_time,\n    hooks_last_fires.result,\n    hooks_last_fires.error,\n    hooks_last_fires.etag\n  from hooks_last_fires\n  where\n    hooks_last_fires.hook_group_id = hook_group_id_in and\n    hooks_last_fires.hook_id = hook_id_in\n  order by hook_group_id, hook_id, task_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get hooks last fires filtered by the `hook_group_id` and `hook_id` arguments,\nordered by `hook_group_id`, `hook_id`, and  `worker_id`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(hook_group_id text, hook_id text, fired_by text, task_id text, task_create_time timestamptz, result text, error text, etag uuid)",
          "serviceName": "hooks"
        },
        "last_fire_3_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "begin\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "hooks"
        },
        "last_fire_3_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "hooks"
        },
        "last_fire_3_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "begin\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "hooks"
        },
        "last_fire_3_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "hooks"
        },
        "last_fire_3_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "begin\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "hooks"
        }
      },
      "migrationScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.  Note that this may lead to concurrent\n  -- updates failing; the important thing is that they not succeed without\n  -- taking effect.  Failed updates will be retried.\n  lock table last_fire_3_entities;\n\n  create table hooks_last_fires\n  as\n    select\n      (value ->> 'hookGroupId')::text as hook_group_id,\n      (value ->> 'hookId')::text as hook_id,\n      (value ->> 'firedBy')::text as fired_by,\n      (value ->> 'taskId')::text as task_id,\n      (value ->> 'taskCreateTime')::timestamptz as task_create_time,\n      (value ->> 'result')::text as result,\n      (value ->> 'error')::text as error,\n      etag\n    from last_fire_3_entities;\n  alter table hooks_last_fires add primary key (hook_group_id, hook_id, task_id);\n  alter table hooks_last_fires\n    alter column hook_group_id set not null,\n    alter column hook_id set not null,\n    alter column fired_by set not null,\n    alter column task_id set not null,\n    alter column task_create_time set not null,\n    alter column result set not null,\n    alter column error set not null,\n    alter column etag set not null,\n    alter column etag set default public.gen_random_uuid();\n\n  revoke select, insert, update, delete on last_fire_3_entities from $db_user_prefix$_hooks;\n  drop table last_fire_3_entities;\n  grant select, insert, update, delete on hooks_last_fires to $db_user_prefix$_hooks;\n\nend\n",
      "version": 32
    },
    {
      "description": "hooks queues phase 2",
      "downgradeScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.\n  lock table hooks_queues;\n\n  create table queues_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n  alter table queues_entities add primary key (partition_key, row_key);\n\n  insert into queues_entities\n  select\n    encode_string_key(hook_group_id) as partition_key,\n    encode_string_key(hook_id) as row_key,\n    entity_buf_encode(\n        jsonb_build_object(\n          'PartitionKey', encode_string_key(hook_group_id),\n          'RowKey', encode_string_key(hook_id),\n          'hookGroupId', hook_group_id,\n          'hookId', hook_id,\n          'queueName', queue_name),\n        'bindings', bindings::text) as value,\n    1 as version,\n    etag\n  from hooks_queues;\n\n  revoke select, insert, update, delete on hooks_queues from $db_user_prefix$_hooks;\n  drop table hooks_queues;\n  grant select, insert, update, delete on queues_entities to $db_user_prefix$_hooks;\nend\n",
      "methods": {
        "create_hooks_queue": {
          "args": "hook_group_id_in text, hook_id_in text, queue_name_in text, bindings_in jsonb",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  insert\n    into hooks_queues (hook_group_id, hook_id, queue_name, bindings, etag)\n    values (hook_group_id_in, hook_id_in, queue_name_in, bindings_in, new_etag);\n\n    return new_etag;\nend",
          "deprecated": false,
          "description": "Create a new hooks queue.  Raises UNIQUE_VIOLATION if the hook already exists.",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "hooks"
        },
        "delete_hooks_queue": {
          "args": "hook_group_id_in text, hook_id_in text",
          "body": "begin\n  delete from hooks_queues\n  where\n    hooks_queues.hook_group_id = hook_group_id_in and\n    hooks_queues.hook_id = hook_id_in;\nend",
          "deprecated": false,
          "description": "Delete a hooks queue.",
          "mode": "write",
          "returns": "void",
          "serviceName": "hooks"
        },
        "get_hooks_queues": {
          "args": "page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    hooks_queues.hook_group_id,\n    hooks_queues.hook_id,\n    hooks_queues.queue_name,\n    hooks_queues.bindings,\n    hooks_queues.etag\n  from hooks_queues\n  order by hook_group_id, hook_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get hooks queues ordered by `hook_group_id` and `hook_id`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(hook_group_id text, hook_id text, queue_name text, bindings jsonb, etag uuid)",
          "serviceName": "hooks"
        },
        "queues_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_row hooks_queues%ROWTYPE;\nbegin\n  select\n    (properties ->> 'hookGroupId')::text,\n    (properties ->> 'hookId')::text,\n    (properties ->> 'queueName')::text,\n    entity_buf_decode(properties, 'bindings')::jsonb,\n    public.gen_random_uuid()\n  into new_row;\n  if overwrite then\n    raise exception 'overwrite not implemented';\n  else\n    execute 'insert into hooks_queues select $1.*' using new_row;\n  end if;\n  return new_row.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "hooks"
        },
        "queues_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select\n    queues_entities_load.partition_key,\n    queues_entities_load.row_key,\n    entity_buf_encode(\n      jsonb_build_object(\n        'PartitionKey', encode_string_key(hook_group_id),\n        'RowKey', encode_string_key(hook_id),\n        'hookGroupId', hook_group_id,\n        'hookId', hook_id,\n        'queueName', queue_name),\n      'bindings', bindings::text) as value,\n    1 as version,\n    hooks_queues.etag as etag\n  from hooks_queues\n  where\n    hooks_queues.hook_group_id = decode_string_key(queues_entities_load.partition_key) and hooks_queues.hook_id = decode_string_key(queues_entities_load.row_key);\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "hooks"
        },
        "queues_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_row hooks_queues%ROWTYPE;\nbegin\n  select\n    (properties ->> 'hookGroupId')::text,\n    (properties ->> 'hookId')::text,\n    (properties ->> 'queueName')::text,\n    entity_buf_decode(properties, 'bindings')::jsonb,\n    public.gen_random_uuid() as etag\n  into new_row;\n  update hooks_queues\n  set (\n    queue_name,\n    bindings,\n    etag\n  ) = (\n    new_row.queue_name,\n    new_row.bindings,\n    new_row.etag\n  )\n  where\n    hooks_queues.hook_group_id = decode_string_key(queues_entities_modify.partition_key) and\n    hooks_queues.hook_id = decode_string_key(queues_entities_modify.row_key) and\n    hooks_queues.etag = queues_entities_modify.old_etag;\n  if found then\n    return query select new_row.etag;\n    return;\n  end if;\n  perform hooks_queues.etag from hooks_queues\n  where\n    hooks_queues.hook_group_id = decode_string_key(queues_entities_modify.partition_key) and\n    hooks_queues.hook_id = decode_string_key(queues_entities_modify.row_key);\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "hooks"
        },
        "queues_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from hooks_queues\n  where\n    hooks_queues.hook_group_id = decode_string_key(queues_entities_remove.partition_key) and\n    hooks_queues.hook_id = decode_string_key(queues_entities_remove.row_key)\n  returning hooks_queues.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "hooks"
        },
        "queues_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  cond text[];\n  exp_cond_operator text;\n  exp_cond_operand timestamptz;\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if not condition is null then\n    cond := regexp_split_to_array(condition, '\\s+');\n    exp_cond_operator := cond[4];\n    exp_cond_operand := cond[5] :: timestamptz;\n\n    return query select\n      encode_string_key(hook_group_id) as partition_key,\n      encode_string_key(hook_id) as row_key,\n      entity_buf_encode(\n        jsonb_build_object(\n          'PartitionKey', encode_string_key(hook_group_id),\n          'RowKey', encode_string_key(hook_id),\n          'hookGroupId', hook_group_id,\n          'hookId', hook_id,\n          'queueName', queue_name),\n        'bindings', bindings::text) as value,\n      1 as version,\n      hooks_queues.etag as etag from hooks_queues\n    where\n      (queues_entities_scan.pk is null or decode_string_key(queues_entities_scan.pk) = hook_group_id) and\n      (queues_entities_scan.rk is null or decode_string_key(queues_entities_scan.rk) = hook_id) and\n      case\n        when exp_cond_operator = '=' then expires = exp_cond_operand\n        when exp_cond_operator = '<' then expires < exp_cond_operand\n        when exp_cond_operator = '<=' then expires <= exp_cond_operand\n        when exp_cond_operator = '>' then expires > exp_cond_operand\n        when exp_cond_operator = '>=' then expires >= exp_cond_operand\n        else expires <> exp_cond_operand\n      end\n    order by hooks_queues.hook_group_id, hooks_queues.hook_id\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (page is not null and page > 0) then page\n      else 0\n    end;\n  else\n    return query select\n      encode_string_key(hook_group_id) as partition_key,\n      encode_string_key(hook_id) as row_key,\n      entity_buf_encode(\n        jsonb_build_object(\n          'PartitionKey', encode_string_key(hook_group_id),\n          'RowKey', encode_string_key(hook_id),\n          'hookGroupId', hook_group_id,\n          'hookId', hook_id,\n          'queueName', queue_name),\n        'bindings', bindings::text) as value,\n      1 as version,\n      hooks_queues.etag as etag from hooks_queues\n    where\n      (queues_entities_scan.pk is null or decode_string_key(queues_entities_scan.pk) = hook_group_id) and\n      (queues_entities_scan.rk is null or decode_string_key(queues_entities_scan.rk) = hook_id)\n    order by hooks_queues.hook_group_id, hooks_queues.hook_id\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (size is not null and size > 0 and page is not null and page > 0) then page\n      else 0\n    end;\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "hooks"
        },
        "update_hooks_queue_bindings": {
          "args": "hook_group_id_in text, hook_id_in text, bindings_in jsonb",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  return query update hooks_queues\n  set\n    bindings = bindings_in,\n    etag = new_etag\n  where\n    hooks_queues.hook_group_id = hook_group_id_in and\n    hooks_queues.hook_id = hook_id_in\n  returning\n    hooks_queues.hook_group_id,\n    hooks_queues.hook_id,\n    hooks_queues.queue_name,\n    hooks_queues.bindings,\n    hooks_queues.etag;\nend",
          "deprecated": false,
          "description": "Update bindings of a hooks queue. If no such queue exists,\nthe return value is an empty set.",
          "mode": "write",
          "returns": "table(hook_group_id text, hook_id text, queue_name text, bindings jsonb, etag uuid)",
          "serviceName": "hooks"
        }
      },
      "migrationScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.  Note that this may lead to concurrent\n  -- updates failing; the important thing is that they not succeed without\n  -- taking effect.  Failed updates will be retried.\n  lock table queues_entities;\n\n  create table hooks_queues\n  as\n    select\n      (value ->> 'hookGroupId')::text as hook_group_id,\n      (value ->> 'hookId')::text as hook_id,\n      (value ->> 'queueName')::text as queue_name,\n      entity_buf_decode(value, 'bindings')::jsonb as bindings,\n      etag\n    from queues_entities;\n  alter table hooks_queues add primary key (hook_group_id, hook_id);\n  alter table hooks_queues\n    alter column hook_group_id set not null,\n    alter column hook_id set not null,\n    alter column queue_name set not null,\n    alter column bindings set not null,\n    alter column etag set not null,\n    alter column etag set default public.gen_random_uuid();\n\n  revoke select, insert, update, delete on queues_entities from $db_user_prefix$_hooks;\n  drop table queues_entities;\n  grant select, insert, update, delete on hooks_queues to $db_user_prefix$_hooks;\nend\n",
      "version": 33
    },
    {
      "description": "add crypto container functions `entity_to_crypto_container_v0` and `encrypted_entity_buf_encode`",
      "downgradeScript": "begin\n  drop function entity_to_crypto_container_v0(value JSONB, name text);\n  drop function encrypted_entity_buf_encode(value JSONB, name text, data jsonb);\nend\n",
      "methods": {
      },
      "migrationScript": "begin\n\n  -- Given an encrypted field from tc-lib-entities, this function will make the required transformations to the data\n  -- structure before storing it in a column.\n  create or replace function entity_to_crypto_container_v0(value jsonb, name text) returns jsonb\n  as $$\n  declare\n    chunks integer;\n    chunk integer = 0;\n    result jsonb = json_build_object('v', 0, 'kid', 'azure');\n    begin\n      chunks = (value ->> ('__bufchunks_' || name))::integer;\n      result = result || jsonb_build_object('__bufchunks_val', chunks);\n\n      loop\n        exit when chunks is null or chunk >= chunks;\n        result = jsonb_set(result, ('{__buf' || chunk || '_val}')::text[], value -> ('__buf' || chunk || '_' || name));\n        chunk = chunk + 1;\n      end loop;\n\n      return result;\n    end;\n  $$\n  language plpgSQL\n  strict immutable;\n\n  -- Replaces the keys in an encrypted column to be compatible with tc-lib-entities.\n  create or replace function encrypted_entity_buf_encode(value jsonb, name text, data jsonb) returns jsonb\n  as $$\n    declare\n      chunks integer;\n      chunk integer = 0;\n      result jsonb = jsonb_build_object();\n      begin\n        chunks = (data -> '__bufchunks_val')::integer;\n        result = jsonb_build_object('__bufchunks_' || name, chunks);\n        value = value || jsonb_build_object('__bufchunks_' || name, chunks);\n\n        loop\n          exit when chunks is null or chunk >= chunks;\n          value = value || jsonb_build_object('__buf' || chunk || '_' || name, data -> ('__buf' || chunk || '_val'));\n          chunk = chunk + 1;\n        end loop;\n\n        return value;\n      end;\n    $$\n    language plpgSQL\n    strict immutable;\n\nend\n",
      "version": 34
    },
    {
      "description": "hooks hooks phase 2",
      "downgradeScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.\n  lock table hooks;\n\n  create table hooks_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n  alter table hooks_entities add primary key (partition_key, row_key);\n\n  insert into hooks_entities\n  select\n    encode_string_key(hook_group_id) as partition_key,\n    encode_string_key(hook_id) as row_key,\n    encrypted_entity_buf_encode(\n      encrypted_entity_buf_encode(\n        entity_buf_encode(\n          entity_buf_encode(\n            entity_buf_encode(\n              entity_buf_encode(\n                entity_buf_encode(\n                  jsonb_build_object(\n                    'PartitionKey', encode_string_key(hook_group_id),\n                    'RowKey', encode_string_key(hook_id),\n                    'hookGroupId', hook_group_id,\n                    'hookId', hook_id,\n                    'nextScheduledDate', next_scheduled_date),\n                  'metadata', metadata::text),\n                'task', task::text),\n              'bindings', bindings::text),\n            'schedule', schedule::text),\n          'triggerSchema', trigger_schema::text),\n        'nextTaskId', encrypted_next_task_id),\n      'triggerToken', encrypted_trigger_token) as value,\n    1 as version,\n    etag\n  from hooks;\n\n  revoke select, insert, update, delete on hooks from $db_user_prefix$_hooks;\n  drop table hooks;\n  grant select, insert, update, delete on hooks_entities to $db_user_prefix$_hooks;\nend\n",
      "methods": {
        "create_hook": {
          "args": "hook_group_id_in text, hook_id_in text, metadata_in jsonb, task_in jsonb, bindings_in jsonb, schedule_in jsonb, encrypted_trigger_token_in jsonb, encrypted_next_task_id_in jsonb, next_scheduled_date_in timestamptz, trigger_schema_in jsonb",
          "body": "begin\n  return query insert\n    into hooks (hook_group_id, hook_id, metadata, task, bindings, schedule, encrypted_trigger_token, encrypted_next_task_id, next_scheduled_date, trigger_schema)\n    values (hook_group_id_in, hook_id_in, metadata_in, task_in, bindings_in, schedule_in, encrypted_trigger_token_in, encrypted_next_task_id_in, next_scheduled_date_in, trigger_schema_in)\n  returning\n    hooks.hook_group_id,\n    hooks.hook_id,\n    hooks.metadata,\n    hooks.task,\n    hooks.bindings,\n    hooks.schedule,\n    hooks.encrypted_trigger_token,\n    hooks.encrypted_next_task_id,\n    hooks.next_scheduled_date,\n    hooks.trigger_schema;\nend",
          "deprecated": false,
          "description": "Create a new hook. Raises UNIQUE_VIOLATION if the artifact already exists.\nReturns the newly created hook.",
          "mode": "write",
          "returns": "table(hook_group_id text, hook_id text, metadata jsonb, task jsonb, bindings jsonb, schedule jsonb, encrypted_trigger_token jsonb, encrypted_next_task_id jsonb, next_scheduled_date timestamptz, trigger_schema jsonb)",
          "serviceName": "hooks"
        },
        "delete_hook": {
          "args": "hook_group_id_in text, hook_id_in text",
          "body": "begin\n  delete from hooks\n  where\n    hooks.hook_group_id = hook_group_id_in and\n    hooks.hook_id = hook_id_in;\nend",
          "deprecated": false,
          "description": "Delete a hook.",
          "mode": "write",
          "returns": "void",
          "serviceName": "hooks"
        },
        "get_hook": {
          "args": "hook_group_id_in text, hook_id_in text",
          "body": "begin\n  return query select\n    hooks.hook_group_id,\n    hooks.hook_id,\n    hooks.metadata,\n    hooks.task,\n    hooks.bindings,\n    hooks.schedule,\n    hooks.encrypted_trigger_token,\n    hooks.encrypted_next_task_id,\n    hooks.next_scheduled_date,\n    hooks.trigger_schema\n  from hooks\n  where\n    hooks.hook_group_id = hook_group_id_in and\n    hooks.hook_id = hook_id_in;\nend",
          "deprecated": false,
          "description": "Get a hook. The returned table will have one or zero rows.",
          "mode": "read",
          "returns": "table(hook_group_id text, hook_id text, metadata jsonb, task jsonb, bindings jsonb, schedule jsonb, encrypted_trigger_token jsonb, encrypted_next_task_id jsonb, next_scheduled_date timestamptz, trigger_schema jsonb)",
          "serviceName": "hooks"
        },
        "get_hooks": {
          "args": "hook_group_id_in text, next_scheduled_date_in timestamptz, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    hooks.hook_group_id,\n    hooks.hook_id,\n    hooks.metadata,\n    hooks.task,\n    hooks.bindings,\n    hooks.schedule,\n    hooks.encrypted_trigger_token,\n    hooks.encrypted_next_task_id,\n    hooks.next_scheduled_date,\n    hooks.trigger_schema\n  from hooks\n  where\n    (hooks.hook_group_id = hook_group_id_in or hook_group_id_in is null) and\n    (hooks.next_scheduled_date < next_scheduled_date_in or next_scheduled_date_in is null)\n  order by hooks.hook_group_id, hooks.hook_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get existing hooks filtered by the optional `hook_group_id`,\nordered by the `hook_group_id` and `hook_id`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(hook_group_id text, hook_id text, metadata jsonb, task jsonb, bindings jsonb, schedule jsonb, encrypted_trigger_token jsonb, encrypted_next_task_id jsonb, next_scheduled_date timestamptz, trigger_schema jsonb)",
          "serviceName": "hooks"
        },
        "hooks_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_row hooks%ROWTYPE;\nbegin\n  select\n    (properties ->> 'hookGroupId')::text,\n    (properties ->> 'hookId')::text,\n    entity_buf_decode(properties, 'metadata')::jsonb,\n    entity_buf_decode(properties, 'task')::jsonb,\n    entity_buf_decode(properties, 'bindings')::jsonb,\n    entity_buf_decode(properties, 'schedule')::jsonb,\n    entity_to_crypto_container_v0(properties, 'triggerToken')::jsonb,\n    entity_to_crypto_container_v0(properties, 'nextTaskId')::jsonb,\n    (properties ->> 'nextScheduledDate')::timestamptz,\n    entity_buf_decode(properties, 'triggerSchema')::jsonb,\n    public.gen_random_uuid()\n  into new_row;\n  if overwrite then\n    raise exception 'overwrite not implemented';\n  else\n    execute 'insert into hooks select $1.*' using new_row;\n  end if;\n  return new_row.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "hooks"
        },
        "hooks_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select\n    hooks_entities_load.partition_key,\n    hooks_entities_load.row_key,\n    encrypted_entity_buf_encode(\n      encrypted_entity_buf_encode(\n        entity_buf_encode(\n          entity_buf_encode(\n            entity_buf_encode(\n              entity_buf_encode(\n                entity_buf_encode(\n                  jsonb_build_object(\n                    'PartitionKey', encode_string_key(hook_group_id),\n                    'RowKey', encode_string_key(hook_id),\n                    'hookGroupId', hook_group_id,\n                    'hookId', hook_id,\n                    'nextScheduledDate', next_scheduled_date),\n                  'metadata', metadata::text),\n                'task', task::text),\n              'bindings', bindings::text),\n            'schedule', schedule::text),\n          'triggerSchema', trigger_schema::text),\n        'nextTaskId', encrypted_next_task_id),\n      'triggerToken', encrypted_trigger_token) as value,\n    1 as version,\n    hooks.etag as etag\n  from hooks\n  where\n    hooks.hook_group_id = decode_string_key(hooks_entities_load.partition_key) and\n    hooks.hook_id = decode_string_key(hooks_entities_load.row_key);\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "hooks"
        },
        "hooks_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_row hooks%ROWTYPE;\nbegin\n  select\n    (properties ->> 'hookGroupId')::text,\n    (properties ->> 'hookId')::text,\n    entity_buf_decode(properties, 'metadata')::jsonb,\n    entity_buf_decode(properties, 'task')::jsonb,\n    entity_buf_decode(properties, 'bindings')::jsonb,\n    entity_buf_decode(properties, 'schedule')::jsonb,\n    entity_to_crypto_container_v0(properties, 'triggerToken')::jsonb,\n    entity_to_crypto_container_v0(properties, 'nextTaskId')::jsonb,\n    (properties ->> 'nextScheduledDate')::timestamptz,\n    entity_buf_decode(properties, 'triggerSchema')::jsonb,\n    public.gen_random_uuid() as etag\n  into new_row;\n  update hooks\n  set (\n    metadata,\n    task,\n    bindings,\n    schedule,\n    encrypted_trigger_token,\n    encrypted_next_task_id,\n    next_scheduled_date,\n    trigger_schema,\n    etag\n  ) = (\n    new_row.metadata,\n    new_row.task,\n    new_row.bindings,\n    new_row.schedule,\n    new_row.encrypted_trigger_token,\n    new_row.encrypted_next_task_id,\n    new_row.next_scheduled_date,\n    new_row.trigger_schema,\n    new_row.etag\n  )\n  where\n    hooks.hook_group_id = decode_string_key(hooks_entities_modify.partition_key) and\n    hooks.hook_id = decode_string_key(hooks_entities_modify.row_key) and\n    hooks.etag = hooks_entities_modify.old_etag;\n  if found then\n    return query select new_row.etag;\n    return;\n  end if;\n  perform hooks.etag from hooks\n  where\n    hooks.hook_group_id = decode_string_key(hooks_entities_modify.partition_key) and\n    hooks.hook_id = decode_string_key(hooks_entities_modify.row_key);\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "hooks"
        },
        "hooks_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from hooks\n  where\n    hooks.hook_group_id = decode_string_key(hooks_entities_remove.partition_key) and\n    hooks.hook_id = decode_string_key(hooks_entities_remove.row_key)\n  returning hooks.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "hooks"
        },
        "hooks_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  cond text[];\n  nsd_cond_operator text;\n  nsd_cond_operand timestamptz;\n  partition_key_var text;\n  row_key_var text;\nbegin\n  if not condition is null then\n    cond := regexp_split_to_array(condition, '\\s+');\n    nsd_cond_operator := cond[4];\n    nsd_cond_operand := cond[5] :: timestamptz;\n\n    return query select\n      encode_string_key(hook_group_id) as partition_key,\n      encode_string_key(hook_id) as row_key,\n      encrypted_entity_buf_encode(\n        encrypted_entity_buf_encode(\n          entity_buf_encode(\n            entity_buf_encode(\n              entity_buf_encode(\n                entity_buf_encode(\n                  entity_buf_encode(\n                    jsonb_build_object(\n                      'PartitionKey', encode_string_key(hook_group_id),\n                      'RowKey', encode_string_key(hook_id),\n                      'hookGroupId', hook_group_id,\n                      'hookId', hook_id,\n                      'nextScheduledDate', next_scheduled_date),\n                    'metadata', metadata::text),\n                  'task', task::text),\n                'bindings', bindings::text),\n              'schedule', schedule::text),\n            'triggerSchema', trigger_schema::text),\n          'nextTaskId', encrypted_next_task_id),\n        'triggerToken', encrypted_trigger_token) as value,\n      1 as version,\n      hooks.etag as etag from hooks\n    where\n      (hooks_entities_scan.pk is null or decode_string_key(hooks_entities_scan.pk) = hook_group_id) and\n      (hooks_entities_scan.pk is null or decode_string_key(hooks_entities_scan.rk) = hook_id) and\n      case\n        when nsd_cond_operator = '=' then next_scheduled_date = nsd_cond_operand\n        when nsd_cond_operator = '<' then next_scheduled_date < nsd_cond_operand\n        when nsd_cond_operator = '<=' then next_scheduled_date <= nsd_cond_operand\n        when nsd_cond_operator = '>' then next_scheduled_date > nsd_cond_operand\n        when nsd_cond_operator = '>=' then next_scheduled_date >= nsd_cond_operand\n        else next_scheduled_date <> nsd_cond_operand\n      end\n    order by hooks.hook_group_id, hooks.hook_id\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (page is not null and page > 0) then page\n      else 0\n    end;\n  else\n    return query select\n      encode_string_key(hook_group_id) as partition_key,\n      encode_string_key(hook_id) as row_key,\n      encrypted_entity_buf_encode(\n        encrypted_entity_buf_encode(\n          entity_buf_encode(\n            entity_buf_encode(\n              entity_buf_encode(\n                entity_buf_encode(\n                  entity_buf_encode(\n                    jsonb_build_object(\n                      'PartitionKey', encode_string_key(hook_group_id),\n                      'RowKey', encode_string_key(hook_id),\n                      'hookGroupId', hook_group_id,\n                      'hookId', hook_id,\n                      'nextScheduledDate', next_scheduled_date),\n                    'metadata', metadata::text),\n                  'task', task::text),\n                'bindings', bindings::text),\n              'schedule', schedule::text),\n            'triggerSchema', trigger_schema::text),\n          'nextTaskId', encrypted_next_task_id),\n        'triggerToken', encrypted_trigger_token) as value,\n      1 as version,\n      hooks.etag as etag from hooks\n    where\n      (hooks_entities_scan.pk is null or decode_string_key(hooks_entities_scan.pk) = hook_group_id) and\n      (hooks_entities_scan.rk is null or decode_string_key(hooks_entities_scan.rk) = hook_id)\n    order by hooks.hook_group_id, hooks.hook_id\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (size is not null and size > 0 and page is not null and page > 0) then page\n      else 0\n    end;\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "hooks"
        },
        "update_hook": {
          "args": "hook_group_id_in text, hook_id_in text, metadata_in jsonb, task_in jsonb, bindings_in jsonb, schedule_in jsonb, encrypted_trigger_token_in jsonb, encrypted_next_task_id_in jsonb, next_scheduled_date_in timestamptz, trigger_schema_in jsonb",
          "body": "declare\n  updated_row hooks%ROWTYPE;\nbegin\n  update hooks\n  set (metadata, task, bindings, schedule, encrypted_trigger_token, encrypted_next_task_id, next_scheduled_date, trigger_schema) = (\n    coalesce(metadata_in, hooks.metadata),\n    coalesce(task_in, hooks.task),\n    coalesce(bindings_in, hooks.bindings),\n    coalesce(schedule_in, hooks.schedule),\n    coalesce(encrypted_trigger_token_in, hooks.encrypted_trigger_token),\n    coalesce(encrypted_next_task_id_in, hooks.encrypted_next_task_id),\n    coalesce(next_scheduled_date_in, hooks.next_scheduled_date),\n    coalesce(trigger_schema_in, hooks.trigger_schema)\n  )\n  where\n    hooks.hook_group_id = hook_group_id_in and\n    hooks.hook_id = hook_id_in\n  returning\n    hooks.hook_group_id,\n    hooks.hook_id,\n    hooks.metadata,\n    hooks.task,\n    hooks.bindings,\n    hooks.schedule,\n    hooks.encrypted_trigger_token,\n    hooks.encrypted_next_task_id,\n    hooks.next_scheduled_date,\n    hooks.trigger_schema\n  into updated_row;\n  if found then\n    return query select\n      updated_row.hook_group_id,\n      updated_row.hook_id,\n      updated_row.metadata,\n      updated_row.task,\n      updated_row.bindings,\n      updated_row.schedule,\n      updated_row.encrypted_trigger_token,\n      updated_row.encrypted_next_task_id,\n      updated_row.next_scheduled_date,\n      updated_row.trigger_schema;\n    return;\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "Update a queue artifact.\nReturns the up-to-date hook row that have the same hook group id and hook id.",
          "mode": "write",
          "returns": "table(hook_group_id text, hook_id text, metadata jsonb, task jsonb, bindings jsonb, schedule jsonb, encrypted_trigger_token jsonb, encrypted_next_task_id jsonb, next_scheduled_date timestamptz, trigger_schema jsonb)",
          "serviceName": "hooks"
        }
      },
      "migrationScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.  Note that this may lead to concurrent\n  -- updates failing; the important thing is that they not succeed without\n  -- taking effect.  Failed updates will be retried.\n  lock table hooks_entities;\n\n  create table hooks\n  as\n    select\n      (value ->> 'hookGroupId')::text as hook_group_id,\n      (value ->> 'hookId')::text as hook_id,\n      entity_buf_decode(value, 'metadata')::jsonb as metadata,\n      entity_buf_decode(value, 'task')::jsonb as task,\n      entity_buf_decode(value, 'bindings')::jsonb as bindings,\n      entity_buf_decode(value, 'schedule')::jsonb as schedule,\n      entity_to_crypto_container_v0(value, 'triggerToken') as encrypted_trigger_token,\n      entity_to_crypto_container_v0(value, 'nextTaskId') as encrypted_next_task_id,\n      (value ->> 'nextScheduledDate')::timestamptz as next_scheduled_date,\n      entity_buf_decode(value, 'triggerSchema')::jsonb as trigger_schema,\n      etag\n    from hooks_entities;\n  alter table hooks add primary key (hook_group_id, hook_id);\n  alter table hooks\n    alter column hook_group_id set not null,\n    alter column hook_id set not null,\n    alter column metadata set not null,\n    alter column task set not null,\n    alter column bindings set not null,\n    alter column schedule set not null,\n    alter column encrypted_trigger_token set not null,\n    alter column encrypted_next_task_id set not null,\n    alter column next_scheduled_date set not null,\n    alter column trigger_schema set not null,\n    alter column etag set not null,\n    alter column etag set default public.gen_random_uuid();\n\n  revoke select, insert, update, delete on hooks_entities from $db_user_prefix$_hooks;\n  drop table hooks_entities;\n  grant select, insert, update, delete on hooks to $db_user_prefix$_hooks;\nend\n",
      "version": 35
    },
    {
      "description": "github integrations phase 2",
      "downgradeScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.\n  lock table github_integrations;\n\n  create table taskcluster_integration_owners_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n  alter table taskcluster_integration_owners_entities add primary key (partition_key, row_key);\n\n  insert into taskcluster_integration_owners_entities\n  select\n    encode_string_key(owner) as partition_key,\n    'someConstant' as row_key,\n    jsonb_build_object(\n      'PartitionKey', encode_string_key(owner),\n      'RowKey', 'someConstant',\n      'owner', owner,\n      'installationId', installation_id) as value,\n    1 as version,\n    public.gen_random_uuid() as etag\n  from github_integrations;\n\n  revoke select, insert, update, delete on github_integrations from $db_user_prefix$_github;\n  drop table github_integrations;\n  grant select, insert, update, delete on taskcluster_integration_owners_entities to $db_user_prefix$_github;\nend\n",
      "methods": {
        "get_github_integration": {
          "args": "owner_in text",
          "body": "begin\n  return query select github_integrations.owner, github_integrations.installation_id from github_integrations where github_integrations.owner = owner_in;\nend",
          "deprecated": false,
          "description": "Get a single integration by owner.",
          "mode": "read",
          "returns": "table (owner text, installation_id integer)",
          "serviceName": "github"
        },
        "get_github_integrations": {
          "args": "page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query select\n    github_integrations.owner,\n    github_integrations.installation_id\n  from github_integrations\n  order by github_integrations.installation_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get a list of integrations.",
          "mode": "read",
          "returns": "table (owner text, installation_id integer)",
          "serviceName": "github"
        },
        "taskcluster_integration_owners_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "begin\n  if not overwrite then\n    raise exception 'must allow overwrite';\n  end if;\n  insert into github_integrations (owner, installation_id) values (\n    (properties ->> 'owner')::text,\n    (properties ->> 'installationId')::integer )\n  on conflict (owner) do update set installation_id = (properties ->> 'installationId')::integer;\n  return public.gen_random_uuid(); -- we don't store this or use it so just return anything\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "github"
        },
        "taskcluster_integration_owners_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n return query\n select\n    taskcluster_integration_owners_entities_load.partition_key,\n    'someConstant' as row_key,\n    jsonb_build_object(\n      'PartitionKey', taskcluster_integration_owners_entities_load.partition_key,\n      'RowKey', 'someConstant',\n      'installationId', installation_id,\n      'owner', owner) as value,\n    1 as version,\n    public.gen_random_uuid() as etag -- we just return this for api compatibility\nfrom github_integrations\nwhere\n  github_integrations.owner = decode_string_key(taskcluster_integration_owners_entities_load.partition_key);\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_integration_owners_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "begin\n    raise exception 'github integrations are immutable';\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_integration_owners_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  delete\n  from github_integrations\n  where\n     github_integrations.owner = decode_string_key(partition_key);\n  -- tc-gh does not care if the row existed\n  return query select gen_random_uuid() as etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_integration_owners_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\nbegin\n  if not condition is null then\n    raise exception 'condition not supported';\n  end if;\n  if not partition_key is null then\n    raise exception 'can only scan entire table';\n  end if;\n  return query\n    select\n      encode_string_key(owner),\n      'someConstant' as row_key,\n      jsonb_build_object(\n        'PartitionKey', encode_string_key(owner),\n        'RowKey', 'someConstant',\n        'installationId', installation_id,\n        'owner', owner) as value,\n      1 as version,\n      public.gen_random_uuid() as etag\n    from github_integrations\n    order by owner\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (page is not null and page > 0) then page\n      else 0\n    end;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "github"
        },
        "upsert_github_integration": {
          "args": "owner_in text, installation_id_in integer",
          "body": "begin\n  insert into github_integrations (owner, installation_id) values (owner_in, installation_id_in)\n  on conflict (owner) do update set installation_id = installation_id_in;\nend",
          "deprecated": false,
          "description": "Create a single integration.",
          "mode": "write",
          "returns": "void",
          "serviceName": "github"
        }
      },
      "migrationScript": "begin\n\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.  Note that this may lead to concurrent\n  -- updates failing; the important thing is that they not succeed without\n  -- taking effect.  Failed updates will be retried.\n  lock table taskcluster_integration_owners_entities;\n\n  create table github_integrations\n  as\n    select\n      (value ->> 'owner')::text as owner,\n      (value ->> 'installationId')::integer as installation_id\n    from taskcluster_integration_owners_entities;\n  alter table github_integrations add primary key (owner);\n  alter table github_integrations\n    alter column owner set not null,\n    alter column installation_id set not null;\n\n  revoke select, insert, update, delete on taskcluster_integration_owners_entities from $db_user_prefix$_github;\n  drop table taskcluster_integration_owners_entities;\n  grant select, insert, update, delete on github_integrations to $db_user_prefix$_github;\nend\n",
      "version": 36
    },
    {
      "description": "github checks phase 2",
      "downgradeScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.\n  lock table github_checks;\n\n  create table taskcluster_check_runs_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n  alter table taskcluster_check_runs_entities add primary key (partition_key, row_key);\n  create table taskcluster_checks_to_tasks_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n  alter table taskcluster_checks_to_tasks_entities add primary key (partition_key, row_key);\n\n  insert into taskcluster_check_runs_entities\n  select\n    encode_string_key(task_group_id) as partition_key,\n    encode_string_key(task_id) as row_key,\n    jsonb_build_object(\n      'PartitionKey', encode_string_key(task_group_id),\n      'RowKey', encode_string_key(task_id),\n      'taskGroupId', task_group_id,\n      'taskId', task_id,\n      'checkSuiteId', check_suite_id,\n      'checkRunId', check_run_id) as value,\n    1 as version,\n    public.gen_random_uuid() as etag\n  from github_checks;\n\n  insert into taskcluster_checks_to_tasks_entities\n  select\n    encode_string_key(check_suite_id) as partition_key,\n    encode_string_key(check_run_id) as row_key,\n    jsonb_build_object(\n      'PartitionKey', encode_string_key(check_suite_id),\n      'RowKey', encode_string_key(check_run_id),\n      'taskGroupId', task_group_id,\n      'taskId', task_id,\n      'checkSuiteId', check_suite_id,\n      'checkRunId', check_run_id) as value,\n    1 as version,\n    public.gen_random_uuid() as etag\n  from github_checks;\n\n  revoke select, insert, update, delete on github_checks from $db_user_prefix$_github;\n  drop table github_checks;\n  grant select, insert, update, delete on taskcluster_check_runs_entities to $db_user_prefix$_github;\n  grant select, insert, update, delete on taskcluster_checks_to_tasks_entities to $db_user_prefix$_github;\nend\n",
      "methods": {
        "create_github_check": {
          "args": "task_group_id_in text, task_id_in text, check_suite_id_in text, check_run_id_in text",
          "body": "begin\n  insert into github_checks (task_group_id, task_id, check_suite_id, check_run_id) values (task_group_id_in, task_id_in, check_suite_id_in, check_run_id_in);\nend",
          "deprecated": false,
          "description": "Create a single check.",
          "mode": "write",
          "returns": "void",
          "serviceName": "github"
        },
        "get_github_check_by_task_id": {
          "args": "task_id_in text",
          "body": "begin\n  return query select\n    github_checks.task_group_id,\n    github_checks.task_id,\n    github_checks.check_suite_id,\n    github_checks.check_run_id\n  from github_checks\n  where github_checks.task_id = task_id_in;\nend",
          "deprecated": false,
          "description": "Get a single check from a task_id.",
          "mode": "read",
          "returns": "table (task_group_id text, task_id text, check_suite_id text, check_run_id text)",
          "serviceName": "github"
        },
        "taskcluster_check_runs_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_row github_checks%ROWTYPE;\nbegin\n  select\n    (properties ->> 'taskGroupId')::text as task_group_id,\n    (properties ->> 'taskId')::text as task_id,\n    (properties ->> 'checkSuiteId')::text as check_suite_id,\n    (properties ->> 'checkRunId')::text as check_run_id\n  into new_row;\n  if overwrite then\n    raise exception 'overwrite not implemented';\n  else\n    execute 'insert into github_checks select $1.*' using new_row;\n  end if;\n  return public.gen_random_uuid(); -- we just return this for api compatibility\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "github"
        },
        "taskcluster_check_runs_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n return query\n select\n    partition_key,\n    row_key,\n    jsonb_build_object(\n      'PartitionKey', partition_key,\n      'RowKey', row_key,\n      'taskGroupId', task_group_id,\n      'taskId', task_id,\n      'checkSuiteId', check_suite_id,\n      'checkRunId', check_run_id) as value,\n    1 as version,\n    public.gen_random_uuid() as etag -- we just return this for api compatibility\nfrom github_checks\nwhere\n  github_checks.task_group_id = decode_string_key(partition_key) and\n  github_checks.task_id = decode_string_key(row_key);\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_check_runs_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "begin\n    raise exception 'github integrations are immutable';\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_check_runs_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  delete\n  from github_checks\n  where\n    github_checks.task_group_id = decode_string_key(partition_key) and\n    github_checks.task_id = decode_string_key(row_key);\n  -- tc-gh does not care if the row existed\n  return query select gen_random_uuid() as etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_check_runs_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "begin\n    raise exception 'not implemented';\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_checks_to_tasks_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "begin\n  -- We do nothing here because this is always written in tandem with a check_run\n  return gen_random_uuid();\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "github"
        },
        "taskcluster_checks_to_tasks_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n return query\n select\n    partition_key,\n    row_key,\n    jsonb_build_object(\n      'PartitionKey', partition_key,\n      'RowKey', row_key,\n      'taskGroupId', task_group_id,\n      'taskId', task_id,\n      'checkSuiteId', check_suite_id,\n      'checkRunId', check_run_id) as value,\n    1 as version,\n    public.gen_random_uuid() as etag -- we just return this for api compatibility\nfrom github_checks\nwhere\n  github_checks.check_suite_id = decode_string_key(partition_key) and\n  github_checks.check_run_id = decode_string_key(row_key);\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_checks_to_tasks_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "begin\n    raise exception 'github integrations are immutable';\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_checks_to_tasks_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  delete\n  from github_checks\n  where\n  github_checks.check_suite_id = decode_string_key(partition_key) and\n  github_checks.check_run_id = decode_string_key(row_key);\n  -- tc-gh does not care if the row existed\n  return query select gen_random_uuid() as etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "github"
        },
        "taskcluster_checks_to_tasks_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "begin\n    raise exception 'not implemented';\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "github"
        }
      },
      "migrationScript": "begin\n\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.  Note that this may lead to concurrent\n  -- updates failing; the important thing is that they not succeed without\n  -- taking effect.  Failed updates will be retried.\n  lock table taskcluster_check_runs_entities;\n  lock table taskcluster_checks_to_tasks_entities;\n\n  create table github_checks\n  as\n    select\n      (value ->> 'taskGroupId')::text as task_group_id,\n      (value ->> 'taskId')::text as task_id,\n      (value ->> 'checkSuiteId')::text as check_suite_id,\n      (value ->> 'checkRunId')::text as check_run_id\n    from taskcluster_check_runs_entities;\n  alter table github_checks add primary key (task_group_id, task_id);\n  alter table github_checks\n    alter column task_group_id set not null,\n    alter column task_id set not null,\n    alter column check_suite_id set not null,\n    alter column check_run_id set not null;\n  create index on github_checks (check_suite_id, check_run_id);\n\n  revoke select, insert, update, delete on taskcluster_check_runs_entities from $db_user_prefix$_github;\n  drop table taskcluster_check_runs_entities;\n  revoke select, insert, update, delete on taskcluster_checks_to_tasks_entities from $db_user_prefix$_github;\n  drop table taskcluster_checks_to_tasks_entities;\n  grant select, insert, update, delete on github_checks to $db_user_prefix$_github;\nend\n",
      "version": 37
    },
    {
      "description": "web-server sessions phase 2",
      "downgradeScript": "begin\n\n  create table session_storage_table_entities(\n    partition_key text,\n    row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n  alter table session_storage_table_entities add primary key (partition_key, row_key);\n  grant select, insert, update, delete on session_storage_table_entities to $db_user_prefix$_web_server;\n\n  revoke select, insert, update, delete on sessions from $db_user_prefix$_web_server;\n  drop table sessions;\n\nend\n",
      "methods": {
        "expire_sessions": {
          "args": "",
          "body": "declare\n  count integer;\nbegin\n  delete from sessions where sessions.expires < now();\n  if found then\n    get diagnostics count = row_count;\n    return count;\n  end if;\n  return 0;\nend",
          "deprecated": false,
          "description": "Delete sessions that expire before the current time.\nReturns a count of rows that have been deleted.",
          "mode": "write",
          "returns": "integer",
          "serviceName": "web_server"
        },
        "session_add": {
          "args": "hashed_session_id_in text, encrypted_session_id_in jsonb, data_in jsonb, expires_in timestamptz",
          "body": "begin\n  insert into sessions(hashed_session_id, encrypted_session_id, data, expires)\n  values (\n    hashed_session_id_in,\n    encrypted_session_id_in,\n    data_in,\n    expires_in\n  ) on conflict (hashed_session_id) do\n  update\n  set (encrypted_session_id, data, expires) = (encrypted_session_id_in, data_in, expires_in)\n  where sessions.hashed_session_id = session_add.hashed_session_id_in;\nend",
          "deprecated": false,
          "description": "Set a session.\n\nIf no session exists with hashed session id `hashed_session_id_in`,\na new row is inserted, otherwise the existing session's data is replaced\nwith the data in `data_in`.",
          "mode": "write",
          "returns": "void",
          "serviceName": "web_server"
        },
        "session_load": {
          "args": "hashed_session_id_in text",
          "body": "begin\n  return query\n  select sessions.hashed_session_id, sessions.encrypted_session_id, sessions.data, sessions.expires from sessions\n  where sessions.hashed_session_id = hashed_session_id_in;\nend",
          "deprecated": false,
          "description": "Returns the session for a given hashed session id.",
          "mode": "read",
          "returns": "table(hashed_session_id text, encrypted_session_id jsonb, data jsonb, expires timestamptz)",
          "serviceName": "web_server"
        },
        "session_remove": {
          "args": "hashed_session_id_in text",
          "body": "begin\n  delete from sessions\n  where\n    sessions.hashed_session_id = hashed_session_id_in;\nend",
          "deprecated": false,
          "description": "Removes a web session",
          "mode": "write",
          "returns": "void",
          "serviceName": "web_server"
        },
        "session_storage_table_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "begin\n  raise exception 'unsuccessful create' using errcode = 'P0004';\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "web_server"
        },
        "session_storage_table_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "web_server"
        },
        "session_storage_table_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "begin\n  raise exception 'unsuccessful update' using errcode = 'P0004';\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "web_server"
        },
        "session_storage_table_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  raise exception 'unsuccessful delete' using errcode = 'P0004';\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "web_server"
        },
        "session_storage_table_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "begin\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "web_server"
        },
        "session_touch": {
          "args": "hashed_session_id_in text, data_in jsonb, expires_in timestamptz",
          "body": "begin\n  perform 1 from sessions where sessions.hashed_session_id = hashed_session_id_in;\n\n  if found then\n   update sessions\n     set\n      data = data_in,\n      expires = expires_in\n     where sessions.hashed_session_id = hashed_session_id_in;\n    return;\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "Touch a given session given a hashed session id and session `data`.\nIf the hashed session id does not exist, then an error code `P0002` will be thrown.",
          "mode": "write",
          "returns": "void",
          "serviceName": "web_server"
        }
      },
      "migrationScript": "begin\n\n  create table sessions(\n    hashed_session_id text not null,\n    encrypted_session_id jsonb not null,\n    data jsonb not null,\n    expires timestamptz not null,\n    etag uuid not null default public.gen_random_uuid()\n  );\n  alter table sessions add primary key (hashed_session_id);\n  grant select, insert, update, delete on sessions to $db_user_prefix$_web_server;\n\n  revoke select, insert, update, delete on session_storage_table_entities from $db_user_prefix$_web_server;\n  drop table session_storage_table_entities;\n\nend\n",
      "version": 38
    },
    {
      "description": "web-server authorization codes phase 2",
      "downgradeScript": "begin\n\n  create table authorization_codes_table_entities(\n    partition_key text,\n    row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n  alter table authorization_codes_table_entities add primary key (partition_key, row_key);\n  grant select, insert, update, delete on authorization_codes_table_entities to $db_user_prefix$_web_server;\n\n  revoke select, insert, update, delete on authorization_codes from $db_user_prefix$_web_server;\n  drop table authorization_codes;\n\nend\n",
      "methods": {
        "authorization_codes_table_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "begin\n  raise exception 'unsuccessful create' using errcode = 'P0004';\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "web_server"
        },
        "authorization_codes_table_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "web_server"
        },
        "authorization_codes_table_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "begin\n  raise exception 'unsuccessful update' using errcode = 'P0004';\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "web_server"
        },
        "authorization_codes_table_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  raise exception 'unsuccessful delete' using errcode = 'P0004';\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "web_server"
        },
        "authorization_codes_table_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "begin\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "web_server"
        },
        "create_authorization_code": {
          "args": "code_in text, client_id_in text, redirect_uri_in text, identity_in text, identity_provider_id_in text, expires_in timestamptz, client_details_in jsonb",
          "body": "begin\n  return query insert\n    into authorization_codes (code, client_id, redirect_uri, identity, identity_provider_id, expires, client_details)\n    values (code_in, client_id_in, redirect_uri_in, identity_in, identity_provider_id_in, expires_in, client_details_in)\n    returning\n      authorization_codes.code,\n      authorization_codes.client_id,\n      authorization_codes.redirect_uri,\n      authorization_codes.identity,\n      authorization_codes.identity_provider_id,\n      authorization_codes.expires,\n      authorization_codes.client_details;\nend",
          "deprecated": false,
          "description": "Create an authorization code.",
          "mode": "write",
          "returns": "table(code text, client_id text, redirect_uri text, identity text, identity_provider_id text, expires timestamptz, client_details jsonb)",
          "serviceName": "web_server"
        },
        "expire_authorization_codes": {
          "args": "expires_in timestamptz",
          "body": "declare\n  count integer;\nbegin\n  delete from authorization_codes where authorization_codes.expires < expires_in;\n  if found then\n    get diagnostics count = row_count;\n    return count;\n  end if;\n  return 0;\nend",
          "deprecated": false,
          "description": "Delete authorization codes that expire before `expires_in`.\nReturns a count of rows that have been deleted.",
          "mode": "write",
          "returns": "integer",
          "serviceName": "web_server"
        },
        "get_authorization_code": {
          "args": "code_in text",
          "body": "begin\n  return query\n  select\n    authorization_codes.code,\n    authorization_codes.client_id,\n    authorization_codes.redirect_uri,\n    authorization_codes.identity,\n    authorization_codes.identity_provider_id,\n    authorization_codes.expires,\n    authorization_codes.client_details\n  from authorization_codes\n  where authorization_codes.code = code_in;\nend",
          "deprecated": false,
          "description": "Get an authorization code entry given a code.",
          "mode": "read",
          "returns": "table(code text, client_id text, redirect_uri text, identity text, identity_provider_id text, expires timestamptz, client_details jsonb)",
          "serviceName": "web_server"
        }
      },
      "migrationScript": "begin\n\n  create table authorization_codes(\n    code text not null,\n    client_id text not null,\n    redirect_uri text not null,\n    identity text not null,\n    identity_provider_id text not null,\n    expires timestamptz not null,\n    client_details jsonb not null,\n    etag uuid not null default public.gen_random_uuid()\n  );\n  alter table authorization_codes add primary key (code);\n  grant select, insert, update, delete on authorization_codes to $db_user_prefix$_web_server;\n\n  revoke select, insert, update, delete on authorization_codes_table_entities from $db_user_prefix$_web_server;\n  drop table authorization_codes_table_entities;\n\nend\n",
      "version": 39
    },
    {
      "description": "web-server access tokens phase 2",
      "downgradeScript": "begin\n\n  create table access_token_table_entities(\n    partition_key text,\n    row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n  alter table access_token_table_entities add primary key (partition_key, row_key);\n  grant select, insert, update, delete on access_token_table_entities to $db_user_prefix$_web_server;\n\n  revoke select, insert, update, delete on access_tokens from $db_user_prefix$_web_server;\n  drop table access_tokens;\n\nend\n",
      "methods": {
        "access_token_table_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "begin\n  raise exception 'unsuccessful create' using errcode = 'P0004';\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "web_server"
        },
        "access_token_table_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "web_server"
        },
        "access_token_table_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "begin\n  raise exception 'unsuccessful update' using errcode = 'P0004';\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "web_server"
        },
        "access_token_table_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  raise exception 'unsuccessful delete' using errcode = 'P0004';\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "web_server"
        },
        "access_token_table_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "begin\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "web_server"
        },
        "create_access_token": {
          "args": "hashed_access_token_in text, encrypted_access_token_in jsonb, client_id_in text, redirect_uri_in text, identity_in text, identity_provider_id_in text, expires_in timestamptz, client_details_in jsonb",
          "body": "begin\n  return query insert\n    into access_tokens (hashed_access_token, encrypted_access_token, client_id, redirect_uri, identity, identity_provider_id, expires, client_details)\n    values (hashed_access_token_in, encrypted_access_token_in, client_id_in, redirect_uri_in, identity_in, identity_provider_id_in, expires_in, client_details_in)\n    returning\n      access_tokens.hashed_access_token,\n      access_tokens.encrypted_access_token,\n      access_tokens.client_id,\n      access_tokens.redirect_uri,\n      access_tokens.identity,\n      access_tokens.identity_provider_id,\n      access_tokens.expires,\n      access_tokens.client_details;\nend",
          "deprecated": false,
          "description": "Create an access token entry.",
          "mode": "write",
          "returns": "table(hashed_access_token text, encrypted_access_token jsonb, client_id text, redirect_uri text, identity text, identity_provider_id text, expires timestamptz, client_details jsonb)",
          "serviceName": "web_server"
        },
        "expire_access_tokens": {
          "args": "expires_in timestamptz",
          "body": "declare\n  count integer;\nbegin\n  delete from access_tokens where access_tokens.expires < expires_in;\n  if found then\n    get diagnostics count = row_count;\n    return count;\n  end if;\n  return 0;\nend",
          "deprecated": false,
          "description": "Delete access token entries that expireq before the current time.\nReturns a count of rows that have been deleted.",
          "mode": "write",
          "returns": "integer",
          "serviceName": "web_server"
        },
        "get_access_token": {
          "args": "hashed_access_token_in text",
          "body": "begin\n  return query\n  select\n    access_tokens.hashed_access_token,\n    access_tokens.encrypted_access_token,\n    access_tokens.client_id,\n    access_tokens.redirect_uri,\n    access_tokens.identity,\n    access_tokens.identity_provider_id,\n    access_tokens.expires,\n    access_tokens.client_details\n  from access_tokens\n  where access_tokens.hashed_access_token = hashed_access_token_in;\nend",
          "deprecated": false,
          "description": "Get an access token entry.",
          "mode": "read",
          "returns": "table(hashed_access_token text, encrypted_access_token jsonb, client_id text, redirect_uri text, identity text, identity_provider_id text, expires timestamptz, client_details jsonb)",
          "serviceName": "web_server"
        }
      },
      "migrationScript": "begin\n\n  create table access_tokens(\n    encrypted_access_token jsonb not null,\n    hashed_access_token text not null,\n    client_id text not null,\n    redirect_uri text not null,\n    identity text not null,\n    identity_provider_id text not null,\n    expires timestamptz not null,\n    client_details jsonb not null,\n    etag uuid not null default public.gen_random_uuid()\n  );\n  alter table access_tokens add primary key (hashed_access_token);\n  grant select, insert, update, delete on access_tokens to $db_user_prefix$_web_server;\n\n  revoke select, insert, update, delete on access_token_table_entities from $db_user_prefix$_web_server;\n  drop table access_token_table_entities;\n\nend\n",
      "version": 40
    },
    {
      "description": "auth clients phase 2",
      "downgradeScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.\n  lock table clients;\n\n  create table clients_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n  alter table clients_entities add primary key (partition_key, row_key);\n\n  insert into clients_entities\n  select\n    encode_string_key(client_id) as partition_key,\n    'client' as row_key,\n    entity_buf_encode(\n      encrypted_entity_buf_encode(\n        entity_buf_encode(\n          entity_buf_encode(\n            jsonb_build_object(\n              'PartitionKey', encode_string_key(client_id),\n              'RowKey', 'client',\n              'clientId', client_id,\n              'disabled', disabled::int,\n              'expires', expires),\n            'description', description),\n          'scopes', scopes::text),\n        'accessToken', encrypted_access_token),\n      'details', jsonb_build_object(\n        'created', created,\n        'lastModified', last_modified,\n        'lastDateUsed', last_date_used,\n        'lastRotated', last_rotated,\n        'deleteOnExpiration', delete_on_expiration\n      )::text) as value,\n    1 as version,\n    etag\n  from clients;\n\n  revoke select, insert, update, delete on clients from $db_user_prefix$_auth;\n  drop table clients;\n  grant select, insert, update, delete on clients_entities to $db_user_prefix$_auth;\nend\n",
      "methods": {
        "clients_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_etag uuid = gen_random_uuid();\nbegin\n  if overwrite then\n    insert into clients\n    select\n      (properties ->> 'clientId')::text as client_id,\n      entity_buf_decode(properties, 'description')::text as description,\n      entity_to_crypto_container_v0(properties, 'accessToken') as encrypted_access_token,\n      (properties ->> 'expires')::timestamptz as expires,\n      (properties ->> 'disabled')::boolean as disabled,\n      entity_buf_decode(properties, 'scopes')::jsonb as scopes,\n      (details ->> 'created')::timestamptz as created,\n      (details ->> 'lastModified')::timestamptz as last_modified,\n      (details ->> 'lastDateUsed')::timestamptz as last_date_used,\n      (details ->> 'lastRotated')::timestamptz as last_rotated,\n      (details ->> 'deleteOnExpiration')::boolean as delete_on_expiration,\n      new_etag as etag\n    from (\n      select\n        entity_buf_decode(clients_entities_create.properties, 'details')::jsonb as details\n    ) as expanded\n    on conflict (client_id) do update set\n      description = excluded.description,\n      encrypted_access_token = excluded.encrypted_access_token,\n      expires = excluded.expires,\n      disabled = excluded.disabled,\n      scopes = excluded.scopes,\n      created = excluded.created,\n      last_modified = excluded.last_modified,\n      last_date_used = excluded.last_date_used,\n      last_rotated = excluded.last_rotated,\n      delete_on_expiration = excluded.delete_on_expiration,\n      etag = excluded.etag;\n  else\n    insert into clients\n    select\n      (properties ->> 'clientId')::text as client_id,\n      entity_buf_decode(properties, 'description')::text as description,\n      entity_to_crypto_container_v0(properties, 'accessToken') as encrypted_access_token,\n      (properties ->> 'expires')::timestamptz as expires,\n      (properties ->> 'disabled')::boolean as disabled,\n      entity_buf_decode(properties, 'scopes')::jsonb as scopes,\n      (details ->> 'created')::timestamptz as created,\n      (details ->> 'lastModified')::timestamptz as last_modified,\n      (details ->> 'lastDateUsed')::timestamptz as last_date_used,\n      (details ->> 'lastRotated')::timestamptz as last_rotated,\n      (details ->> 'deleteOnExpiration')::boolean as delete_on_expiration,\n      new_etag as etag\n    from (\n      select\n        entity_buf_decode(clients_entities_create.properties, 'details')::jsonb as details\n    ) as expanded;\n  end if;\n\n\n  return new_etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "auth"
        },
        "clients_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select\n    clients_entities_load.partition_key,\n    clients_entities_load.row_key,\n    entity_buf_encode(\n      encrypted_entity_buf_encode(\n        entity_buf_encode(\n          entity_buf_encode(\n            jsonb_build_object(\n              'PartitionKey', clients_entities_load.partition_key,\n              'RowKey', clients_entities_load.row_key,\n              'clientId', client_id,\n              'disabled', disabled::int,\n              'expires', expires),\n            'description', description),\n          'scopes', scopes::text),\n        'accessToken', encrypted_access_token),\n      'details', jsonb_build_object(\n        'created', to_js_iso8601(created::text),\n        'lastModified', to_js_iso8601(last_modified::text),\n        'lastDateUsed', to_js_iso8601(last_date_used::text),\n        'lastRotated', to_js_iso8601(last_rotated::text),\n        'deleteOnExpiration', delete_on_expiration\n      )::text) as value,\n    1 as version,\n    clients.etag as etag\n  from clients\n  where\n    clients.client_id = decode_string_key(clients_entities_load.partition_key);\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "auth"
        },
        "clients_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_row clients%ROWTYPE;\nbegin\n  select\n    (properties ->> 'clientId')::text as client_id,\n    entity_buf_decode(properties, 'description')::text as description,\n    entity_to_crypto_container_v0(properties, 'accessToken') as encrypted_access_token,\n    (properties ->> 'expires')::timestamptz as expires,\n    (properties ->> 'disabled')::boolean as disabled,\n    entity_buf_decode(properties, 'scopes')::jsonb as scopes,\n    (details ->> 'created')::timestamptz as created,\n    (details ->> 'lastModified')::timestamptz as last_modified,\n    (details ->> 'lastDateUsed')::timestamptz as last_date_used,\n    (details ->> 'lastRotated')::timestamptz as last_rotated,\n    (details ->> 'deleteOnExpiration')::boolean as delete_on_expiration,\n    public.gen_random_uuid() as etag\n    from (\n      select\n        entity_buf_decode(properties, 'details')::jsonb as details\n    ) as expanded\n  into new_row;\n\n  update clients\n  set (\n    description,\n    encrypted_access_token,\n    expires,\n    disabled,\n    scopes,\n    created,\n    last_modified,\n    last_date_used,\n    last_rotated,\n    delete_on_expiration,\n    etag\n  ) = (\n    new_row.description,\n    new_row.encrypted_access_token,\n    new_row.expires,\n    new_row.disabled,\n    new_row.scopes,\n    new_row.created,\n    new_row.last_modified,\n    new_row.last_date_used,\n    new_row.last_rotated,\n    new_row.delete_on_expiration,\n    new_row.etag\n  )\n  where\n    clients.client_id = decode_string_key(clients_entities_modify.partition_key) and\n    clients.etag = clients_entities_modify.old_etag;\n\n  if found then\n    return query select new_row.etag;\n    return;\n  end if;\n\n  perform clients.etag from clients\n  where\n    clients.client_id = decode_string_key(clients_entities_modify.partition_key);\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "auth"
        },
        "clients_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  delete from clients\n  where\n    clients.client_id = decode_string_key(clients_entities_remove.partition_key)\n  returning clients.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "auth"
        },
        "clients_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  cond text[];\n  exp_cond_operator text;\n  exp_cond_operand timestamptz;\n  partition_key_var text;\n  row_key_var text;\n  expires_cond timestamptz;\nbegin\n  if not condition is null then\n    -- condition is only used for expiration scan\n    cond := regexp_split_to_array(condition, '\\s+');\n    expires_cond := cond[5]::timestamptz;\n  end if;\n\n  return query select\n    encode_string_key(client_id) as partition_key,\n    'client' as row_key,\n    entity_buf_encode(\n      encrypted_entity_buf_encode(\n        entity_buf_encode(\n          entity_buf_encode(\n            jsonb_build_object(\n              'PartitionKey', encode_string_key(client_id),\n              'RowKey', 'client',\n              'clientId', client_id,\n              'disabled', disabled::int,\n              'expires', expires),\n            'description', description),\n          'scopes', scopes::text),\n        'accessToken', encrypted_access_token),\n      'details', jsonb_build_object(\n        'created', to_js_iso8601(created::text),\n        'lastModified', to_js_iso8601(last_modified::text),\n        'lastDateUsed', to_js_iso8601(last_date_used::text),\n        'lastRotated', to_js_iso8601(last_rotated::text),\n        'deleteOnExpiration', delete_on_expiration\n      )::text) as value,\n    1 as version,\n    clients.etag as etag\n    from clients\n  where\n    (clients_entities_scan.pk is null or decode_string_key(clients_entities_scan.pk) = client_id) and\n    (expires_cond is null or expires < expires_cond)\n  order by clients.client_id\n  limit case\n    when (size is not null and size > 0) then size + 1\n    else null\n  end\n  offset case\n    when (page is not null and page > 0) then page\n    else 0\n  end;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "auth"
        },
        "create_client": {
          "args": "client_id_in text,\ndescription_in text,\nencrypted_access_token_in jsonb,\nexpires_in timestamptz,\ndisabled_in boolean,\nscopes_in jsonb,\ndelete_on_expiration_in boolean",
          "body": "begin\n  begin\n    insert into clients (\n      client_id,\n      description,\n      encrypted_access_token,\n      expires,\n      disabled,\n      scopes,\n      created,\n      last_modified,\n      last_date_used,\n      last_rotated,\n      delete_on_expiration\n    ) values (\n      client_id_in,\n      description_in,\n      encrypted_access_token_in,\n      expires_in,\n      disabled_in,\n      scopes_in,\n      now(),\n      now(),\n      now(),\n      now(),\n      delete_on_expiration_in\n    );\n  exception\n    when UNIQUE_VIOLATION then\n      perform 1\n      from clients\n      where\n        client_id = client_id_in and\n        scopes = scopes_in and\n        expires = expires_in and\n        description = description_in and\n        not disabled and\n        created > now() - interval '15 minutes';\n      if not found then\n        raise exception 'client already exists with different values' using errcode = 'unique_violation';\n      end if;\n  end;\nend",
          "deprecated": false,
          "description": "Create a new client.  The created and last_.. timestamps are all\ninitialized to the current time.  If the row exists but scopes,\ndescription, and expires match, disabled is false, and it was created in\nthe last 15 minutes, then nothing is changed.  Otherwise, a\nUNIQUE_VIOLATION is raised.",
          "mode": "write",
          "returns": "void",
          "serviceName": "auth"
        },
        "delete_client": {
          "args": "client_id_in text",
          "body": "begin\n  delete from clients\n  where client_id = client_id_in;\nend",
          "deprecated": false,
          "description": "Delete the given client.  If the client does not exist, nothing happens.",
          "mode": "write",
          "returns": "void",
          "serviceName": "auth"
        },
        "expire_clients": {
          "args": "",
          "body": "declare\n  count integer;\nbegin\n  delete from clients\n  where expires < now() and delete_on_expiration;\n\n  if found then\n    get diagnostics count = row_count;\n    return count;\n  end if;\n  return 0;\nend",
          "deprecated": false,
          "description": "Delete all clients with an 'expires' in the past and with 'delete_on_expiration' set.",
          "mode": "write",
          "returns": "integer",
          "serviceName": "auth"
        },
        "get_client": {
          "args": "client_id_in text",
          "body": "begin\n  return query\n  select\n    clients.client_id,\n    clients.description,\n    clients.encrypted_access_token,\n    clients.expires,\n    clients.disabled,\n    clients.scopes,\n    clients.created,\n    clients.last_modified,\n    clients.last_date_used,\n    clients.last_rotated,\n    clients.delete_on_expiration\n  from clients\n  where clients.client_id = client_id_in;\nend",
          "deprecated": false,
          "description": "Get a client. Returns an empty set if the client does not exist.",
          "mode": "read",
          "returns": "table (\n  client_id text,\n  description text,\n  encrypted_access_token jsonb,\n  expires timestamptz,\n  disabled boolean,\n  scopes jsonb,\n  created timestamptz,\n  last_modified timestamptz,\n  last_date_used timestamptz,\n  last_rotated timestamptz,\n  delete_on_expiration boolean\n)",
          "serviceName": "auth"
        },
        "get_clients": {
          "args": "prefix_in text, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    clients.client_id,\n    clients.description,\n    clients.encrypted_access_token,\n    clients.expires,\n    clients.disabled,\n    clients.scopes,\n    clients.created,\n    clients.last_modified,\n    clients.last_date_used,\n    clients.last_rotated,\n    clients.delete_on_expiration\n  from clients\n  where prefix_in is null or starts_with(clients.client_id, prefix_in)\n  order by clients.client_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get clients, ordered by client_id.   If specified, only clients with\nclient_id beginning with `prefix` are returned.  If the pagination\narguments are both NULL, all rows are returned.  Otherwise, page_size\nrows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table (\n  client_id text,\n  description text,\n  encrypted_access_token jsonb,\n  expires timestamptz,\n  disabled boolean,\n  scopes jsonb,\n  created timestamptz,\n  last_modified timestamptz,\n  last_date_used timestamptz,\n  last_rotated timestamptz,\n  delete_on_expiration boolean\n)",
          "serviceName": "auth"
        },
        "update_client": {
          "args": "client_id_in text,\ndescription_in text,\nencrypted_access_token_in jsonb,\nexpires_in timestamptz,\ndisabled_in boolean,\nscopes_in jsonb,\ndelete_on_expiration_in boolean",
          "body": "begin\n  update clients set\n    description = coalesce(description_in, clients.description),\n    encrypted_access_token = coalesce(encrypted_access_token_in, clients.encrypted_access_token),\n    expires = coalesce(expires_in, clients.expires),\n    disabled = coalesce(disabled_in, clients.disabled),\n    scopes = coalesce(scopes_in, clients.scopes),\n    delete_on_expiration = coalesce(delete_on_expiration_in, clients.delete_on_expiration),\n    last_modified = now(),\n    last_rotated = case when encrypted_access_token_in is null then clients.last_rotated else now() end\n  where clients.client_id = client_id_in;\n\n  if found then\n    return query select * from get_client(client_id_in);\n  end if;\nend",
          "deprecated": false,
          "description": "Update an existing client, returning the updated client or, if no such client\nexists, an empty set.  This does not implement optimistic concurrency: any non-null\narguments to this function will overwrite existing values.  The last_modified\ncolumn is updated automatically, as is last_rotated if the access token is set.",
          "mode": "write",
          "returns": "table (\n  client_id text,\n  description text,\n  encrypted_access_token jsonb,\n  expires timestamptz,\n  disabled boolean,\n  scopes jsonb,\n  created timestamptz,\n  last_modified timestamptz,\n  last_date_used timestamptz,\n  last_rotated timestamptz,\n  delete_on_expiration boolean\n)",
          "serviceName": "auth"
        },
        "update_client_last_used": {
          "args": "client_id_in text",
          "body": "begin\n  update clients\n  set last_date_used = now()\n  where clients.client_id = client_id_in;\nend",
          "deprecated": false,
          "description": "Indicate that this client has been recently used, updating its last_date_used field.\nDoes nothing if the client does not exist.",
          "mode": "write",
          "returns": "void",
          "serviceName": "auth"
        }
      },
      "migrationScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.  Note that this may lead to concurrent\n  -- updates failing; the important thing is that they not succeed without\n  -- taking effect.  Failed updates will be retried.\n  lock table clients_entities;\n\n  create table clients\n  as\n    select\n      (value ->> 'clientId')::text as client_id,\n      entity_buf_decode(value, 'description')::text as description,\n      entity_to_crypto_container_v0(value, 'accessToken') as encrypted_access_token,\n      (value ->> 'expires')::timestamptz as expires,\n      (value ->> 'disabled')::boolean as disabled,\n      entity_buf_decode(value, 'scopes')::jsonb as scopes,\n      (details ->> 'created')::timestamptz as created,\n      (details ->> 'lastModified')::timestamptz as last_modified,\n      (details ->> 'lastDateUsed')::timestamptz as last_date_used,\n      (details ->> 'lastRotated')::timestamptz as last_rotated,\n      (details ->> 'deleteOnExpiration')::boolean as delete_on_expiration,\n      etag\n    from (\n      select\n        value,\n        entity_buf_decode(value, 'details')::jsonb as details,\n        etag\n      from clients_entities\n    ) as expanded;\n  alter table clients add primary key (client_id);\n  alter table clients\n    alter column client_id set not null,\n    alter column description set not null,\n    alter column encrypted_access_token set not null,\n    alter column expires set not null,\n    alter column disabled set not null,\n    alter column scopes set not null,\n    alter column created set not null,\n    alter column last_modified set not null,\n    alter column last_date_used set not null,\n    alter column last_rotated set not null,\n    alter column delete_on_expiration set not null,\n    alter column etag set not null,\n    alter column etag set default public.gen_random_uuid();\n\n  revoke select, insert, update, delete on clients_entities from $db_user_prefix$_auth;\n  drop table clients_entities;\n  grant select, insert, update, delete on clients to $db_user_prefix$_auth;\nend\n",
      "version": 41
    },
    {
      "description": "secrets phase 2",
      "downgradeScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.\n  lock table secrets;\n\n  create table secrets_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n  alter table secrets_entities add primary key (partition_key, row_key);\n\n  insert into secrets_entities\n  select\n    encode_string_key('secrets') as partition_key,\n    encode_string_key(name) as row_key,\n    encrypted_entity_buf_encode(\n      jsonb_build_object(\n        'PartitionKey', encode_string_key('secrets'),\n        'RowKey', encode_string_key(name),\n        'expires', secrets.expires,\n        'name', secrets.name),\n      'secret', secrets.encrypted_secret) as value,\n    1 as version,\n    public.gen_random_uuid() as etag\n  from secrets;\n\n  revoke select, insert, update, delete on secrets from $db_user_prefix$_secrets;\n  drop table secrets;\n  grant select, insert, update, delete on secrets_entities to $db_user_prefix$_secrets;\nend\n",
      "methods": {
        "delete_secret": {
          "args": "name_in text",
          "body": "begin\n  delete from secrets\n  where\n    secrets.name = name_in;\nend",
          "deprecated": false,
          "description": "Delete a secret entirely",
          "mode": "write",
          "returns": "void",
          "serviceName": "secrets"
        },
        "expire_secrets": {
          "args": "",
          "body": "declare\n  count integer;\nbegin\n  delete from secrets where secrets.expires < now();\n  if found then\n    get diagnostics count = row_count;\n    return count;\n  end if;\n  return 0;\nend",
          "deprecated": false,
          "description": "Delete all secrets with an 'expires' in the past.",
          "mode": "write",
          "returns": "integer",
          "serviceName": "secrets"
        },
        "get_secret": {
          "args": "name_in text",
          "body": "begin\n  return query select secrets.name, secrets.encrypted_secret, secrets.expires from secrets\n  where\n    secrets.name = name_in and\n    secrets.expires >= now()\n  limit 1;\nend",
          "deprecated": false,
          "description": "Get a single secret (including secret content and expiration)",
          "mode": "read",
          "returns": "table(name text, encrypted_secret jsonb, expires timestamptz)",
          "serviceName": "secrets"
        },
        "get_secrets": {
          "args": "page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query select secrets.name from secrets\n  where\n    secrets.expires >= now()\n  order by secrets.name\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get many secrets at once. This only includes names.\nFetch an individual secret to get the contents",
          "mode": "read",
          "returns": "table(name text)",
          "serviceName": "secrets"
        },
        "secrets_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "begin\n  if not overwrite then\n    raise exception 'must overwrite';\n  end if;\n  insert into secrets (name, encrypted_secret, expires) values (\n    (properties ->> 'name')::text,\n    entity_to_crypto_container_v0(properties, 'secret'),\n    (properties ->> 'expires')::timestamptz\n  ) on conflict (name) do update set\n    encrypted_secret = entity_to_crypto_container_v0(properties, 'secret'),\n    expires = (properties ->> 'expires')::timestamptz;\n  return public.gen_random_uuid();\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "secrets"
        },
        "secrets_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select\n    secrets_entities_load.partition_key,\n    secrets_entities_load.row_key,\n    encrypted_entity_buf_encode(\n      jsonb_build_object(\n        'PartitionKey', secrets_entities_load.partition_key,\n        'RowKey', secrets_entities_load.row_key,\n        'expires', secrets.expires,\n        'name', secrets.name),\n      'secret', secrets.encrypted_secret) as value,\n    1 as version,\n    public.gen_random_uuid() as etag\n  from secrets\n  where\n    secrets.name = decode_string_key(secrets_entities_load.row_key);\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "secrets"
        },
        "secrets_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "begin\n  raise exception 'not implemented';\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "secrets"
        },
        "secrets_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from secrets\n  where\n    secrets.name = decode_string_key(row_key)\n  returning public.gen_random_uuid();\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "secrets"
        },
        "secrets_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\ncond text[];\nexp_cond_field text;\nexp_cond_operator text;\nexp_cond_operand timestamptz;\nbegin\n  if not condition is null then\n    cond := regexp_split_to_array(condition, '\\s+');\n    exp_cond_field := trim(cond[3], '''');\n    exp_cond_operator := cond[4];\n    exp_cond_operand := cond[5] :: timestamptz;\n\n    if exp_cond_operator != '<' or exp_cond_field != 'expires' then\n      raise exception 'secrets_entities_scan only supports `expires < <timestamp>` conditions. Got (%)', exp_cond_operator;\n    end if;\n  end if;\n  return query\n    select\n      encode_string_key('secrets'),\n      encode_string_key(secrets.name),\n      encrypted_entity_buf_encode(\n        jsonb_build_object(\n          'PartitionKey', encode_string_key('secrets'),\n          'RowKey', encode_string_key(secrets.name),\n          'expires', secrets.expires,\n          'name', secrets.name),\n        'secret', secrets.encrypted_secret) as value,\n      1 as version,\n    public.gen_random_uuid() as etag\n    from secrets\n    where\n      row_key is null or\n      secrets.name = decode_string_key(row_key) or\n      (exp_cond_operand is NULL or expires < exp_cond_operand)\n    order by name\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (page is not null and page > 0) then page\n      else 0\n    end;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "secrets"
        },
        "upsert_secret": {
          "args": "name_in text, encrypted_secret_in jsonb, expires_in timestamptz",
          "body": "begin\n  insert into secrets (name, encrypted_secret, expires) values (\n    name_in,\n    encrypted_secret_in,\n    expires_in\n  ) on conflict (name) do update set\n    encrypted_secret = encrypted_secret_in,\n    expires = expires_in;\nend",
          "deprecated": false,
          "description": "Store an encrypted secret whether it is new or being updated",
          "mode": "write",
          "returns": "void",
          "serviceName": "secrets"
        }
      },
      "migrationScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.  Note that this may lead to concurrent\n  -- updates failing; the important thing is that they not succeed without\n  -- taking effect.  Failed updates will be retried.\n  lock table secrets_entities;\n\n  create table secrets\n  as\n    select\n      (value ->> 'name')::text as name,\n      entity_to_crypto_container_v0(value,'secret') as encrypted_secret,\n      (value ->> 'expires')::timestamptz as expires\n    from secrets_entities;\n  alter table secrets add primary key (name);\n  alter table secrets\n    alter column name set not null,\n    alter column encrypted_secret set not null,\n    alter column expires set not null;\n\n  revoke select, insert, update, delete on secrets_entities from $db_user_prefix$_secrets;\n  drop table secrets_entities;\n  grant select, insert, update, delete on secrets to $db_user_prefix$_secrets;\nend\n",
      "version": 42
    },
    {
      "description": "queue workers phase 2",
      "downgradeScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.\n  lock table queue_workers;\n\n  create table queue_worker_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n  alter table queue_worker_entities add primary key (partition_key, row_key);\n\n  insert into queue_worker_entities\n  select\n    encode_composite_key(provisioner_id, worker_type) as partition_key,\n    encode_composite_key(worker_group, worker_id) as row_key,\n    entity_buf_encode(\n      jsonb_build_object(\n        'PartitionKey', encode_composite_key(provisioner_id, worker_type),\n        'RowKey', encode_composite_key(worker_group, worker_id),\n        'provisionerId', provisioner_id,\n        'workerType', worker_type,\n        'workerGroup', worker_group,\n        'workerId', worker_id,\n        'quarantineUntil', quarantine_until,\n        'expires', expires,\n        'firstClaim', first_claim),\n      'recentTasks', recent_tasks::text) as value,\n    1 as version,\n    etag\n  from queue_workers;\n\n  revoke select, insert, update, delete on queue_workers from $db_user_prefix$_queue;\n  drop table queue_workers;\n  grant select, insert, update, delete on queue_worker_entities to $db_user_prefix$_queue;\nend\n",
      "methods": {
        "create_queue_worker": {
          "args": "provisioner_id_in text, worker_type_in text, worker_group_in text, worker_id_in text, quarantine_until_in timestamptz, expires_in timestamptz, first_claim_in timestamptz, recent_tasks_in jsonb",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  insert\n    into queue_workers (provisioner_id, worker_type, worker_group, worker_id, quarantine_until, expires, first_claim, recent_tasks)\n    values (provisioner_id_in, worker_type_in, worker_group_in, worker_id_in, quarantine_until_in, expires_in, first_claim_in, recent_tasks_in);\n    return new_etag;\nend",
          "deprecated": false,
          "description": "Create a new queue worker.  Raises UNIQUE_VIOLATION if the worker already exists.",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "expire_queue_workers": {
          "args": "expires_in timestamptz",
          "body": "declare\n  count integer;\nbegin\n  delete from queue_workers\n  where\n    queue_workers.expires < expires_in and\n    (queue_workers.expires < expires_in and queue_workers.quarantine_until < expires_in);\n  if found then\n    get diagnostics count = row_count;\n    return count;\n  end if;\n  return 0;\nend",
          "deprecated": false,
          "description": "Expire non-quarantined queue workers that come before `expires_in`.\nReturns a count of rows that have been deleted.",
          "mode": "write",
          "returns": "integer",
          "serviceName": "queue"
        },
        "get_queue_worker": {
          "args": "provisioner_id_in text, worker_type_in text, worker_group_in text, worker_id_in text, expires_in timestamptz",
          "body": "begin\n  return query\n  select\n    queue_workers.provisioner_id,\n    queue_workers.worker_type,\n    queue_workers.worker_group,\n    queue_workers.worker_id,\n    queue_workers.quarantine_until,\n    queue_workers.expires,\n    queue_workers.first_claim,\n    queue_workers.recent_tasks,\n    queue_workers.etag\n  from queue_workers\n  where\n    queue_workers.provisioner_id = provisioner_id_in and\n    queue_workers.worker_type = worker_type_in and\n    queue_workers.worker_group = worker_group_in and\n    queue_workers.worker_id = worker_id_in and\n    (queue_workers.expires > expires_in or queue_workers.quarantine_until > expires_in);\n  end",
          "deprecated": false,
          "description": "Get a non-expired queue worker by provisioner_id, worker_type, worker_group, and worker_id.\nWorkers are not considered expired until after their quarantine date expires.",
          "mode": "read",
          "returns": "table(provisioner_id text, worker_type text, worker_group text, worker_id text, quarantine_until timestamptz, expires timestamptz, first_claim timestamptz, recent_tasks jsonb, etag uuid)",
          "serviceName": "queue"
        },
        "get_queue_workers": {
          "args": "provisioner_id_in text, worker_type_in text, expires_in timestamptz, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    queue_workers.provisioner_id,\n    queue_workers.worker_type,\n    queue_workers.worker_group,\n    queue_workers.worker_id,\n    queue_workers.quarantine_until,\n    queue_workers.expires,\n    queue_workers.first_claim,\n    queue_workers.recent_tasks,\n    queue_workers.etag\n  from queue_workers\n  where\n    (queue_workers.provisioner_id = provisioner_id_in or get_queue_workers.provisioner_id_in is null) and\n    (queue_workers.worker_type = worker_type_in or get_queue_workers.worker_type_in is null) and\n    ((queue_workers.expires > expires_in and queue_workers.quarantine_until < expires_in) or get_queue_workers.expires_in is null)\n  order by provisioner_id, worker_type, worker_group, worker_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get non-expired queue workers ordered by provisioner_id, worker_type, worker_group, and worker_id.\nWorkers are not considered expired until after their quarantine date expires.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(provisioner_id text, worker_type text, worker_group text, worker_id text, quarantine_until timestamptz, expires timestamptz, first_claim timestamptz, recent_tasks jsonb, etag uuid)",
          "serviceName": "queue"
        },
        "queue_worker_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_row queue_workers%ROWTYPE;\nbegin\n  select\n    (properties ->> 'provisionerId')::text,\n    (properties ->> 'workerType')::text,\n    (properties ->> 'workerGroup')::text,\n    (properties ->> 'workerId')::text,\n    entity_buf_decode(properties, 'recentTasks')::jsonb,\n    (properties ->> 'quarantineUntil')::timestamptz,\n    (properties ->> 'expires')::timestamptz,\n    (properties ->> 'firstClaim')::timestamptz,\n    public.gen_random_uuid()\n  into new_row;\n  if overwrite then\n    raise exception 'overwrite not implemented';\n  else\n    execute 'insert into queue_workers select $1.*' using new_row;\n  end if;\n  return new_row.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "queue_worker_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "declare\n  decoded_composite_partition_key text[];\n  decoded_composite_row_key text[];\nbegin\n  decoded_composite_partition_key := decode_composite_key(queue_worker_entities_load.partition_key);\n  decoded_composite_row_key := decode_composite_key(queue_worker_entities_load.row_key);\n  return query\n  select\n    queue_worker_entities_load.partition_key,\n    queue_worker_entities_load.row_key,\n    entity_buf_encode(\n      jsonb_build_object(\n        'PartitionKey', encode_composite_key(provisioner_id, worker_type),\n        'RowKey', encode_composite_key(worker_group, worker_id),\n        'provisionerId', provisioner_id,\n        'workerType', worker_type,\n        'workerGroup', worker_group,\n        'workerId', worker_id,\n        'quarantineUntil', quarantine_until,\n        'expires', expires,\n        'firstClaim', first_claim),\n      'recentTasks', recent_tasks::text) as value,\n    1 as version,\n    queue_workers.etag as etag\n  from queue_workers\n  where\n    queue_workers.provisioner_id = decoded_composite_partition_key[1] and queue_workers.worker_type = decoded_composite_partition_key[2] and\n    queue_workers.worker_group = decoded_composite_row_key[1] and queue_workers.worker_id = decoded_composite_row_key[2];\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_worker_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  decoded_composite_partition_key text[];\n  decoded_composite_row_key text[];\n  new_etag uuid;\nbegin\n  -- NOTE: queue only updates expires, quarantine_until, and recent_tasks, so that's all that's supported here\n  new_etag = public.gen_random_uuid();\n  decoded_composite_partition_key := decode_composite_key(queue_worker_entities_modify.partition_key);\n  decoded_composite_row_key := decode_composite_key(queue_worker_entities_modify.row_key);\n  update queue_workers\n  set (\n    expires,\n    quarantine_until,\n    recent_tasks,\n    etag\n  ) = (\n    (properties ->> 'expires')::timestamptz,\n    (properties ->> 'quarantineUntil')::timestamptz,\n    entity_buf_decode(properties, 'recentTasks')::jsonb,\n    new_etag\n  )\n  where\n    queue_workers.provisioner_id = decoded_composite_partition_key[1] and queue_workers.worker_type = decoded_composite_partition_key[2] and\n    queue_workers.worker_group = decoded_composite_row_key[1] and queue_workers.worker_id = decoded_composite_row_key[2] and\n    queue_workers.etag = queue_worker_entities_modify.old_etag;\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n  perform queue_workers.etag from queue_workers\n  where\n    queue_workers.provisioner_id = decoded_composite_partition_key[1] and queue_workers.worker_type = decoded_composite_partition_key[2] and\n    queue_workers.worker_group = decoded_composite_row_key[1] and queue_workers.worker_id = decoded_composite_row_key[2];\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_worker_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "declare\n  decoded_composite_partition_key text[];\n  decoded_composite_row_key text[];\nbegin\n  decoded_composite_partition_key := decode_composite_key(queue_worker_entities_remove.partition_key);\n  decoded_composite_row_key := decode_composite_key(queue_worker_entities_remove.row_key);\n  return query delete from queue_workers\n  where\n    queue_workers.provisioner_id = decoded_composite_partition_key[1] and queue_workers.worker_type = decoded_composite_partition_key[2] and\n    queue_workers.worker_group = decoded_composite_row_key[1] and queue_workers.worker_id = decoded_composite_row_key[2]\n  returning queue_workers.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_worker_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  cond text[];\n  decoded_composite_partition_key text[];\n  decoded_composite_row_key text[];\n  exp_cond_field text;\n  exp_cond_operator text;\n  exp_cond_operand timestamptz;\nbegin\n  decoded_composite_partition_key := decode_composite_key(pk);\n  decoded_composite_row_key := decode_composite_key(rk);\n  if not condition is null then\n    cond := regexp_split_to_array(condition, '\\s+');\n    exp_cond_field := cond[3];\n    exp_cond_operator := cond[4];\n    exp_cond_operand := cond[5] :: timestamptz;\n\n    return query select\n      encode_composite_key(provisioner_id, worker_type) as partition_key,\n      encode_composite_key(worker_group, worker_id) as row_key,\n      entity_buf_encode(\n        jsonb_build_object(\n          'PartitionKey', encode_composite_key(provisioner_id, worker_type),\n          'RowKey', encode_composite_key(worker_group, worker_id),\n          'provisionerId', provisioner_id,\n          'workerType', worker_type,\n          'workerGroup', worker_group,\n          'workerId', worker_id,\n          'quarantineUntil', quarantine_until,\n          'expires', expires,\n          'firstClaim', first_claim),\n        'recentTasks', recent_tasks::text) as value,\n      1 as version,\n      queue_workers.etag as etag\n    from queue_workers\n    where\n      (queue_worker_entities_scan.pk is null or decoded_composite_partition_key[1] = provisioner_id and decoded_composite_partition_key[2] = worker_type) and\n      (queue_worker_entities_scan.rk is null or decoded_composite_row_key[1] = worker_group and decoded_composite_row_key[2] = worker_id) and\n      case\n        when exp_cond_field = '''quarantineUntil''' then\n        case\n          when exp_cond_operator = '<' then quarantine_until < exp_cond_operand\n          when exp_cond_operator = '>' then quarantine_until > exp_cond_operand\n        end\n        when exp_cond_field = '''expires''' then\n        case\n          when exp_cond_operator = '=' then expires = exp_cond_operand\n          when exp_cond_operator = '<' then expires < exp_cond_operand\n          when exp_cond_operator = '<=' then expires <= exp_cond_operand\n          when exp_cond_operator = '>' then expires > exp_cond_operand\n          when exp_cond_operator = '>=' then expires >= exp_cond_operand\n          else expires <> exp_cond_operand\n        end\n      end\n    order by queue_workers.provisioner_id, queue_workers.worker_type, queue_workers.worker_group, queue_workers.worker_id\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (page is not null and page > 0) then page\n      else 0\n    end;\n  else\n    return query select\n      encode_composite_key(provisioner_id, worker_type) as partition_key,\n      encode_composite_key(worker_group, worker_id) as row_key,\n      entity_buf_encode(\n        jsonb_build_object(\n          'PartitionKey', encode_composite_key(provisioner_id, worker_type),\n          'RowKey', encode_composite_key(worker_group, worker_id),\n          'provisionerId', provisioner_id,\n          'workerType', worker_type,\n          'workerGroup', worker_group,\n          'workerId', worker_id,\n          'quarantineUntil', quarantine_until,\n          'expires', expires,\n          'firstClaim', first_claim),\n      'recentTasks', recent_tasks::text) as value,\n      1 as version,\n      queue_workers.etag as etag\n    from queue_workers\n    where\n      (queue_worker_entities_scan.pk is null or decoded_composite_partition_key[1] = provisioner_id and decoded_composite_partition_key[2] = worker_type) and\n      (queue_worker_entities_scan.rk is null or decoded_composite_row_key[1] = worker_group and decoded_composite_row_key[2] = worker_id)\n    order by queue_workers.provisioner_id, queue_workers.worker_type, queue_workers.worker_group, queue_workers.worker_id\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (size is not null and size > 0 and page is not null and page > 0) then page\n      else 0\n    end;\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "update_queue_worker": {
          "args": "provisioner_id_in text, worker_type_in text, worker_group_in text, worker_id_in text, quarantine_until_in timestamptz, expires_in timestamptz, recent_tasks_in jsonb",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  return query update queue_workers\n  set\n    quarantine_until = quarantine_until_in,\n    expires = expires_in,\n    recent_tasks = recent_tasks_in,\n    etag = new_etag\n  where\n    queue_workers.provisioner_id = provisioner_id_in and\n    queue_workers.worker_type = worker_type_in and\n    queue_workers.worker_group = worker_group_in and\n    queue_workers.worker_id = worker_id_in\n  returning\n    queue_workers.provisioner_id,\n    queue_workers.worker_type,\n    queue_workers.worker_group,\n    queue_workers.worker_id,\n    queue_workers.quarantine_until,\n    queue_workers.expires,\n    queue_workers.first_claim,\n    queue_workers.recent_tasks,\n    queue_workers.etag;\nend",
          "deprecated": false,
          "description": "Update a queue worker's quarantine_until, expires, and recent_tasks.\nAll parameters must be supplied.",
          "mode": "write",
          "returns": "table(provisioner_id text, worker_type text, worker_group text, worker_id text, quarantine_until timestamptz, expires timestamptz, first_claim timestamptz, recent_tasks jsonb, etag uuid)",
          "serviceName": "queue"
        }
      },
      "migrationScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.  Note that this may lead to concurrent\n  -- updates failing; the important thing is that they not succeed without\n  -- taking effect.  Failed updates will be retried.\n  lock table queue_worker_entities;\n\n  create table queue_workers\n  as\n    select\n      (value ->> 'provisionerId')::text as provisioner_id,\n      (value ->> 'workerType')::text as worker_type,\n      (value ->> 'workerGroup')::text as worker_group,\n      (value ->> 'workerId')::text as worker_id,\n      entity_buf_decode(value, 'recentTasks')::jsonb as recent_tasks,\n      (value ->> 'quarantineUntil')::timestamptz as quarantine_until,\n      (value ->> 'expires')::timestamptz as expires,\n      (value ->> 'firstClaim')::timestamptz as first_claim,\n      etag\n    from queue_worker_entities;\n  alter table queue_workers add primary key (provisioner_id, worker_type, worker_group, worker_id);\n  alter table queue_workers\n    alter column provisioner_id set not null,\n    alter column worker_type set not null,\n    alter column worker_group set not null,\n    alter column worker_id set not null,\n    alter column recent_tasks set not null,\n    alter column quarantine_until set not null,\n    alter column expires set not null,\n    alter column first_claim set not null,\n    alter column etag set not null,\n    alter column etag set default public.gen_random_uuid();\n\n  revoke select, insert, update, delete on queue_worker_entities from $db_user_prefix$_queue;\n  drop table queue_worker_entities;\n  grant select, insert, update, delete on queue_workers to $db_user_prefix$_queue;\nend\n",
      "version": 43
    },
    {
      "description": "queue worker types phase 2",
      "downgradeScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.\n  lock table queue_worker_types;\n\n  create table queue_worker_type_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n  alter table queue_worker_type_entities add primary key (partition_key, row_key);\n\n  insert into queue_worker_type_entities\n  select\n    encode_string_key(provisioner_id) as partition_key,\n    encode_string_key(worker_type) as row_key,\n    entity_buf_encode(\n      jsonb_build_object(\n        'PartitionKey', encode_string_key(provisioner_id),\n        'RowKey', encode_string_key(worker_type),\n        'provisionerId', provisioner_id,\n        'workerType', worker_type,\n        'expires', expires,\n        'lastDateActive', last_date_active,\n        'stability', stability),\n      'description', description::text) as value,\n    1 as version,\n    etag\n  from queue_worker_types;\n\n  revoke select, insert, update, delete on queue_worker_types from $db_user_prefix$_queue;\n  drop table queue_worker_types;\n  grant select, insert, update, delete on queue_worker_type_entities to $db_user_prefix$_queue;\nend\n",
      "methods": {
        "create_queue_worker_type": {
          "args": "provisioner_id_in text, worker_type_in text, expires_in timestamptz, last_date_active_in timestamptz, description_in text, stability_in text",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  insert\n    into queue_worker_types (provisioner_id, worker_type, expires, last_date_active, description, stability)\n    values (provisioner_id_in, worker_type_in, expires_in, last_date_active_in, description_in, stability_in);\n    return new_etag;\nend",
          "deprecated": false,
          "description": "Create a new queue worker type.  Raises UNIQUE_VIOLATION if the worker type already exists.",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "delete_queue_worker_type": {
          "args": "provisioner_id text, worker_type text, stability text, description text",
          "body": "begin\n  delete from queue_worker_types\n  where\n    queue_worker_types.provisioner_id = provisioner_id_in and\n    queue_worker_types.worker_type = worker_type_in;\nend",
          "deprecated": false,
          "description": "Delete a queue worker type.",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "expire_queue_worker_types": {
          "args": "expires_in timestamptz",
          "body": "declare\n  count integer;\nbegin\n  delete from queue_worker_types\n  where queue_worker_types.expires < expires_in;\n  if found then\n    get diagnostics count = row_count;\n    return count;\n  end if;\n  return 0;\nend",
          "deprecated": false,
          "description": "Expire queue worker types that come before `expires_in`.\nReturns a count of rows that have been deleted.",
          "mode": "write",
          "returns": "integer",
          "serviceName": "queue"
        },
        "get_queue_worker_type": {
          "args": "provisioner_id_in text, worker_type_in text, expires_in timestamptz",
          "body": "begin\n  return query\n  select\n    queue_worker_types.provisioner_id,\n    queue_worker_types.worker_type,\n    queue_worker_types.expires,\n    queue_worker_types.last_date_active,\n    queue_worker_types.description,\n    queue_worker_types.stability,\n    queue_worker_types.etag\n  from queue_worker_types\n  where\n    queue_worker_types.provisioner_id = provisioner_id_in and\n    queue_worker_types.worker_type = worker_type_in and\n    queue_worker_types.expires > expires_in;\n  end",
          "deprecated": false,
          "description": "Get a non-expired queue worker type by provisioner_id and worker_type.",
          "mode": "read",
          "returns": "table(provisioner_id text, worker_type text, expires timestamptz, last_date_active timestamptz, description text, stability text, etag uuid)",
          "serviceName": "queue"
        },
        "get_queue_worker_types": {
          "args": "provisioner_id_in text, worker_type_in text, expires_in timestamptz, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    queue_worker_types.provisioner_id,\n    queue_worker_types.worker_type,\n    queue_worker_types.expires,\n    queue_worker_types.last_date_active,\n    queue_worker_types.description,\n    queue_worker_types.stability,\n    queue_worker_types.etag\n  from queue_worker_types\n  where\n    (queue_worker_types.provisioner_id = provisioner_id_in or provisioner_id_in is null) and\n    (queue_worker_types.worker_type = worker_type_in or worker_type_in is null) and\n    (queue_worker_types.expires > expires_in or expires_in is null)\n  order by provisioner_id, worker_type\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get queue worker types ordered by `provisioner_id` and `worker_type`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(provisioner_id text, worker_type text, expires timestamptz, last_date_active timestamptz, description text, stability text, etag uuid)",
          "serviceName": "queue"
        },
        "queue_worker_type_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_row queue_worker_types%ROWTYPE;\nbegin\n  select\n    (properties ->> 'provisionerId')::text,\n    (properties ->> 'workerType')::text,\n    (properties ->> 'expires')::timestamptz,\n    (properties ->> 'lastDateActive')::timestamptz,\n    entity_buf_decode(properties, 'description')::text,\n    (properties ->> 'stability')::text,\n    public.gen_random_uuid()\n  into new_row;\n  if overwrite then\n    raise exception 'overwrite not implemented';\n  else\n    execute 'insert into queue_worker_types select $1.*' using new_row;\n  end if;\n  return new_row.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "queue_worker_type_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select\n    queue_worker_type_entities_load.partition_key,\n    queue_worker_type_entities_load.row_key,\n    entity_buf_encode(\n      jsonb_build_object(\n        'PartitionKey', encode_string_key(provisioner_id),\n        'RowKey', encode_string_key(worker_type),\n        'provisionerId', provisioner_id,\n        'workerType', worker_type,\n        'expires', expires,\n        'lastDateActive', last_date_active,\n        'stability', stability),\n      'description', description::text) as value,\n    1 as version,\n    queue_worker_types.etag as etag\n  from queue_worker_types\n  where\n    queue_worker_types.provisioner_id = decode_string_key(partition_key) and\n    queue_worker_types.worker_type = decode_string_key(row_key);\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_worker_type_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid;\nbegin\n  new_etag = public.gen_random_uuid();\n  update queue_worker_types\n  set (\n    expires,\n    last_date_active,\n    stability,\n    description,\n    etag\n  ) = (\n    (properties ->> 'expires')::timestamptz,\n    (properties ->> 'lastDateActive')::timestamptz,\n    (properties ->> 'stability')::text,\n    entity_buf_decode(properties, 'description')::text,\n    new_etag\n  )\n  where\n    queue_worker_types.provisioner_id = decode_string_key(partition_key) and\n    queue_worker_types.worker_type = decode_string_key(row_key) and\n    queue_worker_types.etag = queue_worker_type_entities_modify.old_etag;\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n  perform queue_worker_types.etag from queue_worker_types\n  where\n    queue_worker_types.provisioner_id = decode_string_key(partition_key) and\n    queue_worker_types.worker_type = decode_string_key(row_key);\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_worker_type_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from queue_worker_types\n  where\n    queue_worker_types.provisioner_id = decode_string_key(partition_key) and\n    queue_worker_types.worker_type = decode_string_key(row_key)\n  returning queue_worker_types.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_worker_type_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  cond text[];\n  exp_cond_operator text;\n  exp_cond_operand timestamptz;\nbegin\n  if not condition is null then\n    cond := regexp_split_to_array(condition, '\\s+');\n    exp_cond_operator := cond[4];\n    exp_cond_operand := cond[5] :: timestamptz;\n\n    return query select\n      encode_string_key(provisioner_id) as partition_key,\n      encode_string_key(worker_type) as row_key,\n      entity_buf_encode(\n        jsonb_build_object(\n          'PartitionKey', encode_string_key(provisioner_id),\n          'RowKey', encode_string_key(worker_type),\n          'provisionerId', provisioner_id,\n          'workerType', worker_type,\n          'expires', expires,\n          'lastDateActive', last_date_active,\n          'stability', stability),\n        'description', description::text) as value,\n      1 as version,\n      queue_worker_types.etag as etag\n    from queue_worker_types\n    where\n      (queue_worker_type_entities_scan.pk is null or decode_string_key(partition_key) = provisioner_id) and\n      (queue_worker_type_entities_scan.rk is null or decode_string_key(row_key) = worker_type) and\n      case\n        when exp_cond_operator = '=' then expires = exp_cond_operand\n        when exp_cond_operator = '<' then expires < exp_cond_operand\n        when exp_cond_operator = '<=' then expires <= exp_cond_operand\n        when exp_cond_operator = '>' then expires > exp_cond_operand\n        when exp_cond_operator = '>=' then expires >= exp_cond_operand\n        else expires <> exp_cond_operand\n      end\n    order by queue_worker_types.provisioner_id, queue_worker_types.worker_type\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (page is not null and page > 0) then page\n      else 0\n    end;\n  else\n    return query select\n      encode_string_key(provisioner_id) as partition_key,\n      encode_string_key(worker_type) as row_key,\n      entity_buf_encode(\n        jsonb_build_object(\n          'PartitionKey', encode_string_key(provisioner_id),\n          'RowKey', encode_string_key(worker_type),\n          'provisionerId', provisioner_id,\n          'workerType', worker_type,\n          'expires', expires,\n          'lastDateActive', last_date_active,\n          'stability', stability),\n        'description', description::text) as value,\n      1 as version,\n      queue_worker_types.etag as etag\n    from queue_worker_types\n    where\n      (queue_worker_type_entities_scan.pk is null or decode_string_key(queue_worker_type_entities_scan.pk) = provisioner_id) and\n      (queue_worker_type_entities_scan.rk is null or decode_string_key(queue_worker_type_entities_scan.rk) = worker_type)\n    order by queue_worker_types.provisioner_id, queue_worker_types.worker_type\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (size is not null and size > 0 and page is not null and page > 0) then page\n      else 0\n    end;\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "update_queue_worker_type": {
          "args": "provisioner_id_in text, worker_type_in text, expires_in timestamptz, last_date_active_in timestamptz, description_in text, stability_in text",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  return query update queue_worker_types\n  set\n    expires = expires_in,\n    last_date_active = last_date_active_in,\n    description = description_in,\n    stability = stability_in,\n    etag = new_etag\n  where\n    queue_worker_types.provisioner_id = provisioner_id_in and\n    queue_worker_types.worker_type = worker_type_in\n  returning\n    queue_worker_types.provisioner_id,\n    queue_worker_types.worker_type,\n    queue_worker_types.expires,\n    queue_worker_types.last_date_active,\n    queue_worker_types.description,\n    queue_worker_types.stability,\n    queue_worker_types.etag;\nend",
          "deprecated": false,
          "description": "Update a queue worker type's expires, last_date_active, description, and stability.\nAll parameters must be supplied.",
          "mode": "write",
          "returns": "table(provisioner_id text, worker_type text, expires timestamptz, last_date_active timestamptz, description text, stability text, etag uuid)",
          "serviceName": "queue"
        }
      },
      "migrationScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.  Note that this may lead to concurrent\n  -- updates failing; the important thing is that they not succeed without\n  -- taking effect.  Failed updates will be retried.\n  lock table queue_worker_type_entities;\n\n  create table queue_worker_types\n  as\n    select\n      (value ->> 'provisionerId')::text as provisioner_id,\n      (value ->> 'workerType')::text as worker_type,\n      (value ->> 'expires')::timestamptz as expires,\n      (value ->> 'lastDateActive')::timestamptz as last_date_active,\n      entity_buf_decode(value, 'description')::text as description,\n      (value ->> 'stability')::text as stability,\n      etag\n    from queue_worker_type_entities;\n  alter table queue_worker_types add primary key (provisioner_id, worker_type);\n  alter table queue_worker_types\n    alter column provisioner_id set not null,\n    alter column worker_type set not null,\n    alter column expires set not null,\n    alter column last_date_active set not null,\n    alter column description set not null,\n    alter column stability set not null,\n    alter column etag set not null,\n    alter column etag set default public.gen_random_uuid();\n\n  revoke select, insert, update, delete on queue_worker_type_entities from $db_user_prefix$_queue;\n  drop table queue_worker_type_entities;\n  grant select, insert, update, delete on queue_worker_types to $db_user_prefix$_queue;\nend\n",
      "version": 44
    },
    {
      "description": "queue provisioners phase 2",
      "downgradeScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.\n  lock table queue_provisioners;\n\n  create table queue_provisioner_entities(\n    partition_key text, row_key text,\n    value jsonb not null,\n    version integer not null,\n    etag uuid default public.gen_random_uuid());\n  alter table queue_provisioner_entities add primary key (partition_key, row_key);\n\n  insert into queue_provisioner_entities\n  select\n    encode_string_key(provisioner_id) as partition_key,\n    'provisioner' as row_key,\n    entity_buf_encode(\n      entity_buf_encode(\n        jsonb_build_object(\n          'PartitionKey', encode_string_key(provisioner_id),\n          'RowKey', 'provisioner',\n          'provisionerId', provisioner_id,\n          'expires', expires,\n          'lastDateActive', last_date_active,\n          'stability', stability),\n        'description', description::text),\n      'actions', actions::text) as value,\n    1 as version,\n    etag\n  from queue_provisioners;\n\n  revoke select, insert, update, delete on queue_provisioners from $db_user_prefix$_queue;\n  drop table queue_provisioners;\n  grant select, insert, update, delete on queue_provisioner_entities to $db_user_prefix$_queue;\nend\n",
      "methods": {
        "create_queue_provisioner": {
          "args": "provisioner_id_in text, expires_in timestamptz, last_date_active_in timestamptz, description_in text, stability_in text, actions_in jsonb",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  insert\n    into queue_provisioners (provisioner_id, expires, last_date_active, description, stability, actions)\n    values (provisioner_id_in, expires_in, last_date_active_in, description_in, stability_in, actions_in);\n    return new_etag;\nend",
          "deprecated": false,
          "description": "Create a new queue provisioner.  Raises UNIQUE_VIOLATION if the provisioner already exists.",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "delete_queue_provisioner": {
          "args": "provisioner_id text, stability text, description text",
          "body": "begin\n  delete from queue_provisioners\n  where\n    queue_provisioners.provisioner_id = provisioner_id_in;\nend",
          "deprecated": false,
          "description": "Delete a queue provisioner.",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "expire_queue_provisioners": {
          "args": "expires_in timestamptz",
          "body": "declare\n  count integer;\nbegin\n  delete from queue_provisioners\n  where queue_provisioners.expires < expires_in;\n  if found then\n    get diagnostics count = row_count;\n    return count;\n  end if;\n  return 0;\nend",
          "deprecated": false,
          "description": "Expire provisioners that come before `expires_in`.\nReturns a count of rows that have been deleted.",
          "mode": "write",
          "returns": "integer",
          "serviceName": "queue"
        },
        "get_queue_provisioner": {
          "args": "provisioner_id_in text, expires_in timestamptz",
          "body": "begin\n  return query\n  select\n    queue_provisioners.provisioner_id,\n    queue_provisioners.expires,\n    queue_provisioners.last_date_active,\n    queue_provisioners.description,\n    queue_provisioners.stability,\n    queue_provisioners.actions,\n    queue_provisioners.etag\n  from queue_provisioners\n  where\n    queue_provisioners.provisioner_id = provisioner_id_in and\n    queue_provisioners.expires > expires_in;\n  end",
          "deprecated": false,
          "description": "Get a queue provisioner by provisioner_id.",
          "mode": "read",
          "returns": "table(provisioner_id text, expires timestamptz, last_date_active timestamptz, description text, stability text, actions jsonb, etag uuid)",
          "serviceName": "queue"
        },
        "get_queue_provisioners": {
          "args": "expires_in timestamptz, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    queue_provisioners.provisioner_id,\n    queue_provisioners.expires,\n    queue_provisioners.last_date_active,\n    queue_provisioners.description,\n    queue_provisioners.stability,\n    queue_provisioners.actions,\n    queue_provisioners.etag\n  from queue_provisioners\n  where (queue_provisioners.expires > expires_in or expires_in is null)\n  order by provisioner_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get queue provisioners ordered by `provisioner_id`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(provisioner_id text, expires timestamptz, last_date_active timestamptz, description text, stability text, actions jsonb, etag uuid)",
          "serviceName": "queue"
        },
        "queue_provisioner_entities_create": {
          "args": "pk text, rk text, properties jsonb, overwrite boolean, version integer",
          "body": "declare\n  new_row queue_provisioners%ROWTYPE;\nbegin\n  select\n    (properties ->> 'provisionerId')::text,\n    (properties ->> 'expires')::timestamptz,\n    (properties ->> 'lastDateActive')::timestamptz,\n    entity_buf_decode(properties, 'description')::text,\n    (properties ->> 'stability')::text,\n    entity_buf_decode(properties, 'actions')::jsonb,\n    public.gen_random_uuid()\n  into new_row;\n  if overwrite then\n    raise exception 'overwrite not implemented';\n  else\n    execute 'insert into queue_provisioners select $1.*' using new_row;\n  end if;\n  return new_row.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "queue_provisioner_entities_load": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query\n  select\n    queue_provisioner_entities_load.partition_key,\n    queue_provisioner_entities_load.row_key,\n    entity_buf_encode(\n      entity_buf_encode(\n        jsonb_build_object(\n          'PartitionKey', encode_string_key(provisioner_id),\n          'RowKey', 'provisioner',\n          'provisionerId', provisioner_id,\n          'expires', expires,\n          'lastDateActive', last_date_active,\n          'stability', stability),\n        'description', description::text),\n      'actions', actions::text) as value,\n    1 as version,\n    queue_provisioners.etag as etag\n  from queue_provisioners\n  where\n    queue_provisioners.provisioner_id = decode_string_key(partition_key);\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key_out text, row_key_out text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "queue_provisioner_entities_modify": {
          "args": "partition_key text, row_key text, properties jsonb, version integer, old_etag uuid",
          "body": "declare\n  new_etag uuid;\nbegin\n  new_etag = public.gen_random_uuid();\n  update queue_provisioners\n  set (\n    expires,\n    stability,\n    description,\n    actions,\n    etag\n  ) = (\n    (properties ->> 'expires')::timestamptz,\n    (properties ->> 'stability')::text,\n    entity_buf_decode(properties, 'description')::text,\n    entity_buf_decode(properties, 'actions')::jsonb,\n    new_etag\n  )\n  where\n    queue_provisioners.provisioner_id = decode_string_key(partition_key) and\n    queue_provisioners.etag = queue_provisioner_entities_modify.old_etag;\n  if found then\n    return query select new_etag;\n    return;\n  end if;\n  perform queue_provisioners.etag from queue_provisioners\n  where\n    queue_provisioners.provisioner_id = decode_string_key(partition_key);\n  if found then\n    raise exception 'unsuccessful update' using errcode = 'P0004';\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_provisioner_entities_remove": {
          "args": "partition_key text, row_key text",
          "body": "begin\n  return query delete from queue_provisioners\n  where\n    queue_provisioners.provisioner_id = decode_string_key(partition_key)\n  returning queue_provisioners.etag;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "write",
          "returns": "table (etag uuid)",
          "serviceName": "queue"
        },
        "queue_provisioner_entities_scan": {
          "args": "pk text, rk text, condition text, size integer, page integer",
          "body": "declare\n  cond text[];\n  exp_cond_operator text;\n  exp_cond_operand timestamptz;\nbegin\n  if not condition is null then\n    cond := regexp_split_to_array(condition, '\\s+');\n    exp_cond_operator := cond[4];\n    exp_cond_operand := cond[5] :: timestamptz;\n\n    return query select\n      encode_string_key(provisioner_id) as partition_key,\n      'provisioner' as row_key,\n      entity_buf_encode(\n        entity_buf_encode(\n          jsonb_build_object(\n            'PartitionKey', encode_string_key(provisioner_id),\n            'RowKey', 'provisioner',\n            'provisionerId', provisioner_id,\n            'expires', expires,\n            'lastDateActive', last_date_active,\n            'stability', stability),\n          'description', description::text),\n        'actions', actions::text) as value,\n      1 as version,\n      queue_provisioners.etag as etag\n    from queue_provisioners\n    where\n      (queue_provisioner_entities_scan.pk is null or decode_string_key(partition_key) = provisioner_id) and\n      case\n        when exp_cond_operator = '=' then expires = exp_cond_operand\n        when exp_cond_operator = '<' then expires < exp_cond_operand\n        when exp_cond_operator = '<=' then expires <= exp_cond_operand\n        when exp_cond_operator = '>' then expires > exp_cond_operand\n        when exp_cond_operator = '>=' then expires >= exp_cond_operand\n        else expires <> exp_cond_operand\n      end\n    order by queue_provisioners.provisioner_id\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (page is not null and page > 0) then page\n      else 0\n    end;\n  else\n    return query select\n      encode_string_key(provisioner_id) as partition_key,\n      'provisioner' as row_key,\n      entity_buf_encode(\n        entity_buf_encode(\n          jsonb_build_object(\n            'PartitionKey', encode_string_key(provisioner_id),\n            'RowKey', 'provisioner',\n            'provisionerId', provisioner_id,\n            'expires', expires,\n            'lastDateActive', last_date_active,\n            'stability', stability),\n          'description', description::text),\n        'actions', actions::text) as value,\n      1 as version,\n      queue_provisioners.etag as etag\n    from queue_provisioners\n    where\n      (queue_provisioner_entities_scan.pk is null or decode_string_key(partition_key) = provisioner_id)\n    order by queue_provisioners.provisioner_id\n    limit case\n      when (size is not null and size > 0) then size + 1\n      else null\n    end\n    offset case\n      when (size is not null and size > 0 and page is not null and page > 0) then page\n      else 0\n    end;\n  end if;\nend",
          "deprecated": true,
          "description": "See taskcluster-lib-entities",
          "mode": "read",
          "returns": "table (partition_key text, row_key text, value jsonb, version integer, etag uuid)",
          "serviceName": "queue"
        },
        "update_queue_provisioner": {
          "args": "provisioner_id_in text, expires_in timestamptz, last_date_active_in timestamptz, description_in text, stability_in text, actions_in jsonb",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  return query update queue_provisioners\n  set\n    expires = expires_in,\n    last_date_active = last_date_active_in,\n    description = description_in,\n    stability = stability_in,\n    actions = actions_in,\n    etag = new_etag\n  where\n    queue_provisioners.provisioner_id = provisioner_id_in\n  returning\n    queue_provisioners.provisioner_id,\n    queue_provisioners.expires,\n    queue_provisioners.last_date_active,\n    queue_provisioners.description,\n    queue_provisioners.stability,\n    queue_provisioners.actions,\n    queue_provisioners.etag;\nend",
          "deprecated": false,
          "description": "Update a queue provisioner's expires, last_date_active, description, stability, and actions.\nAll parameters must be supplied.",
          "mode": "write",
          "returns": "table(provisioner_id text, expires timestamptz, last_date_active timestamptz, description text, stability text, actions jsonb, etag uuid)",
          "serviceName": "queue"
        }
      },
      "migrationScript": "begin\n  -- lock this table before reading from it, to prevent loss of concurrent\n  -- updates when the table is dropped.  Note that this may lead to concurrent\n  -- updates failing; the important thing is that they not succeed without\n  -- taking effect.  Failed updates will be retried.\n  lock table queue_provisioner_entities;\n\n  create table queue_provisioners\n  as\n    select\n      (value ->> 'provisionerId')::text as provisioner_id,\n      (value ->> 'expires')::timestamptz as expires,\n      (value ->> 'lastDateActive')::timestamptz as last_date_active,\n      entity_buf_decode(value, 'description')::text as description,\n      (value ->> 'stability')::text as stability,\n      entity_buf_decode(value, 'actions')::jsonb as actions,\n      etag\n    from queue_provisioner_entities;\n  alter table queue_provisioners add primary key (provisioner_id);\n  alter table queue_provisioners\n    alter column provisioner_id set not null,\n    alter column expires set not null,\n    alter column last_date_active set not null,\n    alter column description set not null,\n    alter column stability set not null,\n    alter column actions set not null,\n    alter column etag set not null,\n    alter column etag set default public.gen_random_uuid();\n\n  revoke select, insert, update, delete on queue_provisioner_entities from $db_user_prefix$_queue;\n  drop table queue_provisioner_entities;\n  grant select, insert, update, delete on queue_provisioners to $db_user_prefix$_queue;\nend\n",
      "version": 45
    },
    {
      "description": "fix get_dependent_tasks to return tasks in order",
      "methods": {
        "get_dependent_tasks": {
          "args": "required_task_id_in text, satisfied_in boolean, tasks_after_in text, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    task_dependencies.dependent_task_id,\n    task_dependencies.requires,\n    task_dependencies.satisfied\n  from task_dependencies\n  where\n    required_task_id = required_task_id_in and\n    expires > now() and\n    (satisfied_in is null or task_dependencies.satisfied = satisfied_in) and\n    (tasks_after_in is null or task_dependencies.dependent_task_id > tasks_after_in)\n  order by dependent_task_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get the un-expired tasks that depend on this one, limiting to only (un)satisfied\ndependencies if `satisfied_in` is not null.\n\nOnly dependencies with `dependent_task_id > tasks_after_in` are returned.\nThis supports paginated queries that are not susceptible to rows being\nadded or removed.  Typically only one of `page_offset_in` and\n`tasks_after_in` are non-null.",
          "mode": "read",
          "returns": "table(dependent_task_id text, requires task_requires, satisfied boolean)",
          "serviceName": "queue"
        }
      },
      "version": 46
    },
    {
      "description": "add get_non_stopped_workers db function",
      "methods": {
        "get_non_stopped_workers": {
          "args": "worker_pool_id_in text, worker_group_in text, worker_id_in text, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    workers.worker_pool_id,\n    workers.worker_group,\n    workers.worker_id,\n    workers.provider_id,\n    workers.created,\n    workers.expires,\n    workers.state,\n    workers.provider_data,\n    workers.capacity,\n    workers.last_modified,\n    workers.last_checked\n  from workers\n  where\n    (workers.worker_pool_id = worker_pool_id_in or worker_pool_id_in is null) and\n    (workers.worker_group = worker_group_in or worker_group_in is null) and\n    (workers.worker_id = worker_id_in or worker_id_in is null) and\n    (workers.state <> 'stopped')\n  order by worker_pool_id, worker_group, worker_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get non-stopped workers filtered by the optional arguments,\nordered by `worker_pool_id`, `worker_group`, and  `worker_id`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset `page_offset`.",
          "mode": "read",
          "returns": "table(worker_pool_id text, worker_group text, worker_id text, provider_id text, created timestamptz, expires timestamptz, state text, provider_data jsonb, capacity integer, last_modified timestamptz, last_checked timestamptz)",
          "serviceName": "worker_manager"
        }
      },
      "version": 47
    },
    {
      "description": "Add an index on state in the workers table",
      "downgradeScript": "begin\n  drop index workers_state_idx;\nend",
      "methods": {
        "get_workers_without_provider_data": {
          "args": "worker_pool_id_in text, worker_group_in text, worker_id_in text, state_in text, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    workers.worker_pool_id,\n    workers.worker_group,\n    workers.worker_id,\n    workers.provider_id,\n    workers.created,\n    workers.expires,\n    workers.state,\n    workers.capacity,\n    workers.last_modified,\n    workers.last_checked\n  from workers\n  where\n    (workers.worker_pool_id = worker_pool_id_in or worker_pool_id_in is null) and\n    (workers.worker_group = worker_group_in or worker_group_in is null) and\n    (workers.worker_id = worker_id_in or worker_id_in is null) and\n    (workers.state = state_in or state_in is null)\n  order by worker_pool_id, worker_group, worker_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get existing workers (without their `provider_data`) filtered by the optional arguments,\nordered by `worker_pool_id`, `worker_group`, and  `worker_id`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(worker_pool_id text, worker_group text, worker_id text, provider_id text, created timestamptz, expires timestamptz, state text, capacity integer, last_modified timestamptz, last_checked timestamptz)",
          "serviceName": "worker_manager"
        }
      },
      "migrationScript": "begin\n  create index workers_state_idx on workers (state);\nend",
      "version": 48
    },
    {
      "description": "Remove unnecessary etag columns from DB tables",
      "downgradeScript": "begin\n  alter table clients add column etag uuid not null default public.gen_random_uuid();\n  alter table github_builds add column etag uuid not null default public.gen_random_uuid();\n  alter table hooks add column etag uuid not null default public.gen_random_uuid();\n  alter table hooks_last_fires add column etag uuid not null default public.gen_random_uuid();\n  alter table hooks_queues add column etag uuid not null default public.gen_random_uuid();\n  alter table indexed_tasks add column etag uuid not null default public.gen_random_uuid();\n  alter table denylisted_notifications add column etag uuid not null default public.gen_random_uuid();\n  alter table cache_purges add column etag uuid not null default public.gen_random_uuid();\n  alter table access_tokens add column etag uuid not null default public.gen_random_uuid();\n  alter table authorization_codes add column etag uuid not null default public.gen_random_uuid();\n  alter table sessions add column etag uuid not null default public.gen_random_uuid();\n  alter table queue_provisioners add column etag uuid not null default public.gen_random_uuid();\n  alter table queue_worker_types add column etag uuid not null default public.gen_random_uuid();\n  alter table queue_workers add column etag uuid not null default public.gen_random_uuid();\n  alter table task_groups add column etag uuid not null default public.gen_random_uuid();\n  alter table task_dependencies add column etag uuid not null default public.gen_random_uuid();\n  alter table queue_artifacts add column etag uuid not null default public.gen_random_uuid();\n  alter table tasks add column etag uuid not null default public.gen_random_uuid();\n  alter table worker_pool_errors add column etag uuid not null default public.gen_random_uuid();\n  alter table worker_pools add column etag uuid not null default public.gen_random_uuid();\n  alter table index_namespaces add column etag uuid not null default public.gen_random_uuid();\nend",
      "methods": {
        "add_denylist_address": {
          "args": "notification_type_in text, notification_address_in text",
          "body": "begin\n  insert into denylisted_notifications(notification_type, notification_address)\n  values (\n    notification_type_in,\n    notification_address_in\n  ) on conflict do nothing;\nend",
          "deprecated": false,
          "description": "If the denylist address already exists, this is a no-op. Otherwise, add the denylist\naddress for the taskcluster-notify service, with a new random etag.",
          "mode": "write",
          "returns": "void",
          "serviceName": "notify"
        },
        "add_task_dependency": {
          "args": "dependent_task_id_in text, required_task_id_in text, requires_in task_requires, expires_in timestamptz",
          "body": "begin\n  insert\n  into task_dependencies (dependent_task_id, required_task_id, requires, satisfied, expires)\n  values (\n    dependent_task_id_in,\n    required_task_id_in,\n    requires_in,\n    false,\n    expires_in\n  )\n  on conflict do nothing;\nend",
          "deprecated": false,
          "description": "Create an un-satisfied task dependency between the two tasks, with the given\nrequirement style and expiration. If the dependency already exists, nothing\nhappens.",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "create_hooks_queue": {
          "args": "hook_group_id_in text, hook_id_in text, queue_name_in text, bindings_in jsonb",
          "body": "begin\n  insert\n    into hooks_queues (hook_group_id, hook_id, queue_name, bindings)\n    values (hook_group_id_in, hook_id_in, queue_name_in, bindings_in);\n\n    return public.gen_random_uuid();\nend",
          "deprecated": false,
          "description": "Create a new hooks queue. Raises UNIQUE_VIOLATION if the hook already exists.",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "hooks"
        },
        "create_last_fire": {
          "args": "hook_group_id_in text, hook_id_in text, fired_by_in text, task_id_in text, task_create_time_in timestamptz, result_in text, error_in text",
          "body": "begin\n  insert\n    into hooks_last_fires (hook_group_id, hook_id, fired_by, task_id, task_create_time, result, error)\n    values (hook_group_id_in, hook_id_in, fired_by_in, task_id_in, task_create_time_in, result_in, error_in);\n\n    return public.gen_random_uuid();\nend",
          "deprecated": false,
          "description": "Create a new hook last fire. Raises UNIQUE_VIOLATION if the hook already exists.",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "hooks"
        },
        "create_queue_provisioner": {
          "args": "provisioner_id_in text, expires_in timestamptz, last_date_active_in timestamptz, description_in text, stability_in text, actions_in jsonb",
          "body": "begin\n  insert\n    into queue_provisioners (provisioner_id, expires, last_date_active, description, stability, actions)\n    values (provisioner_id_in, expires_in, last_date_active_in, description_in, stability_in, actions_in);\n    return public.gen_random_uuid();\nend",
          "deprecated": false,
          "description": "Create a new queue provisioner.  Raises UNIQUE_VIOLATION if the provisioner already exists.",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "create_queue_worker": {
          "args": "provisioner_id_in text, worker_type_in text, worker_group_in text, worker_id_in text, quarantine_until_in timestamptz, expires_in timestamptz, first_claim_in timestamptz, recent_tasks_in jsonb",
          "body": "begin\n  insert\n    into queue_workers (provisioner_id, worker_type, worker_group, worker_id, quarantine_until, expires, first_claim, recent_tasks)\n    values (provisioner_id_in, worker_type_in, worker_group_in, worker_id_in, quarantine_until_in, expires_in, first_claim_in, recent_tasks_in);\n    return public.gen_random_uuid();\nend",
          "deprecated": false,
          "description": "Create a new queue worker.  Raises UNIQUE_VIOLATION if the worker already exists.",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "create_queue_worker_type": {
          "args": "provisioner_id_in text, worker_type_in text, expires_in timestamptz, last_date_active_in timestamptz, description_in text, stability_in text",
          "body": "begin\n  insert\n    into queue_worker_types (provisioner_id, worker_type, expires, last_date_active, description, stability)\n    values (provisioner_id_in, worker_type_in, expires_in, last_date_active_in, description_in, stability_in);\n    return public.gen_random_uuid();\nend",
          "deprecated": false,
          "description": "Create a new queue worker type. Raises UNIQUE_VIOLATION if the worker type already exists.",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "create_task": {
          "args": "task_id text,\nprovisioner_id text,\nworker_type text,\nscheduler_id text,\ntask_group_id text,\ndependencies jsonb,\nrequires task_requires,\nroutes jsonb,\npriority task_priority,\nretries integer,\ncreated timestamptz,\ndeadline timestamptz,\nexpires timestamptz,\nscopes jsonb,\npayload jsonb,\nmetadata jsonb,\ntags jsonb,\nextra jsonb",
          "body": "begin\n  insert\n  into tasks (\n    task_id,\n    provisioner_id,\n    worker_type,\n    scheduler_id,\n    task_group_id,\n    dependencies,\n    requires,\n    routes,\n    priority,\n    retries,\n    created,\n    deadline,\n    expires,\n    scopes,\n    payload,\n    metadata,\n    tags,\n    extra,\n    retries_left,\n    runs,\n    taken_until,\n    ever_resolved\n  )\n  values (\n    task_id,\n    provisioner_id,\n    worker_type,\n    scheduler_id,\n    task_group_id,\n    dependencies,\n    requires,\n    routes,\n    priority,\n    retries,\n    created,\n    deadline,\n    expires,\n    scopes,\n    payload,\n    metadata,\n    tags,\n    extra,\n    -- default values for the mutable bits\n    retries,\n    jsonb_build_array(),\n    null, -- not taken\n    false\n  );\nend",
          "deprecated": false,
          "description": "Create a new task, without scheduling it, and with empty values\nfor the status information.",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "delete_hooks_queue": {
          "args": "hook_group_id_in text, hook_id_in text",
          "body": "begin\n  delete from hooks_queues\n  where\n    hooks_queues.hook_group_id = hook_group_id_in and\n    hooks_queues.hook_id = hook_id_in;\nend",
          "deprecated": false,
          "description": "Delete a hooks queue.",
          "mode": "write",
          "returns": "void",
          "serviceName": "hooks"
        },
        "ensure_task_group": {
          "args": "task_group_id_in text,\nscheduler_id_in text,\nexpires_in timestamptz",
          "body": "declare\n  task_group task_groups%ROWTYPE;\nbegin\n  select *\n  from task_groups\n  where task_groups.task_group_id = task_group_id_in\n  for update\n  into task_group;\n\n  -- insert with expiration one hour later than given\n  if task_group.task_group_id is NULL then\n    begin\n      insert\n      into task_groups (task_group_id, scheduler_id, expires)\n      values (\n        task_group_id_in,\n        scheduler_id_in,\n        expires_in + interval '1 hour'\n      );\n      return;\n    exception\n      when unique_violation then\n        -- we raced with another call's insert, so get that inserted row\n        select *\n        from task_groups\n        where task_groups.task_group_id = task_group_id_in\n        for update\n        into task_group;\n    end;\n  end if;\n\n  if task_group.scheduler_id != scheduler_id_in then\n    raise exception 'task group exists with different scheduler_id'\n      using errcode = '23505';\n  end if;\n\n  -- if necessary, update the expires value\n  if expires_in > task_group.expires then\n    update task_groups\n    set expires = expires_in + interval '1 hour'\n    where task_groups.task_group_id = task_group_id_in;\n  end if;\nend",
          "deprecated": false,
          "description": "Ensure that the given task group exists, has the matching scheduler_id,\nand has an expiration greater than the given expiration.  Expiration is\nbumped by an hour at a time to avoid unnecessary updates.  This returns\n23505 (UNIQUE_VIOLATION) when the group exists with a different\nscheduler_id.",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "get_github_build": {
          "args": "task_group_id_in text",
          "body": "begin\n  return query\n  select\n    github_builds.organization,\n    github_builds.repository,\n    github_builds.sha,\n    github_builds.task_group_id,\n    github_builds.state,\n    github_builds.created,\n    github_builds.updated,\n    github_builds.installation_id,\n    github_builds.event_type,\n    github_builds.event_id,\n    public.gen_random_uuid()\n  from github_builds\n  where github_builds.task_group_id = task_group_id_in;\nend",
          "deprecated": false,
          "description": "Get a github build. The returned table will have one or zero rows.",
          "mode": "read",
          "returns": "table (organization text, repository text, sha text, task_group_id text, state text, created timestamptz, updated timestamptz, installation_id integer, event_type text, event_id text, etag uuid)",
          "serviceName": "github"
        },
        "get_github_builds": {
          "args": "page_size_in integer, page_offset_in integer, organization_in text, repository_in text, sha_in text",
          "body": "begin\n  return query\n  select\n    github_builds.organization,\n    github_builds.repository,\n    github_builds.sha,\n    github_builds.task_group_id,\n    github_builds.state,\n    github_builds.created,\n    github_builds.updated,\n    github_builds.installation_id,\n    github_builds.event_type,\n    github_builds.event_id,\n    public.gen_random_uuid()\n  from github_builds\n  where\n    (organization_in is null or github_builds.organization = organization_in) and\n    (repository_in is null or github_builds.repository = repository_in) and\n    (sha_in is null or github_builds.sha = sha_in)\n  order by github_builds.updated asc\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get github builds.",
          "mode": "read",
          "returns": "table (organization text, repository text, sha text, task_group_id text, state text, created timestamptz, updated timestamptz, installation_id integer, event_type text, event_id text, etag uuid)",
          "serviceName": "github"
        },
        "get_hooks_queues": {
          "args": "page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    hooks_queues.hook_group_id,\n    hooks_queues.hook_id,\n    hooks_queues.queue_name,\n    hooks_queues.bindings,\n    public.gen_random_uuid()\n  from hooks_queues\n  order by hook_group_id, hook_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get hooks queues ordered by `hook_group_id` and `hook_id`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(hook_group_id text, hook_id text, queue_name text, bindings jsonb, etag uuid)",
          "serviceName": "hooks"
        },
        "get_last_fire": {
          "args": "hook_group_id_in text, hook_id_in text, task_id_in text",
          "body": "begin\n  return query\n  select\n    hooks_last_fires.hook_group_id,\n    hooks_last_fires.hook_id,\n    hooks_last_fires.fired_by,\n    hooks_last_fires.task_id,\n    hooks_last_fires.task_create_time,\n    hooks_last_fires.result,\n    hooks_last_fires.error,\n    public.gen_random_uuid()\n  from hooks_last_fires\n  where\n    hooks_last_fires.hook_group_id = hook_group_id_in and\n    hooks_last_fires.hook_id = hook_id_in and\n    hooks_last_fires.task_id = task_id_in;\nend",
          "deprecated": false,
          "description": "Get a hook last fire.",
          "mode": "read",
          "returns": "table(hook_group_id text, hook_id text, fired_by text, task_id text, task_create_time timestamptz, result text, error text, etag uuid)",
          "serviceName": "hooks"
        },
        "get_last_fires": {
          "args": "hook_group_id_in text, hook_id_in text, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    hooks_last_fires.hook_group_id,\n    hooks_last_fires.hook_id,\n    hooks_last_fires.fired_by,\n    hooks_last_fires.task_id,\n    hooks_last_fires.task_create_time,\n    hooks_last_fires.result,\n    hooks_last_fires.error,\n    public.gen_random_uuid()\n  from hooks_last_fires\n  where\n    hooks_last_fires.hook_group_id = hook_group_id_in and\n    hooks_last_fires.hook_id = hook_id_in\n  order by hook_group_id, hook_id, task_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get hooks last fires filtered by the `hook_group_id` and `hook_id` arguments,\nordered by `hook_group_id`, `hook_id`, and  `worker_id`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(hook_group_id text, hook_id text, fired_by text, task_id text, task_create_time timestamptz, result text, error text, etag uuid)",
          "serviceName": "hooks"
        },
        "get_queue_provisioner": {
          "args": "provisioner_id_in text, expires_in timestamptz",
          "body": "begin\n  return query\n  select\n    queue_provisioners.provisioner_id,\n    queue_provisioners.expires,\n    queue_provisioners.last_date_active,\n    queue_provisioners.description,\n    queue_provisioners.stability,\n    queue_provisioners.actions,\n    public.gen_random_uuid()\n  from queue_provisioners\n  where\n    queue_provisioners.provisioner_id = provisioner_id_in and\n    queue_provisioners.expires > expires_in;\n  end",
          "deprecated": false,
          "description": "Get a queue provisioner by provisioner_id.",
          "mode": "read",
          "returns": "table(provisioner_id text, expires timestamptz, last_date_active timestamptz, description text, stability text, actions jsonb, etag uuid)",
          "serviceName": "queue"
        },
        "get_queue_provisioners": {
          "args": "expires_in timestamptz, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    queue_provisioners.provisioner_id,\n    queue_provisioners.expires,\n    queue_provisioners.last_date_active,\n    queue_provisioners.description,\n    queue_provisioners.stability,\n    queue_provisioners.actions,\n    public.gen_random_uuid()\n  from queue_provisioners\n  where (queue_provisioners.expires > expires_in or expires_in is null)\n  order by provisioner_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get queue provisioners ordered by `provisioner_id`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(provisioner_id text, expires timestamptz, last_date_active timestamptz, description text, stability text, actions jsonb, etag uuid)",
          "serviceName": "queue"
        },
        "get_queue_worker": {
          "args": "provisioner_id_in text, worker_type_in text, worker_group_in text, worker_id_in text, expires_in timestamptz",
          "body": "begin\n  return query\n  select\n    queue_workers.provisioner_id,\n    queue_workers.worker_type,\n    queue_workers.worker_group,\n    queue_workers.worker_id,\n    queue_workers.quarantine_until,\n    queue_workers.expires,\n    queue_workers.first_claim,\n    queue_workers.recent_tasks,\n    public.gen_random_uuid()\n  from queue_workers\n  where\n    queue_workers.provisioner_id = provisioner_id_in and\n    queue_workers.worker_type = worker_type_in and\n    queue_workers.worker_group = worker_group_in and\n    queue_workers.worker_id = worker_id_in and\n    (queue_workers.expires > expires_in or queue_workers.quarantine_until > expires_in);\n  end",
          "deprecated": false,
          "description": "Get a non-expired queue worker by provisioner_id, worker_type, worker_group, and worker_id.\nWorkers are not considered expired until after their quarantine date expires.",
          "mode": "read",
          "returns": "table(provisioner_id text, worker_type text, worker_group text, worker_id text, quarantine_until timestamptz, expires timestamptz, first_claim timestamptz, recent_tasks jsonb, etag uuid)",
          "serviceName": "queue"
        },
        "get_queue_worker_type": {
          "args": "provisioner_id_in text, worker_type_in text, expires_in timestamptz",
          "body": "begin\n  return query\n  select\n    queue_worker_types.provisioner_id,\n    queue_worker_types.worker_type,\n    queue_worker_types.expires,\n    queue_worker_types.last_date_active,\n    queue_worker_types.description,\n    queue_worker_types.stability,\n    public.gen_random_uuid()\n  from queue_worker_types\n  where\n    queue_worker_types.provisioner_id = provisioner_id_in and\n    queue_worker_types.worker_type = worker_type_in and\n    queue_worker_types.expires > expires_in;\n  end",
          "deprecated": false,
          "description": "Get a non-expired queue worker type by provisioner_id and worker_type.",
          "mode": "read",
          "returns": "table(provisioner_id text, worker_type text, expires timestamptz, last_date_active timestamptz, description text, stability text, etag uuid)",
          "serviceName": "queue"
        },
        "get_queue_worker_types": {
          "args": "provisioner_id_in text, worker_type_in text, expires_in timestamptz, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    queue_worker_types.provisioner_id,\n    queue_worker_types.worker_type,\n    queue_worker_types.expires,\n    queue_worker_types.last_date_active,\n    queue_worker_types.description,\n    queue_worker_types.stability,\n    public.gen_random_uuid()\n  from queue_worker_types\n  where\n    (queue_worker_types.provisioner_id = provisioner_id_in or provisioner_id_in is null) and\n    (queue_worker_types.worker_type = worker_type_in or worker_type_in is null) and\n    (queue_worker_types.expires > expires_in or expires_in is null)\n  order by provisioner_id, worker_type\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get queue worker types ordered by `provisioner_id` and `worker_type`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(provisioner_id text, worker_type text, expires timestamptz, last_date_active timestamptz, description text, stability text, etag uuid)",
          "serviceName": "queue"
        },
        "get_queue_workers": {
          "args": "provisioner_id_in text, worker_type_in text, expires_in timestamptz, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    queue_workers.provisioner_id,\n    queue_workers.worker_type,\n    queue_workers.worker_group,\n    queue_workers.worker_id,\n    queue_workers.quarantine_until,\n    queue_workers.expires,\n    queue_workers.first_claim,\n    queue_workers.recent_tasks,\n    public.gen_random_uuid()\n  from queue_workers\n  where\n    (queue_workers.provisioner_id = provisioner_id_in or get_queue_workers.provisioner_id_in is null) and\n    (queue_workers.worker_type = worker_type_in or get_queue_workers.worker_type_in is null) and\n    ((queue_workers.expires > expires_in and queue_workers.quarantine_until < expires_in) or get_queue_workers.expires_in is null)\n  order by provisioner_id, worker_type, worker_group, worker_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get non-expired queue workers ordered by provisioner_id, worker_type, worker_group, and worker_id.\nWorkers are not considered expired until after their quarantine date expires.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(provisioner_id text, worker_type text, worker_group text, worker_id text, quarantine_until timestamptz, expires timestamptz, first_claim timestamptz, recent_tasks jsonb, etag uuid)",
          "serviceName": "queue"
        },
        "purge_cache": {
          "args": "provisioner_id_in text, worker_type_in text, cache_name_in text, before_in timestamptz, expires_in timestamptz",
          "body": "begin\n  insert into cache_purges(provisioner_id, worker_type, cache_name, before, expires)\n  values (\n    provisioner_id_in,\n    worker_type_in,\n    cache_name_in,\n    before_in,\n    expires_in\n  ) on conflict (provisioner_id, worker_type, cache_name) do\n  update\n  set (before, expires) = (before_in, expires_in)\n  where cache_purges.provisioner_id = provisioner_id_in and cache_purges.worker_type = worker_type_in and cache_purges.cache_name = cache_name_in;\nend",
          "deprecated": false,
          "description": "Publish a request to purge caches with name `cache_name_in`\non `provisioner_id_in`/`worker_type_in` workers.",
          "mode": "write",
          "returns": "void",
          "serviceName": "purge_cache"
        },
        "set_github_build_state": {
          "args": "task_group_id_in text, state_in text",
          "body": "begin\n  update github_builds\n  set (state, updated) = (\n    state_in,\n    now()\n  ) where github_builds.task_group_id = task_group_id_in;\n  if not found then\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "Only update the state of a build and update the `updated` timestamp",
          "mode": "write",
          "returns": "void",
          "serviceName": "github"
        },
        "update_hooks_queue_bindings": {
          "args": "hook_group_id_in text, hook_id_in text, bindings_in jsonb",
          "body": "begin\n  return query update hooks_queues\n  set\n    bindings = bindings_in\n  where\n    hooks_queues.hook_group_id = hook_group_id_in and\n    hooks_queues.hook_id = hook_id_in\n  returning\n    hooks_queues.hook_group_id,\n    hooks_queues.hook_id,\n    hooks_queues.queue_name,\n    hooks_queues.bindings,\n    public.gen_random_uuid();\nend",
          "deprecated": false,
          "description": "Update bindings of a hooks queue. If no such queue exists,\nthe return value is an empty set.",
          "mode": "write",
          "returns": "table(hook_group_id text, hook_id text, queue_name text, bindings jsonb, etag uuid)",
          "serviceName": "hooks"
        },
        "update_queue_provisioner": {
          "args": "provisioner_id_in text, expires_in timestamptz, last_date_active_in timestamptz, description_in text, stability_in text, actions_in jsonb",
          "body": "declare\n  new_etag uuid := public.gen_random_uuid();\nbegin\n  return query update queue_provisioners\n  set\n    expires = expires_in,\n    last_date_active = last_date_active_in,\n    description = description_in,\n    stability = stability_in,\n    actions = actions_in\n  where\n    queue_provisioners.provisioner_id = provisioner_id_in\n  returning\n    queue_provisioners.provisioner_id,\n    queue_provisioners.expires,\n    queue_provisioners.last_date_active,\n    queue_provisioners.description,\n    queue_provisioners.stability,\n    queue_provisioners.actions,\n    public.gen_random_uuid();\nend",
          "deprecated": false,
          "description": "Update a queue provisioner's expires, last_date_active, description, stability, and actions.\nAll parameters must be supplied.",
          "mode": "write",
          "returns": "table(provisioner_id text, expires timestamptz, last_date_active timestamptz, description text, stability text, actions jsonb, etag uuid)",
          "serviceName": "queue"
        },
        "update_queue_worker": {
          "args": "provisioner_id_in text, worker_type_in text, worker_group_in text, worker_id_in text, quarantine_until_in timestamptz, expires_in timestamptz, recent_tasks_in jsonb",
          "body": "begin\n  return query update queue_workers\n  set\n    quarantine_until = quarantine_until_in,\n    expires = expires_in,\n    recent_tasks = recent_tasks_in\n  where\n    queue_workers.provisioner_id = provisioner_id_in and\n    queue_workers.worker_type = worker_type_in and\n    queue_workers.worker_group = worker_group_in and\n    queue_workers.worker_id = worker_id_in\n  returning\n    queue_workers.provisioner_id,\n    queue_workers.worker_type,\n    queue_workers.worker_group,\n    queue_workers.worker_id,\n    queue_workers.quarantine_until,\n    queue_workers.expires,\n    queue_workers.first_claim,\n    queue_workers.recent_tasks,\n    public.gen_random_uuid();\nend",
          "deprecated": false,
          "description": "Update a queue worker's quarantine_until, expires, and recent_tasks.\nAll parameters must be supplied.",
          "mode": "write",
          "returns": "table(provisioner_id text, worker_type text, worker_group text, worker_id text, quarantine_until timestamptz, expires timestamptz, first_claim timestamptz, recent_tasks jsonb, etag uuid)",
          "serviceName": "queue"
        },
        "update_queue_worker_type": {
          "args": "provisioner_id_in text, worker_type_in text, expires_in timestamptz, last_date_active_in timestamptz, description_in text, stability_in text",
          "body": "begin\n  return query update queue_worker_types\n  set\n    expires = expires_in,\n    last_date_active = last_date_active_in,\n    description = description_in,\n    stability = stability_in\n  where\n    queue_worker_types.provisioner_id = provisioner_id_in and\n    queue_worker_types.worker_type = worker_type_in\n  returning\n    queue_worker_types.provisioner_id,\n    queue_worker_types.worker_type,\n    queue_worker_types.expires,\n    queue_worker_types.last_date_active,\n    queue_worker_types.description,\n    queue_worker_types.stability,\n    public.gen_random_uuid();\nend",
          "deprecated": false,
          "description": "Update a queue worker type's expires, last_date_active, description, and stability.\nAll parameters must be supplied.",
          "mode": "write",
          "returns": "table(provisioner_id text, worker_type text, expires timestamptz, last_date_active timestamptz, description text, stability text, etag uuid)",
          "serviceName": "queue"
        }
      },
      "migrationScript": "begin\n  alter table clients drop column etag;\n  alter table github_builds drop column etag;\n  alter table hooks drop column etag;\n  alter table hooks_last_fires drop column etag;\n  alter table hooks_queues drop column etag;\n  alter table indexed_tasks drop column etag;\n  alter table denylisted_notifications drop column etag;\n  alter table cache_purges drop column etag;\n  alter table access_tokens drop column etag;\n  alter table authorization_codes drop column etag;\n  alter table sessions drop column etag;\n  alter table queue_provisioners drop column etag;\n  alter table queue_worker_types drop column etag;\n  alter table queue_workers drop column etag;\n  alter table task_groups drop column etag;\n  alter table task_dependencies drop column etag;\n  alter table queue_artifacts drop column etag;\n  alter table tasks drop column etag;\n  alter table worker_pool_errors drop column etag;\n  alter table worker_pools drop column etag;\n  alter table index_namespaces drop column etag;\nend",
      "version": 49
    },
    {
      "description": "Use worker_pool_id instead of provisioner_id / worker_type for purge-cache",
      "downgradeScript": "begin\n  lock table cache_purges;\n\n  alter table cache_purges drop constraint cache_purges_pkey;\n  alter table cache_purges add column provisioner_id text, add column worker_type text;\n  update cache_purges\n  set\n    provisioner_id = split_part(cache_purges.worker_pool_id, '/', 1),\n    worker_type = split_part(cache_purges.worker_pool_id, '/', 2);\n  alter table cache_purges drop column worker_pool_id;\n  alter table cache_purges alter column provisioner_id set not null;\n  alter table cache_purges alter column worker_type set not null;\n  alter table cache_purges add primary key (provisioner_id, worker_type, cache_name);\nend",
      "methods": {
        "all_purge_requests": {
          "args": "page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    split_part(cache_purges.worker_pool_id, '/', 1) as provisioner_id,\n    split_part(cache_purges.worker_pool_id, '/', 2) as worker_type,\n    cache_purges.cache_name,\n    cache_purges.before\n  from cache_purges\n  order by\n    cache_purges.worker_pool_id,\n    cache_purges.cache_name\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": true,
          "description": "View all active purge requests.",
          "mode": "read",
          "returns": "table (provisioner_id text, worker_type text, cache_name text, before timestamptz)",
          "serviceName": "purge_cache"
        },
        "all_purge_requests_wpid": {
          "args": "page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    cache_purges.worker_pool_id,\n    cache_purges.cache_name,\n    cache_purges.before\n  from cache_purges\n  order by\n    cache_purges.worker_pool_id,\n    cache_purges.cache_name\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "View all active purge requests.",
          "mode": "read",
          "returns": "table (worker_pool_id text, cache_name text, before timestamptz)",
          "serviceName": "purge_cache"
        },
        "purge_cache": {
          "args": "provisioner_id_in text, worker_type_in text, cache_name_in text, before_in timestamptz, expires_in timestamptz",
          "body": "begin\n  insert into cache_purges(worker_pool_id, cache_name, before, expires)\n  values (\n    provisioner_id_in || '/' || worker_type_in,\n    cache_name_in,\n    before_in,\n    expires_in\n  ) on conflict (worker_pool_id, cache_name) do\n  update\n  set (before, expires) = (before_in, expires_in)\n  where\n    cache_purges.worker_pool_id = provisioner_id_in || '/' || worker_type_in and\n    cache_purges.cache_name = cache_name_in;\nend",
          "deprecated": true,
          "description": "Publish a request to purge caches with name `cache_name_in`\non `provisioner_id_in`/`worker_type_in` workers.",
          "mode": "write",
          "returns": "void",
          "serviceName": "purge_cache"
        },
        "purge_cache_wpid": {
          "args": "worker_pool_id_in text, cache_name_in text, before_in timestamptz, expires_in timestamptz",
          "body": "begin\n  insert into cache_purges(worker_pool_id, cache_name, before, expires)\n  values (\n    worker_pool_id_in,\n    cache_name_in,\n    before_in,\n    expires_in\n  ) on conflict (worker_pool_id, cache_name) do\n  update\n  set (before, expires) = (before_in, expires_in)\n  where\n    cache_purges.worker_pool_id = worker_pool_id_in and\n    cache_purges.cache_name = cache_name_in;\nend",
          "deprecated": false,
          "description": "Publish a request to purge caches with name `cache_name_in`\non `provisioner_id_in`/`worker_type_in` workers.",
          "mode": "write",
          "returns": "void",
          "serviceName": "purge_cache"
        },
        "purge_requests": {
          "args": "provisioner_id_in text, worker_type_in text",
          "body": "begin\n  return query\n  select\n    split_part(cache_purges.worker_pool_id, '/', 1) as provisioner_id,\n    split_part(cache_purges.worker_pool_id, '/', 2) as worker_type,\n    cache_purges.cache_name,\n    cache_purges.before\n  from cache_purges\n  where\n    cache_purges.worker_pool_id = provisioner_id_in || '/' || worker_type_in;\nend",
          "deprecated": true,
          "description": "List the caches for this `provisioner_id_in`/`worker_type_in`.",
          "mode": "read",
          "returns": "table (provisioner_id text, worker_type text, cache_name text, before timestamptz)",
          "serviceName": "purge_cache"
        },
        "purge_requests_wpid": {
          "args": "worker_pool_id_in text",
          "body": "begin\n  return query\n  select\n    cache_purges.worker_pool_id,\n    cache_purges.cache_name,\n    cache_purges.before\n  from cache_purges\n  where\n    cache_purges.worker_pool_id = worker_pool_id_in;\nend",
          "deprecated": false,
          "description": "List the caches for this `provisioner_id_in`/`worker_type_in`.",
          "mode": "read",
          "returns": "table (worker_pool_id text, cache_name text, before timestamptz)",
          "serviceName": "purge_cache"
        }
      },
      "migrationScript": "begin\n  lock table cache_purges;\n\n  alter table cache_purges drop constraint cache_purges_pkey;\n  alter table cache_purges add column worker_pool_id text;\n  update cache_purges set worker_pool_id = provisioner_id || '/' || worker_type;\n  alter table cache_purges drop column provisioner_id, drop column worker_type;\n  alter table cache_purges alter column worker_pool_id set not null;\n  alter table cache_purges add primary key (worker_pool_id, cache_name);\nend",
      "version": 50
    },
    {
      "description": "update get_non_stopped_workers db function to include all fields",
      "methods": {
        "get_non_stopped_workers": {
          "deprecated": true
        },
        "get_non_stopped_workers_2": {
          "args": "worker_pool_id_in text, worker_group_in text, worker_id_in text, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    workers.worker_pool_id,\n    workers.worker_group,\n    workers.worker_id,\n    workers.provider_id,\n    workers.created,\n    workers.expires,\n    workers.state,\n    workers.provider_data,\n    workers.capacity,\n    workers.last_modified,\n    workers.last_checked,\n    workers.secret,\n    workers.etag\n  from workers\n  where\n    (workers.worker_pool_id = worker_pool_id_in or worker_pool_id_in is null) and\n    (workers.worker_group = worker_group_in or worker_group_in is null) and\n    (workers.worker_id = worker_id_in or worker_id_in is null) and\n    (workers.state <> 'stopped')\n  order by worker_pool_id, worker_group, worker_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get non-stopped workers filtered by the optional arguments,\nordered by `worker_pool_id`, `worker_group`, and  `worker_id`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset `page_offset`.",
          "mode": "read",
          "returns": "table(worker_pool_id text, worker_group text, worker_id text, provider_id text, created timestamptz, expires timestamptz, state text, provider_data jsonb, capacity integer, last_modified timestamptz, last_checked timestamptz, secret jsonb, etag uuid)",
          "serviceName": "worker_manager"
        }
      },
      "version": 51
    },
    {
      "description": "deprecate get_workers",
      "methods": {
        "get_workers": {
          "args": "worker_pool_id_in text, worker_group_in text, worker_id_in text, state_in text, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    workers.worker_pool_id,\n    workers.worker_group,\n    workers.worker_id,\n    workers.provider_id,\n    workers.created,\n    workers.expires,\n    workers.state,\n    workers.provider_data,\n    workers.capacity,\n    workers.last_modified,\n    workers.last_checked\n  from workers\n  where\n    (workers.worker_pool_id = worker_pool_id_in or worker_pool_id_in is null) and\n    (workers.worker_group = worker_group_in or worker_group_in is null) and\n    (workers.worker_id = worker_id_in or worker_id_in is null) and\n    (workers.state = state_in or state_in is null)\n  order by worker_pool_id, worker_group, worker_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": true,
          "description": "Get existing workers filtered by the optional arguments,\nordered by `worker_pool_id`, `worker_group`, and  `worker_id`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(worker_pool_id text, worker_group text, worker_id text, provider_id text, created timestamptz, expires timestamptz, state text, provider_data jsonb, capacity integer, last_modified timestamptz, last_checked timestamptz)",
          "serviceName": "worker_manager"
        }
      },
      "version": 52
    },
    {
      "description": "Use task_queue_id instead of provisioner_id / worker_type in queues",
      "downgradeScript": "begin\n  lock table task_queues;\n\n  alter table task_queues drop constraint task_queues_pkey;\n  alter table task_queues add column provisioner_id text, add column worker_type text;\n  update task_queues\n  set\n    provisioner_id = split_part(task_queues.task_queue_id, '/', 1),\n    worker_type = split_part(task_queues.task_queue_id, '/', 2);\n  alter table task_queues drop column task_queue_id;\n  alter table task_queues alter column provisioner_id set not null;\n  alter table task_queues alter column worker_type set not null;\n  alter table task_queues add primary key (provisioner_id, worker_type);\n  alter table task_queues rename to queue_worker_types;\n\n  lock table queue_workers;\n\n  alter table queue_workers drop constraint queue_workers_pkey;\n  alter table queue_workers add column provisioner_id text, add column worker_type text;\n  update queue_workers\n  set\n    provisioner_id = split_part(queue_workers.task_queue_id, '/', 1),\n    worker_type = split_part(queue_workers.task_queue_id, '/', 2);\n  alter table queue_workers drop column task_queue_id;\n  alter table queue_workers alter column provisioner_id set not null;\n  alter table queue_workers alter column worker_type set not null;\n  alter table queue_workers add primary key (provisioner_id, worker_type, worker_group, worker_id);\n\n  create table queue_provisioners\n  as\n    select\n      provisioner_id,\n      max(expires) as expires,\n      max(last_date_active) as last_date_active,\n      '' as description,\n      'experimental' as stability,\n      '[]'::jsonb as actions\n    from queue_worker_types\n    group by provisioner_id;\n\n  alter table queue_provisioners add primary key (provisioner_id);\n  alter table queue_provisioners\n    alter column provisioner_id set not null,\n    alter column expires set not null,\n    alter column last_date_active set not null,\n    alter column description set not null,\n    alter column stability set not null,\n    alter column actions set not null;\n\n  grant select, insert, update, delete on queue_provisioners to $db_user_prefix$_queue;\nend",
      "methods": {
        "create_queue_provisioner": {
          "args": "provisioner_id_in text, expires_in timestamptz, last_date_active_in timestamptz, description_in text, stability_in text, actions_in jsonb",
          "body": "begin\n  return public.gen_random_uuid();\nend",
          "deprecated": true,
          "description": "Create a new queue provisioner.  Raises UNIQUE_VIOLATION if the provisioner already exists.",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "create_queue_worker": {
          "args": "provisioner_id_in text, worker_type_in text, worker_group_in text, worker_id_in text, quarantine_until_in timestamptz, expires_in timestamptz, first_claim_in timestamptz, recent_tasks_in jsonb",
          "body": "begin\n  insert\n    into queue_workers (task_queue_id, worker_group, worker_id, quarantine_until, expires, first_claim, recent_tasks)\n    values (\n      provisioner_id_in || '/' || worker_type_in,\n      worker_group_in,\n      worker_id_in,\n      quarantine_until_in,\n      expires_in,\n      first_claim_in,\n      recent_tasks_in\n    );\n    return public.gen_random_uuid();\nend",
          "deprecated": true,
          "description": "Create a new queue worker.  Raises UNIQUE_VIOLATION if the worker already exists.",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "create_queue_worker_tqid": {
          "args": "task_queue_id_in text, worker_group_in text, worker_id_in text, quarantine_until_in timestamptz, expires_in timestamptz, first_claim_in timestamptz, recent_tasks_in jsonb",
          "body": "begin\n  insert\n    into queue_workers (task_queue_id, worker_group, worker_id, quarantine_until, expires, first_claim, recent_tasks)\n    values (\n      task_queue_id_in,\n      worker_group_in,\n      worker_id_in,\n      quarantine_until_in,\n      expires_in,\n      first_claim_in,\n      recent_tasks_in\n    );\n    return public.gen_random_uuid();\nend",
          "deprecated": false,
          "description": "Create a new queue worker.  Raises UNIQUE_VIOLATION if the worker already exists.",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "create_queue_worker_type": {
          "args": "provisioner_id_in text, worker_type_in text, expires_in timestamptz, last_date_active_in timestamptz, description_in text, stability_in text",
          "body": "begin\n  insert\n    into task_queues (task_queue_id, expires, last_date_active, description, stability)\n    values (\n      provisioner_id_in || '/' || worker_type_in,\n      expires_in,\n      last_date_active_in,\n      description_in, stability_in\n    );\n    return public.gen_random_uuid();\nend",
          "deprecated": true,
          "description": "Create a new queue worker type. Raises UNIQUE_VIOLATION if the worker type already exists.",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "create_task_queue": {
          "args": "task_queue_id_in text, expires_in timestamptz, last_date_active_in timestamptz, description_in text, stability_in text",
          "body": "begin\n  insert\n    into task_queues (task_queue_id, expires, last_date_active, description, stability)\n    values (\n      task_queue_id_in,\n      expires_in,\n      last_date_active_in,\n      description_in, stability_in\n    );\n    return public.gen_random_uuid();\nend",
          "deprecated": false,
          "description": "Create a new task queue. Raises UNIQUE_VIOLATION if the task queue already exists.",
          "mode": "write",
          "returns": "uuid",
          "serviceName": "queue"
        },
        "expire_queue_provisioners": {
          "args": "expires_in timestamptz",
          "body": "begin\n  return 0;\nend",
          "deprecated": true,
          "description": "Expire provisioners that come before `expires_in`.\nReturns a count of rows that have been deleted.",
          "mode": "write",
          "returns": "integer",
          "serviceName": "queue"
        },
        "expire_queue_worker_types": {
          "args": "expires_in timestamptz",
          "body": "declare\n  count integer;\nbegin\n  delete from task_queues\n  where task_queues.expires < expires_in;\n  if found then\n    get diagnostics count = row_count;\n    return count;\n  end if;\n  return 0;\nend",
          "deprecated": true,
          "description": "Expire queue worker types that come before `expires_in`.\nReturns a count of rows that have been deleted.",
          "mode": "write",
          "returns": "integer",
          "serviceName": "queue"
        },
        "expire_task_queues": {
          "args": "expires_in timestamptz",
          "body": "declare\n  count integer;\nbegin\n  delete from task_queues\n  where task_queues.expires < expires_in;\n  if found then\n    get diagnostics count = row_count;\n    return count;\n  end if;\n  return 0;\nend",
          "deprecated": false,
          "description": "Expire task queues that come before `expires_in`.\nReturns a count of rows that have been deleted.",
          "mode": "write",
          "returns": "integer",
          "serviceName": "queue"
        },
        "get_queue_provisioner": {
          "args": "provisioner_id_in text, expires_in timestamptz",
          "body": "begin\n  return query\n  select\n    split_part(task_queue_id, '/', 1) as provisioner_id,\n    max(task_queues.expires),\n    max(task_queues.last_date_active),\n    '' as description,\n    'experimental' as stability,\n    '[]'::jsonb as actions,\n    public.gen_random_uuid()\n  from task_queues\n  group by provisioner_id\n  having\n    split_part(task_queue_id, '/', 1) = provisioner_id_in and\n    max(task_queues.expires) > expires_in;\nend",
          "deprecated": true,
          "description": "Get a queue provisioner by provisioner_id.",
          "mode": "read",
          "returns": "table(provisioner_id text, expires timestamptz, last_date_active timestamptz, description text, stability text, actions jsonb, etag uuid)",
          "serviceName": "queue"
        },
        "get_queue_provisioners": {
          "args": "expires_in timestamptz, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    split_part(task_queues.task_queue_id, '/', 1) as provisioner_id,\n    max(task_queues.expires),\n    max(task_queues.last_date_active),\n    '' as description,\n    'experimental' as stability,\n    '[]'::jsonb as actions,\n    public.gen_random_uuid()\n  from task_queues\n  group by provisioner_id\n  having (max(task_queues.expires) > expires_in or expires_in is null)\n  order by provisioner_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": true,
          "description": "Get queue provisioners ordered by `provisioner_id`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(provisioner_id text, expires timestamptz, last_date_active timestamptz, description text, stability text, actions jsonb, etag uuid)",
          "serviceName": "queue"
        },
        "get_queue_worker": {
          "args": "provisioner_id_in text, worker_type_in text, worker_group_in text, worker_id_in text, expires_in timestamptz",
          "body": "begin\n  return query\n  select\n    split_part(queue_workers.task_queue_id, '/', 1) as provisioner_id,\n    split_part(queue_workers.task_queue_id, '/', 2) as worker_type,\n    queue_workers.worker_group,\n    queue_workers.worker_id,\n    queue_workers.quarantine_until,\n    queue_workers.expires,\n    queue_workers.first_claim,\n    queue_workers.recent_tasks,\n    public.gen_random_uuid()\n  from queue_workers\n  where\n    queue_workers.task_queue_id = provisioner_id_in || '/' || worker_type_in and\n    queue_workers.worker_group = worker_group_in and\n    queue_workers.worker_id = worker_id_in and\n    (queue_workers.expires > expires_in or queue_workers.quarantine_until > expires_in);\n  end",
          "deprecated": true,
          "description": "Get a non-expired queue worker by provisioner_id, worker_type, worker_group, and worker_id.\nWorkers are not considered expired until after their quarantine date expires.",
          "mode": "read",
          "returns": "table(provisioner_id text, worker_type text, worker_group text, worker_id text, quarantine_until timestamptz, expires timestamptz, first_claim timestamptz, recent_tasks jsonb, etag uuid)",
          "serviceName": "queue"
        },
        "get_queue_worker_tqid": {
          "args": "task_queue_id_in text, worker_group_in text, worker_id_in text, expires_in timestamptz",
          "body": "begin\n  return query\n  select\n    queue_workers.task_queue_id,\n    queue_workers.worker_group,\n    queue_workers.worker_id,\n    queue_workers.quarantine_until,\n    queue_workers.expires,\n    queue_workers.first_claim,\n    queue_workers.recent_tasks,\n    public.gen_random_uuid()\n  from queue_workers\n  where\n    queue_workers.task_queue_id = task_queue_id_in and\n    queue_workers.worker_group = worker_group_in and\n    queue_workers.worker_id = worker_id_in and\n    (queue_workers.expires > expires_in or queue_workers.quarantine_until > expires_in);\n  end",
          "deprecated": false,
          "description": "Get a non-expired queue worker by task_queue_id, worker_group, and worker_id.\nWorkers are not considered expired until after their quarantine date expires.",
          "mode": "read",
          "returns": "table(task_queue_id text, worker_group text, worker_id text, quarantine_until timestamptz, expires timestamptz, first_claim timestamptz, recent_tasks jsonb, etag uuid)",
          "serviceName": "queue"
        },
        "get_queue_worker_type": {
          "args": "provisioner_id_in text, worker_type_in text, expires_in timestamptz",
          "body": "begin\n  return query\n  select\n    split_part(task_queues.task_queue_id, '/', 1) as provisioner_id,\n    split_part(task_queues.task_queue_id, '/', 2) as worker_type,\n    task_queues.expires,\n    task_queues.last_date_active,\n    task_queues.description,\n    task_queues.stability,\n    public.gen_random_uuid()\n  from task_queues\n  where\n    task_queues.task_queue_id = provisioner_id_in || '/' || worker_type_in and\n    task_queues.expires > expires_in;\n  end",
          "deprecated": true,
          "description": "Get a non-expired queue worker type by provisioner_id and worker_type.",
          "mode": "read",
          "returns": "table(provisioner_id text, worker_type text, expires timestamptz, last_date_active timestamptz, description text, stability text, etag uuid)",
          "serviceName": "queue"
        },
        "get_queue_worker_types": {
          "args": "provisioner_id_in text, worker_type_in text, expires_in timestamptz, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    split_part(task_queues.task_queue_id, '/', 1) as provisioner_id,\n    split_part(task_queues.task_queue_id, '/', 2) as worker_type,\n    task_queues.expires,\n    task_queues.last_date_active,\n    task_queues.description,\n    task_queues.stability,\n    public.gen_random_uuid()\n  from task_queues\n  where\n    (split_part(task_queues.task_queue_id, '/', 1) = provisioner_id_in or provisioner_id_in is null) and\n    (split_part(task_queues.task_queue_id, '/', 2) = worker_type_in or worker_type_in is null) and\n    (task_queues.expires > expires_in or expires_in is null)\n  order by task_queue_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": true,
          "description": "Get queue worker types ordered by `provisioner_id` and `worker_type`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(provisioner_id text, worker_type text, expires timestamptz, last_date_active timestamptz, description text, stability text, etag uuid)",
          "serviceName": "queue"
        },
        "get_queue_workers": {
          "args": "provisioner_id_in text, worker_type_in text, expires_in timestamptz, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    split_part(queue_workers.task_queue_id, '/', 1) as provisioner_id,\n    split_part(queue_workers.task_queue_id, '/', 2) as worker_type,\n    queue_workers.worker_group,\n    queue_workers.worker_id,\n    queue_workers.quarantine_until,\n    queue_workers.expires,\n    queue_workers.first_claim,\n    queue_workers.recent_tasks,\n    public.gen_random_uuid()\n  from queue_workers\n  where\n    (split_part(queue_workers.task_queue_id, '/', 1) = provisioner_id_in or provisioner_id_in is null) and\n    (split_part(queue_workers.task_queue_id, '/', 2) = worker_type_in or worker_type_in is null) and\n    ((queue_workers.expires > expires_in and queue_workers.quarantine_until < expires_in) or get_queue_workers.expires_in is null)\n  order by task_queue_id, worker_group, worker_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": true,
          "description": "Get non-expired queue workers ordered by provisioner_id, worker_type, worker_group, and worker_id.\nWorkers are not considered expired until after their quarantine date expires.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(provisioner_id text, worker_type text, worker_group text, worker_id text, quarantine_until timestamptz, expires timestamptz, first_claim timestamptz, recent_tasks jsonb, etag uuid)",
          "serviceName": "queue"
        },
        "get_queue_workers_tqid": {
          "args": "task_queue_id_in text, expires_in timestamptz, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    queue_workers.task_queue_id,\n    queue_workers.worker_group,\n    queue_workers.worker_id,\n    queue_workers.quarantine_until,\n    queue_workers.expires,\n    queue_workers.first_claim,\n    queue_workers.recent_tasks,\n    public.gen_random_uuid()\n  from queue_workers\n  where\n    (queue_workers.task_queue_id = task_queue_id_in or get_queue_workers_tqid.task_queue_id_in is null) and\n    ((queue_workers.expires > expires_in and queue_workers.quarantine_until < expires_in) or get_queue_workers_tqid.expires_in is null)\n  order by task_queue_id, worker_group, worker_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get non-expired queue workers ordered by task_queue_id, worker_group, and worker_id.\nWorkers are not considered expired until after their quarantine date expires.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(task_queue_id text, worker_group text, worker_id text, quarantine_until timestamptz, expires timestamptz, first_claim timestamptz, recent_tasks jsonb, etag uuid)",
          "serviceName": "queue"
        },
        "get_task_queue": {
          "args": "task_queue_id_in text, expires_in timestamptz",
          "body": "begin\n  return query\n  select\n    task_queues.task_queue_id,\n    task_queues.expires,\n    task_queues.last_date_active,\n    task_queues.description,\n    task_queues.stability,\n    public.gen_random_uuid()\n  from task_queues\n  where\n    task_queues.task_queue_id = task_queue_id_in and\n    task_queues.expires > expires_in;\n  end",
          "deprecated": false,
          "description": "Get a non-expired task queue by task_queue_id.",
          "mode": "read",
          "returns": "table(task_queue_id text, expires timestamptz, last_date_active timestamptz, description text, stability text, etag uuid)",
          "serviceName": "queue"
        },
        "get_task_queues": {
          "args": "task_queue_id_in text, expires_in timestamptz, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    task_queues.task_queue_id,\n    task_queues.expires,\n    task_queues.last_date_active,\n    task_queues.description,\n    task_queues.stability,\n    public.gen_random_uuid()\n  from task_queues\n  where\n    (task_queues.task_queue_id = task_queue_id_in or task_queue_id_in is null) and\n    (task_queues.expires > expires_in or expires_in is null)\n  order by task_queue_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get task queues ordered by `task_queue_id`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(task_queue_id text, expires timestamptz, last_date_active timestamptz, description text, stability text, etag uuid)",
          "serviceName": "queue"
        },
        "update_queue_provisioner": {
          "args": "provisioner_id_in text, expires_in timestamptz, last_date_active_in timestamptz, description_in text, stability_in text, actions_in jsonb",
          "body": "begin\n  return query\n  select\n    split_part(task_queue_id, '/', 1) as provisioner_id,\n    max(task_queues.expires),\n    max(task_queues.last_date_active),\n    '' as description,\n    'experimental' as stability,\n    '[]'::jsonb as actions,\n    public.gen_random_uuid()\n  from task_queues\n  group by provisioner_id\n  having\n    split_part(task_queue_id, '/', 1) = provisioner_id_in;\nend",
          "deprecated": true,
          "description": "Update a queue provisioner's expires, last_date_active, description, stability, and actions.\nAll parameters must be supplied.",
          "mode": "write",
          "returns": "table(provisioner_id text, expires timestamptz, last_date_active timestamptz, description text, stability text, actions jsonb, etag uuid)",
          "serviceName": "queue"
        },
        "update_queue_worker": {
          "args": "provisioner_id_in text, worker_type_in text, worker_group_in text, worker_id_in text, quarantine_until_in timestamptz, expires_in timestamptz, recent_tasks_in jsonb",
          "body": "begin\n  return query update queue_workers\n  set\n    quarantine_until = quarantine_until_in,\n    expires = expires_in,\n    recent_tasks = recent_tasks_in\n  where\n    queue_workers.task_queue_id = provisioner_id_in || '/' || worker_type_in and\n    queue_workers.worker_group = worker_group_in and\n    queue_workers.worker_id = worker_id_in\n  returning\n    split_part(queue_workers.task_queue_id, '/', 1) as provisioner_id,\n    split_part(queue_workers.task_queue_id, '/', 2) as worker_type,\n    queue_workers.worker_group,\n    queue_workers.worker_id,\n    queue_workers.quarantine_until,\n    queue_workers.expires,\n    queue_workers.first_claim,\n    queue_workers.recent_tasks,\n    public.gen_random_uuid();\nend",
          "deprecated": true,
          "description": "Update a queue worker's quarantine_until, expires, and recent_tasks.\nAll parameters must be supplied.",
          "mode": "write",
          "returns": "table(provisioner_id text, worker_type text, worker_group text, worker_id text, quarantine_until timestamptz, expires timestamptz, first_claim timestamptz, recent_tasks jsonb, etag uuid)",
          "serviceName": "queue"
        },
        "update_queue_worker_tqid": {
          "args": "task_queue_id_in text, worker_group_in text, worker_id_in text, quarantine_until_in timestamptz, expires_in timestamptz, recent_tasks_in jsonb",
          "body": "begin\n  return query update queue_workers\n  set\n    quarantine_until = quarantine_until_in,\n    expires = expires_in,\n    recent_tasks = recent_tasks_in\n  where\n    queue_workers.task_queue_id = task_queue_id_in and\n    queue_workers.worker_group = worker_group_in and\n    queue_workers.worker_id = worker_id_in\n  returning\n    queue_workers.task_queue_id,\n    queue_workers.worker_group,\n    queue_workers.worker_id,\n    queue_workers.quarantine_until,\n    queue_workers.expires,\n    queue_workers.first_claim,\n    queue_workers.recent_tasks,\n    public.gen_random_uuid();\nend",
          "deprecated": false,
          "description": "Update a queue worker's quarantine_until, expires, and recent_tasks.\nAll parameters must be supplied.",
          "mode": "write",
          "returns": "table(task_queue_id text, worker_group text, worker_id text, quarantine_until timestamptz, expires timestamptz, first_claim timestamptz, recent_tasks jsonb, etag uuid)",
          "serviceName": "queue"
        },
        "update_queue_worker_type": {
          "args": "provisioner_id_in text, worker_type_in text, expires_in timestamptz, last_date_active_in timestamptz, description_in text, stability_in text",
          "body": "begin\n  return query update task_queues\n  set\n    expires = expires_in,\n    last_date_active = last_date_active_in,\n    description = description_in,\n    stability = stability_in\n  where\n    task_queues.task_queue_id = provisioner_id_in || '/' || worker_type_in\n  returning\n    split_part(task_queues.task_queue_id, '/', 1) as provisioner_id,\n    split_part(task_queues.task_queue_id, '/', 2) as worker_type,\n    task_queues.expires,\n    task_queues.last_date_active,\n    task_queues.description,\n    task_queues.stability,\n    public.gen_random_uuid();\nend",
          "deprecated": true,
          "description": "Update a queue worker type's expires, last_date_active, description, and stability.\nAll parameters must be supplied.",
          "mode": "write",
          "returns": "table(provisioner_id text, worker_type text, expires timestamptz, last_date_active timestamptz, description text, stability text, etag uuid)",
          "serviceName": "queue"
        },
        "update_task_queue": {
          "args": "task_queue_id_in text, expires_in timestamptz, last_date_active_in timestamptz, description_in text, stability_in text",
          "body": "begin\n  return query update task_queues\n  set\n    expires = expires_in,\n    last_date_active = last_date_active_in,\n    description = description_in,\n    stability = stability_in\n  where\n    task_queues.task_queue_id = task_queue_id_in\n  returning\n    task_queues.task_queue_id,\n    task_queues.expires,\n    task_queues.last_date_active,\n    task_queues.description,\n    task_queues.stability,\n    public.gen_random_uuid();\nend",
          "deprecated": false,
          "description": "Update a task queue's expires, last_date_active, description, and stability.\nAll parameters must be supplied.",
          "mode": "write",
          "returns": "table(task_queue_id text, expires timestamptz, last_date_active timestamptz, description text, stability text, etag uuid)",
          "serviceName": "queue"
        }
      },
      "migrationScript": "begin\n  lock table queue_worker_types;\n\n  alter table queue_worker_types drop constraint queue_worker_types_pkey;\n  alter table queue_worker_types rename to task_queues;\n  alter table task_queues add column task_queue_id text;\n  update task_queues set task_queue_id = provisioner_id || '/' || worker_type;\n  alter table task_queues drop column provisioner_id, drop column worker_type;\n  alter table task_queues alter column task_queue_id set not null;\n  alter table task_queues add primary key (task_queue_id);\n\n  lock table queue_workers;\n\n  alter table queue_workers drop constraint queue_workers_pkey;\n  alter table queue_workers add column task_queue_id text;\n  update queue_workers set task_queue_id = provisioner_id || '/' || worker_type;\n  alter table queue_workers drop column provisioner_id, drop column worker_type;\n  alter table queue_workers alter column task_queue_id set not null;\n  alter table queue_workers add primary key (task_queue_id, worker_group, worker_id);\n\n  lock table queue_provisioners;\n\n  revoke select, insert, update, delete on queue_provisioners from $db_user_prefix$_queue;\n  drop table queue_provisioners;\nend",
      "version": 53
    },
    {
      "description": "Add objects table and functions",
      "downgradeScript": "begin\n  revoke select, insert, update, delete on objects from $db_user_prefix$_object;\n  drop table objects;\nend",
      "methods": {
        "create_object": {
          "args": "name_in text, project_id_in text, backend_id_in text, data_in jsonb, expires_in timestamptz",
          "body": "begin\n  insert\n    into objects (name, data, project_id, backend_id, expires)\n    values (name_in, data_in, project_id_in, backend_id_in, expires_in);\nend",
          "deprecated": false,
          "description": "Upload object.",
          "mode": "write",
          "returns": "void",
          "serviceName": "object"
        },
        "delete_object": {
          "args": "name_in text",
          "body": "begin\n  delete\n  from objects\n  where name = name_in;\nend",
          "deprecated": false,
          "description": "Delete an object.",
          "mode": "write",
          "returns": "void",
          "serviceName": "object"
        },
        "get_expired_objects": {
          "args": "limit_in integer, start_at_in text",
          "body": "begin\n  return query\n  select\n    objects.name,\n    objects.data,\n    objects.project_id,\n    objects.backend_id,\n    objects.expires\n  from objects\n  where\n    (start_at_in is null or objects.name > start_at_in) and\n    objects.expires < now()\n  order by name\n  limit limit_in;\nend",
          "deprecated": false,
          "description": "Get objects with an expiration before the current time.  If given, only\nobjects with a name greater than `start_at_in` are returned.  The\n`limit_in` argument limits the number of results returned.",
          "mode": "read",
          "returns": "table (name text, data jsonb, project_id text, backend_id text, expires timestamptz)",
          "serviceName": "object"
        },
        "get_object": {
          "args": "name_in text",
          "body": "begin\n  return query\n  select\n    objects.name,\n    objects.data,\n    objects.project_id,\n    objects.backend_id,\n    objects.expires\n  from objects\n  where objects.name = name_in;\nend",
          "deprecated": false,
          "description": "Get an object by name, or an empty set if no such object exists.",
          "mode": "read",
          "returns": "table (name text, data jsonb, project_id text, backend_id text, expires timestamptz)",
          "serviceName": "object"
        }
      },
      "migrationScript": "begin\n  create table objects (\n    name text not null,\n    data jsonb not null,\n    backend_id text not null,\n    project_id text not null,\n    expires timestamptz not null\n  );\n  alter table objects add primary key (name);\n\n  grant select, insert, update, delete on objects to $db_user_prefix$_object;\nend",
      "version": 54
    },
    {
      "description": "Insert object into database, raising P0004 if name already exists, but entire row not identical.",
      "methods": {
        "create_object": {
          "args": "name_in text, project_id_in text, backend_id_in text, data_in jsonb, expires_in timestamptz",
          "body": "begin\n  insert\n    into objects (name, data, project_id, backend_id, expires)\n    values (name_in, data_in, project_id_in, backend_id_in, expires_in)\n  on conflict (name) do\n  update set name = name_in\n  where\n    objects.name = name_in\n    and objects.data = data_in\n    and objects.project_id = project_id_in\n    and objects.backend_id = backend_id_in\n    and objects.expires = expires_in;\n  if found then\n    return;\n  end if;\n  raise exception 'conflict' using errcode = 'P0004';\nend",
          "deprecated": false,
          "description": "Upload object.",
          "mode": "write",
          "returns": "void",
          "serviceName": "object"
        }
      },
      "version": 55
    },
    {
      "description": "Add objects.upload_id, upload_expires, and ready columns",
      "downgradeScript": "begin\n  drop index objects_upload_id_idx;\n\n  alter table objects\n  drop column upload_id,\n  drop column upload_expires;\nend",
      "methods": {
        "create_object": {
          "deprecated": true
        },
        "create_object_for_upload": {
          "args": "name_in text,\nproject_id_in text,\nbackend_id_in text,\nupload_id_in text,\nupload_expires_in timestamptz,\ndata_in jsonb,\nexpires_in timestamptz",
          "body": "begin\n  if upload_id_in is null or upload_expires_in is null then\n    raise exception 'upload_id and upload_expires are required' using errcode = 'NOT_NULL_VIOLATION';\n  end if;\n\n  -- NOTE: This table has two unique columns (name and upload_id).  If the inserted name is novel\n  -- but the inserted upload_id is not, this will generate a UNIQUE_VIOLATION error as desired.\n  -- If the inserted name exists, but the upload_id is novel, then the on-conflict clause will\n  -- apply and we will raise UNIQUE_VIOLATION manually.\n  insert\n    into objects (name, data, project_id, backend_id, upload_id, upload_expires, expires)\n    values (name_in, data_in, project_id_in, backend_id_in, upload_id_in, upload_expires_in, expires_in)\n  on conflict (name) do\n  update set name = name_in\n  where\n    objects.name = name_in\n    and objects.data = data_in\n    and objects.project_id = project_id_in\n    and objects.backend_id = backend_id_in\n    and objects.upload_id = upload_id_in\n    -- note that upload_expires isn't consulted\n    and objects.expires = expires_in;\n  if not found then\n    raise exception 'upload already exists' using errcode = 'unique_violation';\n  end if;\nend",
          "deprecated": false,
          "description": "Create an object record ready for upload.\n\nThis method is idempotent, and will succeed if called multiple times with\nthe same parameters, as long as `upload_id` is still set (that is, until\nthe upload is completed).  Otherwise it will raise a UNIQUE_VIOLATION\nexception.  `upload_expires_in` is excluded from this comparison.",
          "mode": "write",
          "returns": "void",
          "serviceName": "object"
        },
        "get_expired_objects": {
          "args": "limit_in integer, start_at_in text",
          "body": "begin\n  return query\n  select\n    objects.name,\n    objects.data,\n    objects.project_id,\n    objects.backend_id,\n    objects.expires\n  from objects\n  where\n    (start_at_in is null or objects.name > start_at_in) and\n    (objects.expires < now() or objects.upload_expires < now())\n  order by name\n  limit limit_in;\nend",
          "deprecated": false,
          "description": "Get objects with an expiration before the current time.  If given, only\nobjects with a name greater than `start_at_in` are returned.  The\n`limit_in` argument limits the number of results returned.  This returns\nboth expired objects (expires < now) and expired uploads (upload_expires\n< now).",
          "mode": "read",
          "returns": "table (name text, data jsonb, project_id text, backend_id text, expires timestamptz)",
          "serviceName": "object"
        },
        "get_object": {
          "deprecated": true
        },
        "get_object_with_upload": {
          "args": "name_in text",
          "body": "begin\n  return query\n  select\n    objects.name,\n    objects.data,\n    objects.project_id,\n    objects.backend_id,\n    objects.upload_id,\n    objects.upload_expires,\n    objects.expires\n  from objects\n  where\n    objects.name = name_in;\nend",
          "deprecated": false,
          "description": "Get an object by name, or an empty set if no such object exists.",
          "mode": "read",
          "returns": "table (name text, data jsonb, project_id text, backend_id text, upload_id text, upload_expires timestamptz, expires timestamptz)",
          "serviceName": "object"
        },
        "object_upload_complete": {
          "args": "name_in text,\nupload_id_in text",
          "body": "begin\n  update objects\n  set\n    upload_id = null,\n    upload_expires = null\n  where\n    name = name_in\n    and upload_id = upload_id_in;\nend",
          "deprecated": false,
          "description": "Mark an object as uploaded and ready for download.\n\nThis method is idempotent, and will succeed if the object is already ready\nfor download.",
          "mode": "write",
          "returns": "void",
          "serviceName": "object"
        }
      },
      "migrationScript": "begin\n  alter table objects\n  add column upload_id text,\n  add column upload_expires timestamptz;\n\n  -- index by upload_id when it is set\n  create unique index\n  objects_upload_id_idx\n  on objects(upload_id)\n  where objects.upload_id is not NULL;\nend",
      "version": 56
    },
    {
      "description": "add get_queue_artifacts_paginated with index-based pagination",
      "methods": {
        "get_queue_artifacts": {
          "deprecated": true
        },
        "get_queue_artifacts_paginated": {
          "args": "task_id_in text, run_id_in integer, expires_in timestamptz, page_size_in integer, after_task_id_in text, after_run_id_in integer, after_name_in text",
          "body": "begin\n  return query\n  select\n    queue_artifacts.task_id,\n    queue_artifacts.run_id,\n    queue_artifacts.name,\n    queue_artifacts.storage_type,\n    queue_artifacts.content_type,\n    queue_artifacts.details,\n    queue_artifacts.present,\n    queue_artifacts.expires\n  from queue_artifacts\n  where\n    (queue_artifacts.task_id = task_id_in or task_id_in is null) and\n    (queue_artifacts.run_id = run_id_in or run_id_in is null) and\n    (queue_artifacts.expires < expires_in or expires_in is null) and\n    (after_task_id_in is null or\n      (queue_artifacts.task_id > after_task_id_in or\n        (queue_artifacts.task_id = after_task_id_in and\n          (queue_artifacts.run_id > after_run_id_in or\n            (queue_artifacts.run_id = after_run_id_in and\n              queue_artifacts.name > after_name_in\n            )\n          )\n        )\n      )\n    )\n  order by queue_artifacts.task_id, queue_artifacts.run_id, queue_artifacts.name\n  limit get_page_limit(page_size_in);\nend",
          "deprecated": false,
          "description": "Get existing queue artifacts, filtered by the optional arguments, ordered\nby the `task_id`, `run_id`, and `name`.  The `after_*` arguments specify\nwhere the page of results should begin, and must all be specified if any\nare specified.  Typically these values would be drawn from the last item\nin the previous page.",
          "mode": "read",
          "returns": "table(task_id text, run_id integer, name text, storage_type text, content_type text, details jsonb, present boolean, expires timestamptz)",
          "serviceName": "queue"
        }
      },
      "version": 57
    },
    {
      "description": "make create_github_check idempotent",
      "methods": {
        "create_github_check": {
          "args": "task_group_id_in text, task_id_in text, check_suite_id_in text, check_run_id_in text",
          "body": "begin\n  insert into github_checks (task_group_id, task_id, check_suite_id, check_run_id) values (task_group_id_in, task_id_in, check_suite_id_in, check_run_id_in)\n  on conflict (task_group_id, task_id) do update set\n    check_suite_id = check_suite_id_in,\n    check_run_id = check_run_id_in;\nend",
          "deprecated": false,
          "description": "Upsert a single check.",
          "mode": "write",
          "returns": "void",
          "serviceName": "github"
        }
      },
      "version": 58
    },
    {
      "description": "Add task_queue_id column to tasks table and populate with online migration",
      "downgradeScript": "begin\n  alter table tasks drop column task_queue_id;\n\nend",
      "methods": {
        "create_task": {
          "args": "task_id text,\nprovisioner_id text,\nworker_type text,\nscheduler_id text,\ntask_group_id text,\ndependencies jsonb,\nrequires task_requires,\nroutes jsonb,\npriority task_priority,\nretries integer,\ncreated timestamptz,\ndeadline timestamptz,\nexpires timestamptz,\nscopes jsonb,\npayload jsonb,\nmetadata jsonb,\ntags jsonb,\nextra jsonb",
          "body": "begin\n  insert\n  into tasks (\n    task_id,\n    provisioner_id,\n    worker_type,\n    task_queue_id,\n    scheduler_id,\n    task_group_id,\n    dependencies,\n    requires,\n    routes,\n    priority,\n    retries,\n    created,\n    deadline,\n    expires,\n    scopes,\n    payload,\n    metadata,\n    tags,\n    extra,\n    retries_left,\n    runs,\n    taken_until,\n    ever_resolved\n  )\n  values (\n    task_id,\n    provisioner_id,\n    worker_type,\n    provisioner_id || '/' || worker_type,\n    scheduler_id,\n    task_group_id,\n    dependencies,\n    requires,\n    routes,\n    priority,\n    retries,\n    created,\n    deadline,\n    expires,\n    scopes,\n    payload,\n    metadata,\n    tags,\n    extra,\n    -- default values for the mutable bits\n    retries,\n    jsonb_build_array(),\n    null, -- not taken\n    false\n  );\nend",
          "deprecated": false,
          "description": "Create a new task, without scheduling it, and with empty values\nfor the status information.",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "get_task": {
          "args": "task_id_in text",
          "body": "begin\n  return query\n  select\n    tasks.task_id,\n    coalesce(tasks.provisioner_id, split_part(tasks.task_queue_id, '/', 1)),\n    coalesce(tasks.worker_type, split_part(tasks.task_queue_id, '/', 2)),\n    tasks.scheduler_id,\n    tasks.task_group_id,\n    tasks.dependencies,\n    tasks.requires,\n    tasks.routes,\n    tasks.priority,\n    tasks.retries,\n    tasks.retries_left,\n    tasks.created,\n    tasks.deadline,\n    tasks.expires,\n    tasks.scopes,\n    tasks.payload,\n    tasks.metadata,\n    tasks.tags,\n    tasks.extra,\n    tasks.runs,\n    tasks.taken_until\n  from tasks\n  where\n    tasks.task_id = task_id_in;\nend",
          "deprecated": false,
          "description": "Get all properties of a task.  Note that all properties but `runs`,\n`retries_left`, and `taken_until` are immutable.",
          "mode": "read",
          "returns": "table (\n  task_id text,\n  provisioner_id text,\n  worker_type text,\n  scheduler_id text,\n  task_group_id text,\n  dependencies jsonb,\n  requires task_requires,\n  routes jsonb,\n  priority task_priority,\n  retries integer,\n  retries_left int,\n  created timestamptz,\n  deadline timestamptz,\n  expires timestamptz,\n  scopes jsonb,\n  payload jsonb,\n  metadata jsonb,\n  tags jsonb,\n  extra jsonb,\n  runs jsonb,\n  taken_until timestamptz\n)",
          "serviceName": "queue"
        },
        "get_tasks_by_task_group": {
          "args": "task_group_id_in text, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    tasks.task_id,\n    coalesce(tasks.provisioner_id, split_part(tasks.task_queue_id, '/', 1)),\n    coalesce(tasks.worker_type, split_part(tasks.task_queue_id, '/', 2)),\n    tasks.scheduler_id,\n    tasks.task_group_id,\n    tasks.dependencies,\n    tasks.requires,\n    tasks.routes,\n    tasks.priority,\n    tasks.retries,\n    tasks.retries_left,\n    tasks.created,\n    tasks.deadline,\n    tasks.expires,\n    tasks.scopes,\n    tasks.payload,\n    tasks.metadata,\n    tasks.tags,\n    tasks.extra,\n    tasks.runs,\n    tasks.taken_until\n  from tasks\n  where tasks.task_group_id = task_group_id_in\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": true,
          "description": "Get all properties of all tasks in the given task group.",
          "mode": "read",
          "returns": "table (\n  task_id text,\n  provisioner_id text,\n  worker_type text,\n  scheduler_id text,\n  task_group_id text,\n  dependencies jsonb,\n  requires task_requires,\n  routes jsonb,\n  priority task_priority,\n  retries integer,\n  retries_left int,\n  created timestamptz,\n  deadline timestamptz,\n  expires timestamptz,\n  scopes jsonb,\n  payload jsonb,\n  metadata jsonb,\n  tags jsonb,\n  extra jsonb,\n  runs jsonb,\n  taken_until timestamptz\n)",
          "serviceName": "queue"
        }
      },
      "migrationScript": "begin\n  alter table tasks add column task_queue_id text;\n\n  create function online_migration_v59_batch(batch_size_in integer, state_in jsonb)\n  returns table (count integer, state jsonb) as $$\n  declare\n    item record;\n    count integer;\n    last_task_id text;\n  begin\n    count := 0;\n    for item in\n      select task_id\n      from tasks\n      where\n        (state_in ->> 'task_id' is null or task_id > state_in ->> 'task_id') and\n        task_queue_id is null\n      order by task_id\n      limit batch_size_in\n    loop\n      update tasks\n      set task_queue_id = tasks.provisioner_id || '/' || tasks.worker_type\n      where tasks.task_id = item.task_id;\n      count := count + 1;\n    end loop;\n    return query select\n      count as count,\n      to_jsonb(item) as state;\n  end\n  $$ language plpgsql;\n\n  create function online_migration_v59_is_complete() returns boolean as $$\n  begin\n    perform * from tasks where task_queue_id is null limit 1;\n    return not found;\n  end\n  $$ language plpgsql;\nend",
      "version": 59
    },
    {
      "description": "Drop provisioner_id / worker_type in queues and only use task_queue_id instead",
      "downgradeScript": "begin\n  alter table tasks drop constraint task_queue_id_not_null;\n  alter table tasks add column provisioner_id text, add column worker_type text;\n\n  create function online_downgrade_v60_batch(batch_size_in integer, state_in jsonb)\n  returns table (count integer, state jsonb) as $$\n  declare\n    item record;\n    count integer;\n  begin\n    count := 0;\n\n    for item in\n      select task_id\n      from tasks\n      where\n        (state_in ->> 'task_id' is null or task_id > state_in ->> 'task_id') and\n        worker_type is null or provisioner_id is null\n      order by task_id\n      limit batch_size_in\n    loop\n      update tasks\n      set\n        provisioner_id = split_part(tasks.task_queue_id, '/', 1),\n        worker_type = split_part(tasks.task_queue_id, '/', 2)\n      where tasks.task_id = item.task_id;\n      count := count + 1;\n    end loop;\n    return query select\n      count as count,\n      to_jsonb(item) as state;\n  end\n  $$ language plpgsql;\n\n  create function online_downgrade_v60_is_complete() returns boolean as $$\n  begin\n    perform * from tasks where worker_type is null or provisioner_id is null limit 1;\n    return not found;\n  end\n  $$ language plpgsql;\n\nend",
      "methods": {
        "create_task": {
          "args": "task_id text,\nprovisioner_id text,\nworker_type text,\nscheduler_id text,\ntask_group_id text,\ndependencies jsonb,\nrequires task_requires,\nroutes jsonb,\npriority task_priority,\nretries integer,\ncreated timestamptz,\ndeadline timestamptz,\nexpires timestamptz,\nscopes jsonb,\npayload jsonb,\nmetadata jsonb,\ntags jsonb,\nextra jsonb",
          "body": "begin\n  insert\n  into tasks (\n    task_id,\n    task_queue_id,\n    scheduler_id,\n    task_group_id,\n    dependencies,\n    requires,\n    routes,\n    priority,\n    retries,\n    created,\n    deadline,\n    expires,\n    scopes,\n    payload,\n    metadata,\n    tags,\n    extra,\n    retries_left,\n    runs,\n    taken_until,\n    ever_resolved\n  )\n  values (\n    task_id,\n    provisioner_id || '/' || worker_type,\n    scheduler_id,\n    task_group_id,\n    dependencies,\n    requires,\n    routes,\n    priority,\n    retries,\n    created,\n    deadline,\n    expires,\n    scopes,\n    payload,\n    metadata,\n    tags,\n    extra,\n    -- default values for the mutable bits\n    retries,\n    jsonb_build_array(),\n    null, -- not taken\n    false\n  );\nend",
          "deprecated": false,
          "description": "Create a new task, without scheduling it, and with empty values\nfor the status information.",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "get_task": {
          "args": "task_id_in text",
          "body": "begin\n  return query\n  select\n    tasks.task_id,\n    split_part(tasks.task_queue_id, '/', 1) as provisioner_id,\n    split_part(tasks.task_queue_id, '/', 2) as worker_type,\n    tasks.scheduler_id,\n    tasks.task_group_id,\n    tasks.dependencies,\n    tasks.requires,\n    tasks.routes,\n    tasks.priority,\n    tasks.retries,\n    tasks.retries_left,\n    tasks.created,\n    tasks.deadline,\n    tasks.expires,\n    tasks.scopes,\n    tasks.payload,\n    tasks.metadata,\n    tasks.tags,\n    tasks.extra,\n    tasks.runs,\n    tasks.taken_until\n  from tasks\n  where\n    tasks.task_id = task_id_in;\nend",
          "deprecated": false,
          "description": "Get all properties of a task.  Note that all properties but `runs`,\n`retries_left`, and `taken_until` are immutable.",
          "mode": "read",
          "returns": "table (\n  task_id text,\n  provisioner_id text,\n  worker_type text,\n  scheduler_id text,\n  task_group_id text,\n  dependencies jsonb,\n  requires task_requires,\n  routes jsonb,\n  priority task_priority,\n  retries integer,\n  retries_left int,\n  created timestamptz,\n  deadline timestamptz,\n  expires timestamptz,\n  scopes jsonb,\n  payload jsonb,\n  metadata jsonb,\n  tags jsonb,\n  extra jsonb,\n  runs jsonb,\n  taken_until timestamptz\n)",
          "serviceName": "queue"
        },
        "get_tasks_by_task_group": {
          "args": "task_group_id_in text, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    tasks.task_id,\n    split_part(tasks.task_queue_id, '/', 1) as provisioner_id,\n    split_part(tasks.task_queue_id, '/', 2) as worker_type,\n    tasks.scheduler_id,\n    tasks.task_group_id,\n    tasks.dependencies,\n    tasks.requires,\n    tasks.routes,\n    tasks.priority,\n    tasks.retries,\n    tasks.retries_left,\n    tasks.created,\n    tasks.deadline,\n    tasks.expires,\n    tasks.scopes,\n    tasks.payload,\n    tasks.metadata,\n    tasks.tags,\n    tasks.extra,\n    tasks.runs,\n    tasks.taken_until\n  from tasks\n  where tasks.task_group_id = task_group_id_in\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get all properties of all tasks in the given task group.",
          "mode": "read",
          "returns": "table (\n  task_id text,\n  provisioner_id text,\n  worker_type text,\n  scheduler_id text,\n  task_group_id text,\n  dependencies jsonb,\n  requires task_requires,\n  routes jsonb,\n  priority task_priority,\n  retries integer,\n  retries_left int,\n  created timestamptz,\n  deadline timestamptz,\n  expires timestamptz,\n  scopes jsonb,\n  payload jsonb,\n  metadata jsonb,\n  tags jsonb,\n  extra jsonb,\n  runs jsonb,\n  taken_until timestamptz\n)",
          "serviceName": "queue"
        }
      },
      "migrationScript": "begin\n  alter table tasks drop column provisioner_id, drop column worker_type;\n  alter table tasks add constraint task_queue_id_not_null check (task_queue_id is not null) not valid;\nend",
      "version": 60
    },
    {
      "description": "Deprecate functions that use provisioner_id/worker_type and add equivalents using task_queue_id instead, for tasks table",
      "methods": {
        "create_task": {
          "deprecated": true
        },
        "create_task_tqid": {
          "args": "task_id text,\ntask_queue_id text,\nscheduler_id text,\ntask_group_id text,\ndependencies jsonb,\nrequires task_requires,\nroutes jsonb,\npriority task_priority,\nretries integer,\ncreated timestamptz,\ndeadline timestamptz,\nexpires timestamptz,\nscopes jsonb,\npayload jsonb,\nmetadata jsonb,\ntags jsonb,\nextra jsonb",
          "body": "begin\n  insert\n  into tasks (\n    task_id,\n    task_queue_id,\n    scheduler_id,\n    task_group_id,\n    dependencies,\n    requires,\n    routes,\n    priority,\n    retries,\n    created,\n    deadline,\n    expires,\n    scopes,\n    payload,\n    metadata,\n    tags,\n    extra,\n    retries_left,\n    runs,\n    taken_until,\n    ever_resolved\n  )\n  values (\n    task_id,\n    task_queue_id,\n    scheduler_id,\n    task_group_id,\n    dependencies,\n    requires,\n    routes,\n    priority,\n    retries,\n    created,\n    deadline,\n    expires,\n    scopes,\n    payload,\n    metadata,\n    tags,\n    extra,\n    -- default values for the mutable bits\n    retries,\n    jsonb_build_array(),\n    null, -- not taken\n    false\n  );\nend",
          "deprecated": false,
          "description": "Create a new task, without scheduling it, and with empty values\nfor the status information.",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "get_task": {
          "deprecated": true
        },
        "get_task_tqid": {
          "args": "task_id_in text",
          "body": "begin\n  return query\n  select\n    tasks.task_id,\n    tasks.task_queue_id,\n    tasks.scheduler_id,\n    tasks.task_group_id,\n    tasks.dependencies,\n    tasks.requires,\n    tasks.routes,\n    tasks.priority,\n    tasks.retries,\n    tasks.retries_left,\n    tasks.created,\n    tasks.deadline,\n    tasks.expires,\n    tasks.scopes,\n    tasks.payload,\n    tasks.metadata,\n    tasks.tags,\n    tasks.extra,\n    tasks.runs,\n    tasks.taken_until\n  from tasks\n  where\n    tasks.task_id = task_id_in;\nend",
          "deprecated": false,
          "description": "Get all properties of a task.  Note that all properties but `runs`,\n`retries_left`, and `taken_until` are immutable.",
          "mode": "read",
          "returns": "table (\n  task_id text,\n  task_queue_id text,\n  scheduler_id text,\n  task_group_id text,\n  dependencies jsonb,\n  requires task_requires,\n  routes jsonb,\n  priority task_priority,\n  retries integer,\n  retries_left int,\n  created timestamptz,\n  deadline timestamptz,\n  expires timestamptz,\n  scopes jsonb,\n  payload jsonb,\n  metadata jsonb,\n  tags jsonb,\n  extra jsonb,\n  runs jsonb,\n  taken_until timestamptz\n)",
          "serviceName": "queue"
        },
        "get_tasks_by_task_group": {
          "deprecated": true
        },
        "get_tasks_by_task_group_tqid": {
          "args": "task_group_id_in text, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    tasks.task_id,\n    tasks.task_queue_id,\n    tasks.scheduler_id,\n    tasks.task_group_id,\n    tasks.dependencies,\n    tasks.requires,\n    tasks.routes,\n    tasks.priority,\n    tasks.retries,\n    tasks.retries_left,\n    tasks.created,\n    tasks.deadline,\n    tasks.expires,\n    tasks.scopes,\n    tasks.payload,\n    tasks.metadata,\n    tasks.tags,\n    tasks.extra,\n    tasks.runs,\n    tasks.taken_until\n  from tasks\n  where tasks.task_group_id = task_group_id_in\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get all properties of all tasks in the given task group.",
          "mode": "read",
          "returns": "table (\n  task_id text,\n  task_queue_id text,\n  scheduler_id text,\n  task_group_id text,\n  dependencies jsonb,\n  requires task_requires,\n  routes jsonb,\n  priority task_priority,\n  retries integer,\n  retries_left int,\n  created timestamptz,\n  deadline timestamptz,\n  expires timestamptz,\n  scopes jsonb,\n  payload jsonb,\n  metadata jsonb,\n  tags jsonb,\n  extra jsonb,\n  runs jsonb,\n  taken_until timestamptz\n)",
          "serviceName": "queue"
        }
      },
      "version": 61
    },
    {
      "description": "add update_queue_artifact_2 to update artifact storage-type as well",
      "methods": {
        "update_queue_artifact": {
          "deprecated": true
        },
        "update_queue_artifact_2": {
          "args": "task_id_in text, run_id_in integer, name_in text, storage_type_in text, details_in jsonb, expires_in timestamptz",
          "body": "declare\n  updated_row queue_artifacts%ROWTYPE;\nbegin\n  update queue_artifacts\n  set (details, storage_type, expires) = (\n    coalesce(details_in, queue_artifacts.details),\n    coalesce(storage_type_in, queue_artifacts.storage_type),\n    coalesce(expires_in, queue_artifacts.expires)\n  )\n  where\n    queue_artifacts.task_id = task_id_in and\n    queue_artifacts.run_id = run_id_in and\n    queue_artifacts.name = name_in\n  returning\n    queue_artifacts.task_id,\n    queue_artifacts.run_id,\n    queue_artifacts.name,\n    queue_artifacts.storage_type,\n    queue_artifacts.content_type,\n    queue_artifacts.details,\n    queue_artifacts.present,\n    queue_artifacts.expires\n  into updated_row;\n  if found then\n    return query select\n      updated_row.task_id,\n      updated_row.run_id,\n      updated_row.name,\n      updated_row.storage_type,\n      updated_row.content_type,\n      updated_row.details,\n      updated_row.present,\n      updated_row.expires\n    return;\n  else\n    raise exception 'no such row' using errcode = 'P0002';\n  end if;\nend",
          "deprecated": false,
          "description": "Update a queue artifact, including its storageType.\nReturns the up-to-date artifact row that have the same task id, run id, and name.",
          "mode": "write",
          "returns": "table(task_id text, run_id integer, name text, storage_type text, content_type text, details jsonb, present boolean, expires timestamptz)",
          "serviceName": "queue"
        }
      },
      "version": 62
    },
    {
      "description": "Add `project_id` to tasks table",
      "downgradeScript": "begin\n  alter table tasks drop column project_id;\nend",
      "methods": {
        "create_task_projid": {
          "args": "task_id text,\ntask_queue_id text,\nscheduler_id text,\nproject_id text,\ntask_group_id text,\ndependencies jsonb,\nrequires task_requires,\nroutes jsonb,\npriority task_priority,\nretries integer,\ncreated timestamptz,\ndeadline timestamptz,\nexpires timestamptz,\nscopes jsonb,\npayload jsonb,\nmetadata jsonb,\ntags jsonb,\nextra jsonb",
          "body": "begin\n  insert\n  into tasks (\n    task_id,\n    task_queue_id,\n    scheduler_id,\n    project_id,\n    task_group_id,\n    dependencies,\n    requires,\n    routes,\n    priority,\n    retries,\n    created,\n    deadline,\n    expires,\n    scopes,\n    payload,\n    metadata,\n    tags,\n    extra,\n    retries_left,\n    runs,\n    taken_until,\n    ever_resolved\n  )\n  values (\n    task_id,\n    task_queue_id,\n    scheduler_id,\n    project_id,\n    task_group_id,\n    dependencies,\n    requires,\n    routes,\n    priority,\n    retries,\n    created,\n    deadline,\n    expires,\n    scopes,\n    payload,\n    metadata,\n    tags,\n    extra,\n    -- default values for the mutable bits\n    retries,\n    jsonb_build_array(),\n    null, -- not taken\n    false\n  );\nend",
          "deprecated": false,
          "description": "Create a new task, without scheduling it, and with empty values\nfor the status information.",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "create_task_tqid": {
          "deprecated": true
        },
        "get_task_projid": {
          "args": "task_id_in text",
          "body": "begin\n  return query\n  select\n    tasks.task_id,\n    tasks.task_queue_id,\n    tasks.scheduler_id,\n    -- treat null project_id as 'none'\n    coalesce(tasks.project_id, 'none') as project_id,\n    tasks.task_group_id,\n    tasks.dependencies,\n    tasks.requires,\n    tasks.routes,\n    tasks.priority,\n    tasks.retries,\n    tasks.retries_left,\n    tasks.created,\n    tasks.deadline,\n    tasks.expires,\n    tasks.scopes,\n    tasks.payload,\n    tasks.metadata,\n    tasks.tags,\n    tasks.extra,\n    tasks.runs,\n    tasks.taken_until\n  from tasks\n  where\n    tasks.task_id = task_id_in;\nend",
          "deprecated": false,
          "description": "Get all properties of a task.  Note that all properties but `runs`,\n`retries_left`, and `taken_until` are immutable.",
          "mode": "read",
          "returns": "table (\n  task_id text,\n  task_queue_id text,\n  scheduler_id text,\n  project_id text,\n  task_group_id text,\n  dependencies jsonb,\n  requires task_requires,\n  routes jsonb,\n  priority task_priority,\n  retries integer,\n  retries_left int,\n  created timestamptz,\n  deadline timestamptz,\n  expires timestamptz,\n  scopes jsonb,\n  payload jsonb,\n  metadata jsonb,\n  tags jsonb,\n  extra jsonb,\n  runs jsonb,\n  taken_until timestamptz\n)",
          "serviceName": "queue"
        },
        "get_task_tqid": {
          "deprecated": true
        },
        "get_tasks_by_task_group_projid": {
          "args": "task_group_id_in text, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    tasks.task_id,\n    tasks.task_queue_id,\n    tasks.scheduler_id,\n    -- treat null project_id as 'none'\n    coalesce(tasks.project_id, 'none') as project_id,\n    tasks.task_group_id,\n    tasks.dependencies,\n    tasks.requires,\n    tasks.routes,\n    tasks.priority,\n    tasks.retries,\n    tasks.retries_left,\n    tasks.created,\n    tasks.deadline,\n    tasks.expires,\n    tasks.scopes,\n    tasks.payload,\n    tasks.metadata,\n    tasks.tags,\n    tasks.extra,\n    tasks.runs,\n    tasks.taken_until\n  from tasks\n  where tasks.task_group_id = task_group_id_in\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get all properties of all tasks in the given task group.",
          "mode": "read",
          "returns": "table (\n  task_id text,\n  task_queue_id text,\n  scheduler_id text,\n  project_id text,\n  task_group_id text,\n  dependencies jsonb,\n  requires task_requires,\n  routes jsonb,\n  priority task_priority,\n  retries integer,\n  retries_left int,\n  created timestamptz,\n  deadline timestamptz,\n  expires timestamptz,\n  scopes jsonb,\n  payload jsonb,\n  metadata jsonb,\n  tags jsonb,\n  extra jsonb,\n  runs jsonb,\n  taken_until timestamptz\n)",
          "serviceName": "queue"
        },
        "get_tasks_by_task_group_tqid": {
          "deprecated": true
        }
      },
      "migrationScript": "begin\n  alter table tasks add column project_id text;\nend",
      "version": 63
    },
    {
      "description": "Add `queue_worker_seen` and `queue_task_queue_seen` functions",
      "methods": {
        "create_queue_worker_tqid": {
          "deprecated": true
        },
        "create_task_queue": {
          "deprecated": true
        },
        "quarantine_queue_worker": {
          "args": "task_queue_id_in text, worker_group_in text, worker_id_in text, quarantine_until_in timestamptz",
          "body": "begin\n  return query update queue_workers\n  set\n    quarantine_until = quarantine_until_in,\n    expires = greatest(queue_workers.expires, now() + interval '1 day')\n  where\n    queue_workers.task_queue_id = task_queue_id_in and\n    queue_workers.worker_group = worker_group_in and\n    queue_workers.worker_id = worker_id_in\n  returning\n    queue_workers.task_queue_id,\n    queue_workers.worker_group,\n    queue_workers.worker_id,\n    queue_workers.quarantine_until,\n    queue_workers.expires,\n    queue_workers.first_claim,\n    queue_workers.recent_tasks;\nend",
          "deprecated": false,
          "description": "Update the quarantine_until date for a worker.  The Queue service interprets a date in the past\nas \"not quarantined\".  This function also \"bumps\" the expiration of the worker so that un-quarantined\nworkers do not immediately expire.  Returns the worker row just as get_queue_worker would, or no rows if\nno such worker exists.",
          "mode": "write",
          "returns": "table(task_queue_id text, worker_group text, worker_id text, quarantine_until timestamptz, expires timestamptz, first_claim timestamptz, recent_tasks jsonb)",
          "serviceName": "queue"
        },
        "queue_worker_seen": {
          "args": "task_queue_id_in text, worker_group_in text, worker_id_in text, expires_in timestamptz",
          "body": "begin\n  insert\n    into queue_workers (task_queue_id, worker_group, worker_id, quarantine_until, expires, first_claim, recent_tasks)\n    values (\n      task_queue_id_in,\n      worker_group_in,\n      worker_id_in,\n      now() - interval '10 years',\n      expires_in,\n      now(),\n      jsonb_build_array()\n    )\n    on conflict (task_queue_id, worker_group, worker_id) do update\n    set\n      expires = greatest(coalesce(expires_in, queue_workers.expires), queue_workers.expires)\n    where\n      queue_workers.task_queue_id = task_queue_id_in and\n      queue_workers.worker_group = worker_group_in and\n      queue_workers.worker_id = worker_id_in;\nend",
          "deprecated": false,
          "description": "Recognize that a worker has been seen by the queue, creating it if necessary.  This is called\nwhen workers claim or re-claim work.  The expiration time is not allowed to move backward.\n\nThis function always writes to the DB, so calls should be suitably rate-limited at the\nclient side.",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "queue_worker_task_seen": {
          "args": "task_queue_id_in text, worker_group_in text, worker_id_in text, task_run_in jsonb",
          "body": "begin\n  update queue_workers\n  set\n    -- append without increasing size over 20\n    recent_tasks = case\n      when jsonb_array_length(recent_tasks) > 19 then (recent_tasks - 0)\n      else recent_tasks\n    end || jsonb_build_array(task_run_in)\n  where\n    queue_workers.task_queue_id = task_queue_id_in and\n    queue_workers.worker_group = worker_group_in and\n    queue_workers.worker_id = worker_id_in;\nend",
          "deprecated": false,
          "description": "Update the worker record to indicate that this task run was seen there.  The\ntask run should be a JSON object with keys `taskId` and `runId`.  This will\nadd the task to `recent_tasks`, keeping the most recent 20 tasks. This\nwill do nothing, but not fail, if the worker does not exist, as it is\nunusual for a nonexistent worker to claim work.",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "task_queue_seen": {
          "args": "task_queue_id_in text, expires_in timestamptz, description_in text, stability_in text",
          "body": "begin\n  insert\n    into task_queues (task_queue_id, expires, last_date_active, description, stability)\n    values (\n      task_queue_id_in,\n      expires_in,\n      now(),\n      coalesce(description_in, ''),\n      coalesce(stability_in, 'experimental')\n    )\n    on conflict (task_queue_id) do update\n    set\n      expires = greatest(coalesce(expires_in, task_queues.expires), task_queues.expires),\n      last_date_active = now(),\n      description = coalesce(description_in, task_queues.description),\n      stability = coalesce(stability_in, task_queues.stability)\n    where task_queues.task_queue_id = task_queue_id_in;\nend",
          "deprecated": false,
          "description": "Recognize that a task queue has been seen, creating it if necessary, updating\nits properties if not null, and in any case bumping its last seen time time.\nThe expiration time is not allowed to move backward.\n\nThis function always writes to the DB, so calls should be suitably rate-limited at the\nclient side.",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        },
        "update_queue_worker_tqid": {
          "deprecated": true
        },
        "update_task_queue": {
          "deprecated": true
        }
      },
      "version": 64
    },
    {
      "description": "add `delete_indexed_task`",
      "methods": {
        "delete_indexed_task": {
          "args": "namespace_in text, name_in text",
          "body": "begin\n  delete\n  from indexed_tasks\n  where\n    indexed_tasks.namespace = namespace_in and\n    indexed_tasks.name = name_in;\nend",
          "deprecated": false,
          "description": "Delete the named task from the index.  Returns succesfully even if the named\ntask does not exist.",
          "mode": "write",
          "returns": "void",
          "serviceName": "index"
        }
      },
      "version": 65
    },
    {
      "description": "add get_non_stopped_workers_with_quarantine to include quarantine info from the queue",
      "downgradeScript": "begin\n  revoke select on queue_workers from $db_user_prefix$_worker_manager;\nend\n",
      "methods": {
        "get_non_stopped_workers_2": {
          "deprecated": true
        },
        "get_non_stopped_workers_quntil": {
          "args": "worker_pool_id_in text, worker_group_in text, worker_id_in text, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    workers.worker_pool_id,\n    workers.worker_group,\n    workers.worker_id,\n    workers.provider_id,\n    workers.created,\n    workers.expires,\n    workers.state,\n    workers.provider_data,\n    workers.capacity,\n    workers.last_modified,\n    workers.last_checked,\n    workers.secret,\n    workers.etag,\n    queue_workers.quarantine_until\n  from\n    workers\n    left join queue_workers on\n      workers.worker_pool_id = queue_workers.task_queue_id and\n      workers.worker_id = queue_workers.worker_id and\n      workers.worker_group = queue_workers.worker_group\n  where\n    (workers.worker_pool_id = worker_pool_id_in or worker_pool_id_in is null) and\n    (workers.worker_group = worker_group_in or worker_group_in is null) and\n    (workers.worker_id = worker_id_in or worker_id_in is null) and\n    (workers.state <> 'stopped')\n  order by worker_pool_id, worker_group, worker_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get non-stopped workers filtered by the optional arguments,\nordered by `worker_pool_id`, `worker_group`, and  `worker_id`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset `page_offset`.\nThe `quaratine_until` contains NULL or a date in the past if the\nworker is not quarantined, otherwise the date until which it is\nquaratined.",
          "mode": "read",
          "returns": "table(worker_pool_id text, worker_group text, worker_id text, provider_id text, created timestamptz, expires timestamptz, state text, provider_data jsonb, capacity integer, last_modified timestamptz, last_checked timestamptz, secret jsonb, etag uuid, quarantine_until timestamptz)",
          "serviceName": "worker_manager"
        }
      },
      "migrationScript": "begin\n  grant select on queue_workers to $db_user_prefix$_worker_manager;\nend\n",
      "version": 66
    },
    {
      "description": "add secure storage of object hashes",
      "downgradeScript": "begin\n  revoke select, insert, update, delete on object_hashes from $db_user_prefix$_object;\n  drop table object_hashes;\nend",
      "methods": {
        "add_object_hashes": {
          "args": "name_in text, hashes_in jsonb",
          "body": "declare\n  item record;\n  object record;\nbegin\n  select objects.name, objects.upload_id\n    into object\n    from objects\n    where name = name_in;\n  raise log 'object %', object;\n  if not found then\n    raise exception 'object does not exist' using errcode = 'foreign_key_violation';\n  end if;\n  if object.upload_id is null then\n    raise exception 'object upload is already finished' using errcode = 'check_violation';\n  end if;\n\n  -- insert each hash individually; in this case at least all hashes\n  -- end up inserted in the same transaction.  On conflict, we verify\n  -- that the hash value matches.\n  for item in\n    select\n      name_in as name,\n      key as algorithm,\n      value as hash\n    from jsonb_each_text(hashes_in)\n  loop\n    begin\n      insert\n      into object_hashes (name, algorithm, hash)\n      values (item.name, item.algorithm, item.hash);\n    exception\n      when UNIQUE_VIOLATION then\n        perform 1\n        from object_hashes\n        where\n          name = item.name and\n          algorithm = item.algorithm and\n          hash = item.hash;\n        if not found then\n          raise exception 'object hash already exists with different value' using errcode = 'unique_violation';\n        end if;\n    end;\n  end loop;\nend",
          "deprecated": false,
          "description": "Add the given hashes, of the form `{algorithm: hash}`, to the named\nobject.  The named object must already exist.  If any of the given\nalgorithms already exist in the table, then the hash must match exactly.\nThis function raises a CHECK_VIOLATION if the object's upload has been\nfinished (upload_id is null) or FOREIGN_KEY_VIOLATION if the object does\nnot exist.",
          "mode": "write",
          "returns": "void",
          "serviceName": "object"
        },
        "get_object_hashes": {
          "args": "name_in text",
          "body": "begin\n  return query\n    select\n      object_hashes.algorithm,\n      object_hashes.hash\n    from object_hashes\n    where name = name_in\n    order by algorithm;\nend",
          "deprecated": false,
          "description": "Get all hashes for the named object.  If the given object has no hashes,\nor doesn't exist, this function returns an empty result.",
          "mode": "read",
          "returns": "table ( algorithm text, hash text )",
          "serviceName": "object"
        }
      },
      "migrationScript": "begin\n  create table object_hashes (\n    name text not null\n      references objects (name)\n      on delete cascade,\n    algorithm text not null,\n    hash text not null\n  );\n\n  alter table object_hashes add primary key (name, algorithm);\n\n  grant select, insert, update, delete on object_hashes to $db_user_prefix$_object;\nend",
      "version": 67
    },
    {
      "description": "add queue_artifact_present",
      "methods": {
        "queue_artifact_present": {
          "args": "task_id_in text, run_id_in integer, name_in text",
          "body": "begin\n  return query\n  update queue_artifacts\n  set present = true\n  where\n    queue_artifacts.task_id = task_id_in and\n    queue_artifacts.run_id = run_id_in and\n    queue_artifacts.name = name_in\n  returning\n    queue_artifacts.task_id,\n    queue_artifacts.run_id,\n    queue_artifacts.name,\n    queue_artifacts.storage_type,\n    queue_artifacts.content_type,\n    queue_artifacts.details,\n    queue_artifacts.present,\n    queue_artifacts.expires;\nend",
          "deprecated": false,
          "description": "Mark the given queue artifact as present, returning the updated artifact.  Returns\nnothing if no such artifact exists.",
          "mode": "write",
          "returns": "table(task_id text, run_id integer, name text, storage_type text, content_type text, details jsonb, present boolean, expires timestamptz)",
          "serviceName": "queue"
        }
      },
      "version": 68
    },
    {
      "description": "update get_queue_artifacts_paginated with improved index usage",
      "methods": {
        "get_queue_artifacts_paginated": {
          "args": "task_id_in text, run_id_in integer, expires_in timestamptz, page_size_in integer, after_task_id_in text, after_run_id_in integer, after_name_in text",
          "body": "begin\n  return query\n  select\n    queue_artifacts.task_id,\n    queue_artifacts.run_id,\n    queue_artifacts.name,\n    queue_artifacts.storage_type,\n    queue_artifacts.content_type,\n    queue_artifacts.details,\n    queue_artifacts.present,\n    queue_artifacts.expires\n  from queue_artifacts\n  where\n    (queue_artifacts.task_id = task_id_in or task_id_in is null) and\n    (queue_artifacts.run_id = run_id_in or run_id_in is null) and\n    (queue_artifacts.expires < expires_in or expires_in is null) and\n    (after_task_id_in is null or\n      -- must use AND on the top level to use multicolumn index\n      (queue_artifacts.task_id >= after_task_id_in and\n        (queue_artifacts.task_id > after_task_id_in or\n          (queue_artifacts.task_id = after_task_id_in and\n            (queue_artifacts.run_id > after_run_id_in or\n              (queue_artifacts.run_id = after_run_id_in and\n                queue_artifacts.name > after_name_in\n              )\n            )\n          )\n        )\n      )\n    )\n  order by queue_artifacts.task_id, queue_artifacts.run_id, queue_artifacts.name\n  limit get_page_limit(page_size_in);\nend",
          "deprecated": false,
          "description": "Get existing queue artifacts, filtered by the optional arguments, ordered\nby the `task_id`, `run_id`, and `name`.  The `after_*` arguments specify\nwhere the page of results should begin, and must all be specified if any\nare specified.  Typically these values would be drawn from the last item\nin the previous page.",
          "mode": "read",
          "returns": "table(task_id text, run_id integer, name text, storage_type text, content_type text, details jsonb, present boolean, expires timestamptz)",
          "serviceName": "queue"
        }
      },
      "version": 69
    },
    {
      "description": "add `get_worker_pool_with_capacity_and_counts_by_state`, `get_worker_pools_with_capacity_and_counts_by_state`, and `update_worker_pool_with_capacity_and_counts_by_state` functions to get worker counts and capacity by state for worker pools",
      "methods": {
        "get_worker_pool_with_capacity": {
          "deprecated": true
        },
        "get_worker_pool_with_capacity_and_counts_by_state": {
          "args": "worker_pool_id_in text",
          "body": "begin\n  return query\n  select\n    worker_pools.worker_pool_id,\n    worker_pools.provider_id,\n    worker_pools.previous_provider_ids,\n    worker_pools.description,\n    worker_pools.config,\n    worker_pools.created,\n    worker_pools.last_modified,\n    worker_pools.owner,\n    worker_pools.email_on_error,\n    worker_pools.provider_data,\n    coalesce(sum(case when workers.state != 'stopped' then workers.capacity else 0 end))::integer,\n    coalesce(count(case when workers.state = 'requested' then workers.worker_id end))::integer,\n    coalesce(count(case when workers.state = 'running' then workers.worker_id end))::integer,\n    coalesce(count(case when workers.state = 'stopping' then workers.worker_id end))::integer,\n    coalesce(count(case when workers.state = 'stopped' then workers.worker_id end))::integer,\n    coalesce(sum(case when workers.state = 'requested' then workers.capacity else 0 end))::integer,\n    coalesce(sum(case when workers.state = 'running' then workers.capacity else 0 end))::integer,\n    coalesce(sum(case when workers.state = 'stopping' then workers.capacity else 0 end))::integer,\n    coalesce(sum(case when workers.state = 'stopped' then workers.capacity else 0 end))::integer\n  from worker_pools\n  left join workers on workers.worker_pool_id = worker_pools.worker_pool_id\n  where worker_pools.worker_pool_id = worker_pool_id_in\n  group by worker_pools.worker_pool_id\n  order by worker_pools.worker_pool_id;\nend",
          "deprecated": false,
          "description": "Get an existing worker pool.  The returned table will have one or (if no such worker pool is defined) zero rows.",
          "mode": "read",
          "returns": "table(worker_pool_id text, provider_id text, previous_provider_ids jsonb, description text, config jsonb, created timestamptz, last_modified timestamptz, owner text, email_on_error boolean, provider_data jsonb, current_capacity integer, requested_count integer, running_count integer, stopping_count integer, stopped_count integer, requested_capacity integer, running_capacity integer, stopping_capacity integer, stopped_capacity integer)",
          "serviceName": "worker_manager"
        },
        "get_worker_pools_with_capacity": {
          "deprecated": true
        },
        "get_worker_pools_with_capacity_and_counts_by_state": {
          "args": "page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    worker_pools.worker_pool_id,\n    worker_pools.provider_id,\n    worker_pools.previous_provider_ids,\n    worker_pools.description,\n    worker_pools.config,\n    worker_pools.created,\n    worker_pools.last_modified,\n    worker_pools.owner,\n    worker_pools.email_on_error,\n    worker_pools.provider_data,\n    coalesce(sum(case when workers.state != 'stopped' then workers.capacity else 0 end))::integer,\n    coalesce(count(case when workers.state = 'requested' then workers.worker_id end))::integer,\n    coalesce(count(case when workers.state = 'running' then workers.worker_id end))::integer,\n    coalesce(count(case when workers.state = 'stopping' then workers.worker_id end))::integer,\n    coalesce(count(case when workers.state = 'stopped' then workers.worker_id end))::integer,\n    coalesce(sum(case when workers.state = 'requested' then workers.capacity else 0 end))::integer,\n    coalesce(sum(case when workers.state = 'running' then workers.capacity else 0 end))::integer,\n    coalesce(sum(case when workers.state = 'stopping' then workers.capacity else 0 end))::integer,\n    coalesce(sum(case when workers.state = 'stopped' then workers.capacity else 0 end))::integer\n  from worker_pools\n  left join workers on workers.worker_pool_id = worker_pools.worker_pool_id\n  group by worker_pools.worker_pool_id\n  order by worker_pools.worker_pool_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get existing worker pools, ordered by `worker_pool_id`.  If the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(worker_pool_id text, provider_id text, previous_provider_ids jsonb, description text, config jsonb, created timestamptz, last_modified timestamptz, owner text, email_on_error boolean, provider_data jsonb, current_capacity integer, requested_count integer, running_count integer, stopping_count integer, stopped_count integer, requested_capacity integer, running_capacity integer, stopping_capacity integer, stopped_capacity integer)",
          "serviceName": "worker_manager"
        },
        "update_worker_pool_with_capacity": {
          "deprecated": true
        },
        "update_worker_pool_with_capacity_and_counts_by_state": {
          "args": "worker_pool_id_in text, provider_id_in text, description_in text, config_in jsonb, last_modified_in timestamptz, owner_in text, email_on_error_in boolean",
          "body": "declare\n  existing record;\nbegin\n  select\n    worker_pools.provider_id,\n    worker_pools.previous_provider_ids\n  from worker_pools\n  where worker_pools.worker_pool_id = worker_pool_id_in\n  -- lock this row for the duration of this transaction..\n  for update\n  into existing;\n\n  -- update previous_provider_ids, if the provider_id has changed\n  if existing.provider_id <> provider_id_in then\n    -- remove both provider IDs to avoid duplicates, then re-add existing.provider_id\n    existing.previous_provider_ids = (existing.previous_provider_ids - provider_id_in - existing.provider_id) || jsonb_build_array(existing.provider_id);\n  end if;\n\n  return query update worker_pools\n  set\n    provider_id = provider_id_in,\n    description = description_in,\n    config = config_in,\n    last_modified = last_modified_in,\n    owner = owner_in,\n    email_on_error = email_on_error_in,\n    previous_provider_ids = existing.previous_provider_ids\n  where worker_pools.worker_pool_id = worker_pool_id_in\n  returning\n    worker_pools.worker_pool_id,\n    worker_pools.provider_id,\n    worker_pools.description,\n    worker_pools.config,\n    worker_pools.created,\n    worker_pools.last_modified,\n    worker_pools.owner,\n    worker_pools.email_on_error,\n    existing.provider_id as previous_provider_id,\n    coalesce((\n      select sum(workers.capacity) from workers where\n        workers.worker_pool_id = worker_pool_id_in and\n        workers.state != 'stopped'),\n      0)::integer,\n    coalesce((\n      select count(*) from workers where\n        workers.worker_pool_id = worker_pool_id_in and\n        workers.state = 'requested'),\n      0)::integer,\n    coalesce((\n      select count(*) from workers where\n        workers.worker_pool_id = worker_pool_id_in and\n        workers.state = 'running'),\n      0)::integer,\n    coalesce((\n      select count(*) from workers where\n        workers.worker_pool_id = worker_pool_id_in and\n        workers.state = 'stopping'),\n      0)::integer,\n    coalesce((\n      select count(*) from workers where\n        workers.worker_pool_id = worker_pool_id_in and\n        workers.state = 'stopped'),\n      0)::integer,\n    coalesce((\n      select sum(workers.capacity) from workers where\n        workers.worker_pool_id = worker_pool_id_in and\n        workers.state = 'requested'),\n      0)::integer,\n    coalesce((\n      select sum(workers.capacity) from workers where\n        workers.worker_pool_id = worker_pool_id_in and\n        workers.state = 'running'),\n      0)::integer,\n    coalesce((\n      select sum(workers.capacity) from workers where\n        workers.worker_pool_id = worker_pool_id_in and\n        workers.state = 'stopping'),\n      0)::integer,\n    coalesce((\n      select sum(workers.capacity) from workers where\n        workers.worker_pool_id = worker_pool_id_in and\n        workers.state = 'stopped'),\n      0)::integer;\nend",
          "deprecated": false,
          "description": "Update API-accessible columns on an existig worker pool.  All fields are\noverridden, but if the provider_id changes, then the existing provider_id\nis added to previous_provider_ids.  The return value contains values\nrequired for an API response and previous_provider_id (singular) containing\nthe provider_id found before the update.  If no such worker pool exists,\nthe return value is an empty set.",
          "mode": "write",
          "returns": "table(worker_pool_id text, provider_id text, description text, config jsonb, created timestamptz, last_modified timestamptz, owner text, email_on_error boolean, previous_provider_id text, current_capacity integer, requested_count integer, running_count integer, stopping_count integer, stopped_count integer, requested_capacity integer, running_capacity integer, stopping_capacity integer, stopped_capacity integer)",
          "serviceName": "worker_manager"
        }
      },
      "version": 70
    },
    {
      "description": "add provider filter to get_non_stopped_workers_quntil",
      "downgradeScript": "begin\n  revoke select on queue_workers from $db_user_prefix$_worker_manager;\nend\n",
      "methods": {
        "get_non_stopped_workers_quntil": {
          "deprecated": true
        },
        "get_non_stopped_workers_quntil_providers": {
          "args": "worker_pool_id_in text, worker_group_in text, worker_id_in text, providers_filter_cond text, providers_filter_value text, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    workers.worker_pool_id,\n    workers.worker_group,\n    workers.worker_id,\n    workers.provider_id,\n    workers.created,\n    workers.expires,\n    workers.state,\n    workers.provider_data,\n    workers.capacity,\n    workers.last_modified,\n    workers.last_checked,\n    workers.secret,\n    workers.etag,\n    queue_workers.quarantine_until\n  from\n    workers\n    left join queue_workers on\n      workers.worker_pool_id = queue_workers.task_queue_id and\n      workers.worker_id = queue_workers.worker_id and\n      workers.worker_group = queue_workers.worker_group\n  where\n    (workers.worker_pool_id = worker_pool_id_in or worker_pool_id_in is null) and\n    (workers.worker_group = worker_group_in or worker_group_in is null) and\n    (workers.worker_id = worker_id_in or worker_id_in is null) and\n    (workers.state <> 'stopped') and\n    (providers_filter_cond is null or providers_filter_value is null or\n      case\n        when providers_filter_cond = '='\n          then workers.provider_id = ANY(string_to_array(providers_filter_value, ','))\n        when providers_filter_cond = '<>'\n          then workers.provider_id <> ALL(string_to_array(providers_filter_value, ','))\n      end\n      )\n  order by worker_pool_id, worker_group, worker_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get non-stopped workers filtered by the optional arguments,\nordered by `worker_pool_id`, `worker_group`, and  `worker_id`.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset `page_offset`.\nThe `quaratine_until` contains NULL or a date in the past if the\nworker is not quarantined, otherwise the date until which it is\nquaratined. `providers_filter_cond` and `providers_filter_value` used to\nfilter `=` or `<>` provider by value.",
          "mode": "read",
          "returns": "table(worker_pool_id text, worker_group text, worker_id text, provider_id text, created timestamptz, expires timestamptz, state text, provider_data jsonb, capacity integer, last_modified timestamptz, last_checked timestamptz, secret jsonb, etag uuid, quarantine_until timestamptz)",
          "serviceName": "worker_manager"
        }
      },
      "migrationScript": "begin\n  grant select on queue_workers to $db_user_prefix$_worker_manager;\nend\n",
      "version": 71
    },
    {
      "description": "add last_date_active to queue_workers",
      "downgradeScript": "begin\n  alter table queue_workers drop column last_date_active;\nend\n",
      "methods": {
        "get_queue_worker_tqid": {
          "deprecated": true
        },
        "get_queue_worker_tqid_with_last_date_active": {
          "args": "task_queue_id_in text, worker_group_in text, worker_id_in text, expires_in timestamptz",
          "body": "begin\n  return query\n  select\n    queue_workers.task_queue_id,\n    queue_workers.worker_group,\n    queue_workers.worker_id,\n    queue_workers.quarantine_until,\n    queue_workers.expires,\n    queue_workers.first_claim,\n    queue_workers.recent_tasks,\n    queue_workers.last_date_active,\n    public.gen_random_uuid()\n  from queue_workers\n  where\n    queue_workers.task_queue_id = task_queue_id_in and\n    queue_workers.worker_group = worker_group_in and\n    queue_workers.worker_id = worker_id_in and\n    (queue_workers.expires > expires_in or queue_workers.quarantine_until > expires_in);\n  end",
          "deprecated": false,
          "description": "Get a non-expired queue worker by task_queue_id, worker_group, and worker_id.\nWorkers are not considered expired until after their quarantine date expires.",
          "mode": "read",
          "returns": "table(task_queue_id text, worker_group text, worker_id text, quarantine_until timestamptz, expires timestamptz, first_claim timestamptz, recent_tasks jsonb, last_date_active timestamptz, etag uuid)",
          "serviceName": "queue"
        },
        "get_queue_workers_tqid": {
          "deprecated": true
        },
        "get_queue_workers_tqid_with_last_date_active": {
          "args": "task_queue_id_in text, expires_in timestamptz, page_size_in integer, page_offset_in integer",
          "body": "begin\n  return query\n  select\n    queue_workers.task_queue_id,\n    queue_workers.worker_group,\n    queue_workers.worker_id,\n    queue_workers.quarantine_until,\n    queue_workers.expires,\n    queue_workers.first_claim,\n    queue_workers.recent_tasks,\n    queue_workers.last_date_active,\n    public.gen_random_uuid()\n  from queue_workers\n  where\n    (queue_workers.task_queue_id = task_queue_id_in or get_queue_workers_tqid_with_last_date_active.task_queue_id_in is null) and\n    ((queue_workers.expires > expires_in and queue_workers.quarantine_until < expires_in) or get_queue_workers_tqid_with_last_date_active.expires_in is null)\n  order by task_queue_id, worker_group, worker_id\n  limit get_page_limit(page_size_in)\n  offset get_page_offset(page_offset_in);\nend",
          "deprecated": false,
          "description": "Get non-expired queue workers ordered by task_queue_id, worker_group, and worker_id.\nWorkers are not considered expired until after their quarantine date expires.\nIf the pagination arguments are both NULL, all rows are returned.\nOtherwise, page_size rows are returned at offset page_offset.",
          "mode": "read",
          "returns": "table(task_queue_id text, worker_group text, worker_id text, quarantine_until timestamptz, expires timestamptz, first_claim timestamptz, recent_tasks jsonb, last_date_active timestamptz, etag uuid)",
          "serviceName": "queue"
        },
        "quarantine_queue_worker": {
          "deprecated": true
        },
        "quarantine_queue_worker_with_last_date_active": {
          "args": "task_queue_id_in text, worker_group_in text, worker_id_in text, quarantine_until_in timestamptz",
          "body": "begin\n  return query update queue_workers\n  set\n    quarantine_until = quarantine_until_in,\n    expires = greatest(queue_workers.expires, now() + interval '1 day')\n  where\n    queue_workers.task_queue_id = task_queue_id_in and\n    queue_workers.worker_group = worker_group_in and\n    queue_workers.worker_id = worker_id_in\n  returning\n    queue_workers.task_queue_id,\n    queue_workers.worker_group,\n    queue_workers.worker_id,\n    queue_workers.quarantine_until,\n    queue_workers.expires,\n    queue_workers.first_claim,\n    queue_workers.recent_tasks,\n    queue_workers.last_date_active;\nend",
          "deprecated": false,
          "description": "Update the quarantine_until date for a worker.  The Queue service interprets a date in the past\nas \"not quarantined\".  This function also \"bumps\" the expiration of the worker so that un-quarantined\nworkers do not immediately expire.  Returns the worker row just as get_queue_worker would, or no rows if\nno such worker exists.",
          "mode": "write",
          "returns": "table(task_queue_id text, worker_group text, worker_id text, quarantine_until timestamptz, expires timestamptz, first_claim timestamptz, recent_tasks jsonb, last_date_active timestamptz)",
          "serviceName": "queue"
        },
        "queue_worker_seen": {
          "deprecated": true
        },
        "queue_worker_seen_with_last_date_active": {
          "args": "task_queue_id_in text, worker_group_in text, worker_id_in text, expires_in timestamptz",
          "body": "begin\n  insert\n    into queue_workers (task_queue_id, worker_group, worker_id, quarantine_until, expires, first_claim, recent_tasks, last_date_active)\n    values (\n      task_queue_id_in,\n      worker_group_in,\n      worker_id_in,\n      now() - interval '10 years',\n      expires_in,\n      now(),\n      jsonb_build_array(),\n      now()\n    )\n    on conflict (task_queue_id, worker_group, worker_id) do update\n    set\n      expires = greatest(coalesce(expires_in, queue_workers.expires), queue_workers.expires),\n      last_date_active = now()\n    where\n      queue_workers.task_queue_id = task_queue_id_in and\n      queue_workers.worker_group = worker_group_in and\n      queue_workers.worker_id = worker_id_in;\nend",
          "deprecated": false,
          "description": "Recognize that a worker has been seen by the queue, creating it if necessary.  This is called\nwhen workers claim or re-claim work.  The expiration time is not allowed to move backward.\nWill also always bump its last date active time.\n\nThis function always writes to the DB, so calls should be suitably rate-limited at the\nclient side.",
          "mode": "write",
          "returns": "void",
          "serviceName": "queue"
        }
      },
      "migrationScript": "begin\n  alter table queue_workers add column last_date_active timestamp with time zone;\nend\n",
      "version": 72
    }
  ]
}