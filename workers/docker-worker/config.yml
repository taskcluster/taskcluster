defaults:
  monitorProject: 'docker-worker'
  deviceManagement:
    cpu:
      enabled: false
    loopbackAudio:
      enabled: true
    loopbackVideo:
      enabled: true
  phone:
    enabled: false
    sims: '1'
    type: null
  dockerConfig:
    # Privileged mode will allow tasks to run with elevated privileges similar
    # to process running on the host.  The task containers will have access to
    # all host devices and create docker daemons within containers.  Use this
    # option with caution.
    # TODO: Consider killing the node after completing one task or locking down
    # in other ways.
    allowPrivileged: false
    # Default registry to use when making authenticated image requests.  This
    # is similar to what docker pull does when `docker pull ubuntu:14.10`.
    defaultRegistry: 'registry.hub.docker.com'
    # Maximum number of attempts to make when pulling an image
    maxAttempts: 5
    # Delay factor rand randomizationFactor used to computer randomized
    # exponential backoff when attempting to retry docker pulls
    delayFactor: 15000
    # Value between 0 and 1
    randomizationFactor: 0.25
  features:
    relengAPIProxy:
      image: 'taskcluster/relengapi-proxy:2.0.1'
      token: !env RELENG_API_TOKEN
  ssl:
    certificate: '/etc/star_taskcluster-worker_net.crt'
    key: '/etc/star_taskcluster-worker_net.key'

  # Hostname of this docker worker
  host: 'localhost'

  statelessHostname:
    enabled: false
    secret: process.env.DNS_SERVER_SECRET
    domain: 'taskcluster-worker.net'

  # Run test only teardown and logging events.
  testMode: false

  # Run each container in as restrict fashion as possible (one core per container)
  # When this is true the capacity is always overriden to the number of cores.
  restrictCPU: false

  # Image used to  create the taskcluster proxy container.
  taskclusterProxyImage: 'taskcluster/taskcluster-proxy:latest'
  taskclusterLogImage: 'taskcluster/livelog:v4'
  testdroidProxyImage: 'taskcluster/testdroid-proxy:0.0.7'
  balrogVPNProxyImage: 'taskclusterprivate/taskcluster-vpn-proxy:0.0.3'
  dindImage: 'taskcluster/dind-service:v4.0'

  alivenessCheckInterval: 30000

  capacityManagement:
    diskspaceThreshold: 10000000000

  dockerVolume: '/mnt'

  # Garbage Collection configuration
  garbageCollection:
    imageExpiration: 7200000
    # Amount of time that should elapse before an exited container that was not
    # explicitly marked for removal is removed.
    containerExpiration: 1800000
    interval: 60000

  # Shutdown configuration...
  shutdown:
    enabled: false
    minimumCycleSeconds: null

  cache:
    volumeCachePath: '/mnt/var/cache/docker-worker'

  logging:
    # When enabled live logs will be served over SSL
    secureLiveLogging: false
    # Used by Azure live logger to chunk writes and send on an interval
    liveLogChunkInterval: 5000 # 5 seconds
    # Added to the current date to make up the expiry time for logs. This is
    # hack to generate a year in ms... Note that two args (year, month) are
    # required here instead of one due to some quirk of v8...
    liveLogExpires: '1 year'
    bulkLogExpires: '1 year'

  task:
    # We must reclaim somewhat frequently (but not too frequently) this is the
    # divisor used to figure out when to issue the reclaim based on taken until
    # for example `2` would mean half the time between now and taken until.
    reclaimDivisor: 1.3
    # Tasks should be removed from the queue if they have been dequeued a lot.
    # Possible signs that the task is bad
    dequeueCount: 15

  taskQueue:
    # Task queue will be polled on a frequent interval for new pending tasks
    pollInterval: 5000
    # Task queue will be polled slower near the end of a billing cycle by this many times
    # The higher this number is, the more likely we are to overkill on scaling down
    # The lower this number is, the more likely we are to keep containers alive at lower than
    # maximum capacity
    # No particular reasoning for the choice of 12, just a trial for now
    pollIntervalMultiplier: 12
    # Task polling will slow down in the last 1/x of a billing cycle
    # Task run times have a distribution of something like 3/6/9min=Q1/2/3, so by reducing the number
    # of tasks we grab in the last 60/3=20 min, we can increase the chances of a node dying
    # if there is enough capacity to handle tasks without it
    # If we increase this time (decrease the number here), it will decrease the efficiency
    # of a node as it nears the end stages because there's a chance it could pick up two short tasks
    # before it dies.
    # If we decrease this time (increase the number here), it will cause more nodes to live past
    # the next threshold (0:58:00, 1:58:00, etc) in exchange for increasing efficiency near end of life.
    # Ideally we want to keep the period less than double the average task but more than the runtime of
    # most (say, ~75%) of tasks.
    slowdownDivisor: 3

  #Registries which we can authenticate against for pulls:
  #  registries: {
  #  Note that these match based on the nearest path so the below
  #  will authenticate for quay.io/mozilla/xfoo, etc...
  #    'quay.io/mozilla': {
  #      username: '...',
  #      password: '...'
  #    }
  #  }
  registries: {}

  # Taskcluster client `credentials`.
  taskcluster:
    clientId:    !env TASKCLUSTER_CLIENT_ID
    accessToken: !env TASKCLUSTER_ACCESS_TOKEN

  # When true will create durable queue on pulse.
  createQueue: true

  # Pulse credentials
  pulse:
    username:   !env PULSE_USERNAME
    password:   !env PULSE_PASSWORD

  metricsCollection:
    # Only collect host level metrics every minute
    hostMetricsInterval: 60000

  influx:
    connectionString: null
    # Wait no more than 10 minutes before flushing stats
    maxDelay: 600
    # Maximum number of points to queue before flush
    maxPendingPoints: 100

  testdroid:
    url:      !env TESTDROID_URL
    username: !env TESTDROID_USERNAME
    password: !env TESTDROID_PASSWORD

  dockerWorkerPrivateKey: '/etc/docker-worker-priv.pem'
  signingKeyLocation: '/etc/docker-worker-gpg-signing-key.key'

  dockerSave:
    expiration: '7 days'

  interactive:
    ssl: true
    # Minimum time, in seconds, between start of task and end of task
    # Could be cut short due to worker node shutdown
    minTime: 180
    # Extra time the task stays alive after an interactive session finishes
    expirationAfterSession: 900
    #base path of the socket artifact
    artifactName: 'private/docker-worker/'

  schema:
    region: 'us-west-2'
    bucket: 'schemas.taskcluster.net'
    path: 'docker-worker/v1/'

production:
  logging:
    secureLiveLogging: true

  # DNS Server secret used for constructing hostnames from using the
  # stateless dns server
  statelessHostname:
    enabled: true
    secret:  !env DNS_SERVER_SECRET
    domain:  'taskcluster-worker.net'

test:
  monitorProject: 'docker-worker-test'
  capacity: 1
  testMode: true
  createQueue: false

  dockerConfig:
    allowPrivileged: false
    defaultRegistry: 'registry.hub.docker.com'
    maxAttempts: 5
    delayFactor: 100
    randomizationFactor: 0.25

  taskQueue:
    pollInterval: 1000
    pollIntervalMultiplier: 4
    slowdownDivisor: 2

  influx:
    maxDelay: 1
    maxPendingPoints: 1
    allowHTTP: true

  ssl:
    certificate: '/worker/test/fixtures/ssl_cert.crt'
    key: '/worker/test/fixtures/ssl_cert.key'

  logging:
    # Expires one hour from now so test logs don't live too long...
    liveLogExpires: '1 hour'
    bulkLogExpires: '1 hour'

  cache:
    volumeCachePath: '/tmp/test-cache'

  capacityManagement:
    diskspaceThreshold: 1000000000

  dockerVolume: '/tmp'

  dockerWorkerPrivateKey: '/worker/test/docker-worker-priv.pem'
  signingKeyLocation: '/worker/test/fixtures/gpg_signing_key.key'

  interactive:
    artifactName: 'private/docker-worker-tests/'
